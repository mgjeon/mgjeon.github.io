[
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "Create an image using Diffusers library.\n\n\n\n!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)\n\n\n\n\n\n# 1. Load the autoencoder model which will be used to decode the latents into image space.\nvae = pipe.vae\n\n# 2. Load the tokenizer and text encoder to tokenize and encode the text.\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\n\n# 3. The UNet model for generating the latents.\nunet = pipe.unet\n\n\n\n\nvae\n\nAutoencoderKL(\n  (encoder): Encoder(\n    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (down_blocks): ModuleList(\n      (0): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (1): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (2): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (3): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (decoder): Decoder(\n    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (up_blocks): ModuleList(\n      (0-1): 2 x UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0-2): 3 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (2): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (3): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\ntokenizer\n\nCLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"&lt;|startoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True)\n\n\n\ntext_encoder\n\nCLIPTextModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 768)\n      (position_embedding): Embedding(77, 768)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n\n\n\nunet\n\nUNet2DConditionModel(\n  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n\n\n\n\n\ntext_input = tokenizer(\n    prompt * batch_size,\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\ntext_input.shape, text_input\n\n(torch.Size([2, 77]),\n tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input)[0]\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nuncond_input = tokenizer(\n    [\"\"] * batch_size,\n    padding=\"max_length\",\n    max_length=text_input.shape[-1],\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nuncond_input.shape, uncond_input\n\n(torch.Size([2, 77]),\n tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input)[0]\n\nuncond_embeddings.shape, uncond_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]]],\n        device='cuda:0'))\n\n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([4, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nlatents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n).to(device)\n\nlatents.shape, latents\n\n(torch.Size([2, 4, 64, 64]),\n tensor([[[[ 0.1884, -0.6394,  0.1089,  ..., -0.9887, -0.7133, -1.1545],\n           [ 0.4124,  1.5587, -0.3407,  ...,  2.1968, -0.0356, -0.0810],\n           [-1.8912,  0.0528, -0.4425,  ...,  1.3110,  0.7100,  0.6802],\n           ...,\n           [-1.3443, -0.1747, -0.6298,  ...,  0.4572, -0.8584, -0.1284],\n           [-1.7920, -0.6554, -0.0439,  ...,  0.5436,  2.2266, -0.5003],\n           [ 0.6213, -1.3155,  0.7470,  ..., -0.2354,  0.7097,  0.6170]],\n \n          [[-0.5007, -1.4418,  0.2598,  ..., -0.2586,  2.3239, -1.3245],\n           [ 0.8540, -0.4135,  0.5658,  ..., -1.9556,  2.0454, -0.2454],\n           [-0.3212, -1.9329, -1.1598,  ...,  0.7156, -0.7228, -0.6992],\n           ...,\n           [ 0.0180, -0.7993,  2.3330,  ...,  0.2594, -0.0333, -0.0826],\n           [-1.2569, -0.8219,  1.3467,  ...,  0.4792,  1.8265, -0.6156],\n           [-1.9367, -0.0949,  0.0720,  ...,  0.0806,  0.2966, -1.0284]],\n \n          [[ 0.2291, -0.0936, -1.3283,  ...,  1.4995, -0.1965, -0.2879],\n           [-1.0226, -1.2896,  1.6202,  ..., -0.3910, -0.3834,  0.5519],\n           [ 0.5424,  0.2685,  0.4912,  ...,  0.9773, -0.8260,  1.1552],\n           ...,\n           [-1.5280, -0.2530, -1.3748,  ..., -1.4948,  1.3661, -1.1294],\n           [ 0.4241, -0.2996,  1.8231,  ...,  0.6968,  0.8247, -0.0279],\n           [-3.3711, -0.7468, -1.3212,  ..., -0.4128,  0.4621,  2.6297]],\n \n          [[-0.7510, -0.7452, -0.8998,  ..., -1.6957, -0.4004, -0.2596],\n           [-1.2092, -1.8881, -0.5828,  ..., -1.0428, -0.6500,  0.3601],\n           [-0.4254,  0.9478,  1.3083,  ..., -0.0259, -0.4542,  0.4353],\n           ...,\n           [-0.1918,  0.4858,  0.0666,  ...,  0.8505, -0.6606, -0.3193],\n           [ 1.3620,  0.2283,  0.6292,  ..., -0.9271,  1.7018,  0.2161],\n           [-0.3891, -1.8911, -0.7501,  ..., -0.2330, -1.0460,  0.4121]]],\n \n \n         [[[ 0.3649, -1.3183, -1.3308,  ..., -0.5548, -1.3610, -1.9329],\n           [-0.0071,  0.1977,  1.5517,  ..., -1.6664,  1.6551,  0.1798],\n           [-1.0404,  0.6524,  0.4654,  ..., -0.5947, -1.0871,  2.2230],\n           ...,\n           [-0.6844,  0.1692, -0.2559,  ...,  0.5511,  0.9734,  0.7936],\n           [-1.1951,  0.5016,  0.8089,  ...,  0.2337, -0.2213, -1.1724],\n           [-0.5055, -0.7491, -1.4940,  ..., -2.1332,  0.9120,  0.2057]],\n \n          [[ 1.3668, -1.1680, -0.8574,  ..., -0.0635, -1.9132, -0.6023],\n           [ 1.0974, -0.9654,  1.2987,  ...,  1.3187, -0.0241, -0.5427],\n           [-2.0427, -1.4358, -0.7115,  ...,  0.1088,  0.0764,  0.7254],\n           ...,\n           [ 1.0957,  1.4058, -0.0178,  ...,  0.5748,  0.0953,  0.7550],\n           [ 0.4080,  0.8792,  0.6801,  ..., -0.7215,  1.1261,  0.0551],\n           [-0.3183, -2.3306,  0.7155,  ...,  0.4291, -0.2074, -1.1237]],\n \n          [[-0.2401,  0.9229,  0.0212,  ...,  0.2128, -0.4705, -0.3262],\n           [ 0.1108,  0.8909,  0.5309,  ..., -1.7175, -1.6657, -1.7706],\n           [-0.1654, -0.4582, -1.2832,  ...,  0.5297, -0.8363,  1.0293],\n           ...,\n           [-1.3526,  2.1482,  0.5417,  ..., -2.2156, -1.9940, -0.9745],\n           [-0.5821,  0.0492,  0.6693,  ..., -0.8610,  0.5864, -0.6040],\n           [ 1.0180,  1.4447,  0.9563,  ...,  0.9034,  0.7988, -1.7119]],\n \n          [[-1.6146,  0.0868,  0.6415,  ...,  0.2083,  0.4058,  0.2813],\n           [ 0.1969, -0.3334, -0.6526,  ..., -1.4639, -1.6302, -0.6036],\n           [ 0.1556, -0.0859, -0.0230,  ..., -0.7900, -0.3481,  0.8767],\n           ...,\n           [ 0.6056,  0.8374, -0.3834,  ..., -0.6636, -0.4814,  0.8244],\n           [ 0.6982, -0.4884, -1.3777,  ...,  0.5876, -2.0944,  0.0853],\n           [ 0.0388, -0.5761, -0.5116,  ..., -1.6645,  0.1752, -0.1923]]]],\n        device='cuda:0'))\n\n\n\nscheduler.set_timesteps(num_inference_steps)\n\nscheduler.timesteps.shape, pipe.scheduler.timesteps\n\n(torch.Size([50]),\n tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n         856.2857, 835.8980, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n         713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n         570.8571, 550.4694, 530.0816, 509.6939, 489.3061, 468.9184, 448.5306,\n         428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n         285.4286, 265.0408, 244.6531, 224.2653, 203.8776, 183.4898, 163.1020,\n         142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n           0.0000], dtype=torch.float64))\n\n\n\n\n\n\n\\tilde{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{z}_t, \\mathbf{c}) = w\\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) + (1-w)\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t).\n\nHere, \\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) and \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t) are conditional and unconditional \\boldsymbol{\\epsilon}-predictions, given by \\boldsymbol{\\epsilon}_\\theta := (\\mathbf{z}_t - \\alpha_t\\hat{\\mathbf{x}}_\\theta)/\\sigma_t, and w is the guidance weight. Setting w = 1 disables classifier-free guidance, while increasing w &gt; 1 strengthens the effect of guidance.1\n\nfrom tqdm.auto import tqdm\n\nlatents = latents * scheduler.init_noise_sigma\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(\n            latent_model_input, t, encoder_hidden_states=text_embeddings\n        ).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = (\n        guidance_scale * noise_pred_text + (1 - guidance_scale) * noise_pred_uncond\n    )\n\n    # compute the previous noisy sample x_t -&gt; x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n\nfrom PIL import Image\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\n\n\nfor pil_image in pil_images:\n    display(pil_image)\n\n\n\n\n\n\n\n\n\n\n\n\nPatil et al.(2022) Stable Diffusion with  Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#install-and-import-libraries",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#install-and-import-libraries",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#low-level",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#low-level",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "# 1. Load the autoencoder model which will be used to decode the latents into image space.\nvae = pipe.vae\n\n# 2. Load the tokenizer and text encoder to tokenize and encode the text.\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\n\n# 3. The UNet model for generating the latents.\nunet = pipe.unet\n\n\n\n\nvae\n\nAutoencoderKL(\n  (encoder): Encoder(\n    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (down_blocks): ModuleList(\n      (0): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (1): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (2): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (3): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (decoder): Decoder(\n    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (up_blocks): ModuleList(\n      (0-1): 2 x UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0-2): 3 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (2): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (3): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\ntokenizer\n\nCLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"&lt;|startoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True)\n\n\n\ntext_encoder\n\nCLIPTextModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 768)\n      (position_embedding): Embedding(77, 768)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n\n\n\nunet\n\nUNet2DConditionModel(\n  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n\n\n\n\n\ntext_input = tokenizer(\n    prompt * batch_size,\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\ntext_input.shape, text_input\n\n(torch.Size([2, 77]),\n tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input)[0]\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nuncond_input = tokenizer(\n    [\"\"] * batch_size,\n    padding=\"max_length\",\n    max_length=text_input.shape[-1],\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nuncond_input.shape, uncond_input\n\n(torch.Size([2, 77]),\n tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input)[0]\n\nuncond_embeddings.shape, uncond_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]]],\n        device='cuda:0'))\n\n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([4, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nlatents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n).to(device)\n\nlatents.shape, latents\n\n(torch.Size([2, 4, 64, 64]),\n tensor([[[[ 0.1884, -0.6394,  0.1089,  ..., -0.9887, -0.7133, -1.1545],\n           [ 0.4124,  1.5587, -0.3407,  ...,  2.1968, -0.0356, -0.0810],\n           [-1.8912,  0.0528, -0.4425,  ...,  1.3110,  0.7100,  0.6802],\n           ...,\n           [-1.3443, -0.1747, -0.6298,  ...,  0.4572, -0.8584, -0.1284],\n           [-1.7920, -0.6554, -0.0439,  ...,  0.5436,  2.2266, -0.5003],\n           [ 0.6213, -1.3155,  0.7470,  ..., -0.2354,  0.7097,  0.6170]],\n \n          [[-0.5007, -1.4418,  0.2598,  ..., -0.2586,  2.3239, -1.3245],\n           [ 0.8540, -0.4135,  0.5658,  ..., -1.9556,  2.0454, -0.2454],\n           [-0.3212, -1.9329, -1.1598,  ...,  0.7156, -0.7228, -0.6992],\n           ...,\n           [ 0.0180, -0.7993,  2.3330,  ...,  0.2594, -0.0333, -0.0826],\n           [-1.2569, -0.8219,  1.3467,  ...,  0.4792,  1.8265, -0.6156],\n           [-1.9367, -0.0949,  0.0720,  ...,  0.0806,  0.2966, -1.0284]],\n \n          [[ 0.2291, -0.0936, -1.3283,  ...,  1.4995, -0.1965, -0.2879],\n           [-1.0226, -1.2896,  1.6202,  ..., -0.3910, -0.3834,  0.5519],\n           [ 0.5424,  0.2685,  0.4912,  ...,  0.9773, -0.8260,  1.1552],\n           ...,\n           [-1.5280, -0.2530, -1.3748,  ..., -1.4948,  1.3661, -1.1294],\n           [ 0.4241, -0.2996,  1.8231,  ...,  0.6968,  0.8247, -0.0279],\n           [-3.3711, -0.7468, -1.3212,  ..., -0.4128,  0.4621,  2.6297]],\n \n          [[-0.7510, -0.7452, -0.8998,  ..., -1.6957, -0.4004, -0.2596],\n           [-1.2092, -1.8881, -0.5828,  ..., -1.0428, -0.6500,  0.3601],\n           [-0.4254,  0.9478,  1.3083,  ..., -0.0259, -0.4542,  0.4353],\n           ...,\n           [-0.1918,  0.4858,  0.0666,  ...,  0.8505, -0.6606, -0.3193],\n           [ 1.3620,  0.2283,  0.6292,  ..., -0.9271,  1.7018,  0.2161],\n           [-0.3891, -1.8911, -0.7501,  ..., -0.2330, -1.0460,  0.4121]]],\n \n \n         [[[ 0.3649, -1.3183, -1.3308,  ..., -0.5548, -1.3610, -1.9329],\n           [-0.0071,  0.1977,  1.5517,  ..., -1.6664,  1.6551,  0.1798],\n           [-1.0404,  0.6524,  0.4654,  ..., -0.5947, -1.0871,  2.2230],\n           ...,\n           [-0.6844,  0.1692, -0.2559,  ...,  0.5511,  0.9734,  0.7936],\n           [-1.1951,  0.5016,  0.8089,  ...,  0.2337, -0.2213, -1.1724],\n           [-0.5055, -0.7491, -1.4940,  ..., -2.1332,  0.9120,  0.2057]],\n \n          [[ 1.3668, -1.1680, -0.8574,  ..., -0.0635, -1.9132, -0.6023],\n           [ 1.0974, -0.9654,  1.2987,  ...,  1.3187, -0.0241, -0.5427],\n           [-2.0427, -1.4358, -0.7115,  ...,  0.1088,  0.0764,  0.7254],\n           ...,\n           [ 1.0957,  1.4058, -0.0178,  ...,  0.5748,  0.0953,  0.7550],\n           [ 0.4080,  0.8792,  0.6801,  ..., -0.7215,  1.1261,  0.0551],\n           [-0.3183, -2.3306,  0.7155,  ...,  0.4291, -0.2074, -1.1237]],\n \n          [[-0.2401,  0.9229,  0.0212,  ...,  0.2128, -0.4705, -0.3262],\n           [ 0.1108,  0.8909,  0.5309,  ..., -1.7175, -1.6657, -1.7706],\n           [-0.1654, -0.4582, -1.2832,  ...,  0.5297, -0.8363,  1.0293],\n           ...,\n           [-1.3526,  2.1482,  0.5417,  ..., -2.2156, -1.9940, -0.9745],\n           [-0.5821,  0.0492,  0.6693,  ..., -0.8610,  0.5864, -0.6040],\n           [ 1.0180,  1.4447,  0.9563,  ...,  0.9034,  0.7988, -1.7119]],\n \n          [[-1.6146,  0.0868,  0.6415,  ...,  0.2083,  0.4058,  0.2813],\n           [ 0.1969, -0.3334, -0.6526,  ..., -1.4639, -1.6302, -0.6036],\n           [ 0.1556, -0.0859, -0.0230,  ..., -0.7900, -0.3481,  0.8767],\n           ...,\n           [ 0.6056,  0.8374, -0.3834,  ..., -0.6636, -0.4814,  0.8244],\n           [ 0.6982, -0.4884, -1.3777,  ...,  0.5876, -2.0944,  0.0853],\n           [ 0.0388, -0.5761, -0.5116,  ..., -1.6645,  0.1752, -0.1923]]]],\n        device='cuda:0'))\n\n\n\nscheduler.set_timesteps(num_inference_steps)\n\nscheduler.timesteps.shape, pipe.scheduler.timesteps\n\n(torch.Size([50]),\n tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n         856.2857, 835.8980, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n         713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n         570.8571, 550.4694, 530.0816, 509.6939, 489.3061, 468.9184, 448.5306,\n         428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n         285.4286, 265.0408, 244.6531, 224.2653, 203.8776, 183.4898, 163.1020,\n         142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n           0.0000], dtype=torch.float64))\n\n\n\n\n\n\n\\tilde{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{z}_t, \\mathbf{c}) = w\\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) + (1-w)\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t).\n\nHere, \\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) and \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t) are conditional and unconditional \\boldsymbol{\\epsilon}-predictions, given by \\boldsymbol{\\epsilon}_\\theta := (\\mathbf{z}_t - \\alpha_t\\hat{\\mathbf{x}}_\\theta)/\\sigma_t, and w is the guidance weight. Setting w = 1 disables classifier-free guidance, while increasing w &gt; 1 strengthens the effect of guidance.1\n\nfrom tqdm.auto import tqdm\n\nlatents = latents * scheduler.init_noise_sigma\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(\n            latent_model_input, t, encoder_hidden_states=text_embeddings\n        ).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = (\n        guidance_scale * noise_pred_text + (1 - guidance_scale) * noise_pred_uncond\n    )\n\n    # compute the previous noisy sample x_t -&gt; x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n\nfrom PIL import Image\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\n\n\nfor pil_image in pil_images:\n    display(pil_image)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#references",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#references",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "Patil et al.(2022) Stable Diffusion with  Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#footnotes",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#footnotes",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSaharia et al.(2022) Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, https://arxiv.org/abs/2205.11487"
  },
  {
    "objectID": "posts/spherical-astronomy/index.html",
    "href": "posts/spherical-astronomy/index.html",
    "title": "Spherical Astronomy",
    "section": "",
    "text": "This post is based on the project report that was written during the astronomical observation class in 2019."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#introduction",
    "href": "posts/spherical-astronomy/index.html#introduction",
    "title": "Spherical Astronomy",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is the purpose of this post?\nThe purpose of this post is to help you grasp the concept of the celestial sphere and the motions of celestial objects by utilizing the celestial sphere model. In detail, you will learn and understand that diurnal motion is the result of Earths rotation, and annual motion is the result of Earths revolution. Furthermore, you will gain insights into solar time, sidereal time, the celestial coordinate system (horizontal and equatorial), and coordinate conversion.\n\n\nWhy do you learn this?\nAll astronomical studies are based on the results of astronomical observations. The majority of these observations are conducted on Earth, with some exceptions. However, Earth is not an inertial frame of reference because it both rotates on its axis and orbits the Sun. Consequently, when we observe celestial objects from Earth, we inevitably observe apparent motion due to Earths own movements. Therefore, it is essential to identify the apparent motion in the motion of celestial objects. Additionally, all celestial objects are projected onto the celestial sphere, and as a result, their movements occur on the surface of this sphere. For these reasons, learning spherical astronomy is crucial for astronomers."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#celestial-sphere-model",
    "href": "posts/spherical-astronomy/index.html#celestial-sphere-model",
    "title": "Spherical Astronomy",
    "section": "Celestial Sphere Model",
    "text": "Celestial Sphere Model\n\nCelestial Sphere\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Celestial Sphere is an imaginary sphere with an infinite radius centered at the Earths center. While most motions of celestial objects are a result of Earths motion, for the sake of convenience, we assume that the Earth is fixed, and all celetial objects are embedded on the surface of the Celetial Sphere. We attribute the motions of celestial objects to the motion of the Celestial Sphere.\nSince the Celestial Sphere is a sphere defined by a set of points equidistant from its center, we designate certain special points or circles for convenience. There are various methods for naming these points and circles, but we typically use Earths Rotation, Earths Revolution, and Observer. For each of them, we begin by selecting a reference axis and a reference plane that is perpendicular to the axis. We then extend the axis and plane infinitely, resulting in three intersections (two points and one circle), each of which is given a specific name. If additional points or circles on the Celestial Sphere are required, we can refer to 9 intersections (6 points and 3 circles) for reference.\n\nEarths Rotation\n\nReference axis: Earths rotation axis\nReference plane: The plane containing Earths equator\n\n\nTwo points on the Celestial Sphere\n\nCelestial North Pole: The point near Earths North Pole\nCelestial South Pole: The point near Earths South Pole\n\n\n\nOne circle on the Celestial Sphere\n\nCelestial Equator: The great circle determined by the reference plane of Earths Rotation\n\n\n\n\nEarths Revolution\n\nReference axis: The axis parallel to Earths revolution axis and passing through Earths center\nReference plane: The plane containing Earths orbital path around the Sun\n\n\nTwo points on the Celestial Sphere\n\nEcliptic North Pole: The point near Earths North Pole\nEcliptic South Pole: The point near Earths South Pole\n\n\n\nOne circle on the Celestial Sphere\n\nEcliptic: The great circle determined by the reference plane of Earths Revolution\n\n\n\n\nObserver\n\nReference axis: The line perpendicular to the reference plane at the observers location\nReference plane: The plane parallel to the tangent plane of the Earth at the observers location and passing though Earths center\n\n\nTwo points on the Celestial Sphere\n\nZenith: The point near observers head\nNadir: The point near observers feet\n\n\n\nOne circle on the Celestial Sphere\n\nHorizon: The great circle determined by the reference plane of observer\n\n\n\n\nAdditional Points and Circles\n\nCircle on the Celestial Sphere\n\nMeridian: The great circle passing though the Celestial North Pole, the Celestial South Pole, the Zenith, and the Nadir\n\n\n\n4 points on the Ecliptic\n\nVernal Equinox : The point of intersection of the Ecliptic and the Celestial Equator, where the Sun moves from the celestial southern hemisphere to the celestial northern hemisphere\nSummer Solstice : The point on the Ecliptic nearest to the Celestial North Pole\nAutumnal Equinox : The point of intersection of the Ecliptic and the Celestial Equator, where the Sun moves from the celestial northern hemisphere to the celestial southern hemisphere\nWinter Solstice : The point on the Ecliptic nearest to the Celestial South Pole\n\n\n\n4 points on the Horizon\n\nNorth point : The point of intersection of the Horizon and the Meridian near the Celestial North Pole\nEast point : The point on the Horizon located 90 degree clockwise from the North point\nSouth point : The point of intersection of the Horizon and the Meridian near the Celestial South Pole\nWest point : The point on the Horizon located 90 degree clockwise from the South point\n\n\n\n\nAltitude of Celestial Poles\n\n\n\n\n\n\n\n(a) Northern hemisphere observer\n\n\n\n\n\n\n\n(b) Southern hemisphere observer\n\n\n\n\nFigure1: Schematic illustrations explaining the altitude of celestial poles\n\n\nFigure1 (a) represents a situation in which the observer is located in the northern hemisphere, while Figure1 (b) depicts a case in the southern hemisphere. In these diagrams, the blue line represents the horizon, the red line signifies the celestial equator, the black line is perpendicular to the horizon, the green line is perpendicular to the celestial equator, the orange point represents the observer, the green point represents Polaris, and \\phi denotes the observers latitude. From these figures, we can conclude that the altitude of celestial poles is equal to the observers latitude.\n\n\n\nEarths Rotation\nFirst, lets review the following concept for convenience.\n\n\n\n\n\n\n\n(a) Left-hand screw rule\n\n\n\n\n\n\n\n(b) Right-hand screw rule\n\n\n\n\nFigure2: Illustrations explaining the rotation direction\n\n\nFigure2 (a) illustrates the left-hand screw rule, while Figure2 (b) illustrates the right-hand screw rule. Imagine youre directly looking at your thumbs and curling your other fingers. If you focus on your left thumb, the rotation direction is clockwise. Conversely, if you focus on your right thumb, the rotation direction is counterclockwise. To determine the rotation direction, simply observe your thumbs, curl your other fingers, and match the fingers rotation direction with the given rotation direction: clockwise for the left hand and counterclockwise for the right hand.\n\n\n\n\n\nIn 3D space, the direction of a rotation vector is uniqe, but the rotation direction is not. Therefore, when describing a rotation, it is necessary to specify the viewpoint or indicate the direction of the rotation vector.\n\n\n\n\n\nIf your right thumb points in the direction of (Celestial) North Pole, then the Earths rotation follows right-hand screw rule. In other words, if you were to observe Earths North Pole from space, you would see that it rotates counterclockwise. Therefore, the direction of the Earths rotation vector is from South Pole to North Pole. Due to relative motion, the celestial sphere appears to rotate in the opposite direction around the Earth.\nWhen you rotate the celestial sphere model, you can observe that the Sun and all celestial objects rise in the east and set in the west in all hemispheres. Furthermore, youll notice that at the Suns upper culmination (its highest point), it appears in the southern sky in the northern hemisphere and in the northern sky in the southern hemisphere. At the poles, the Sun either remains either continuously visible or permanently hidden.\n\n\nSidereal Time\n\n\n\n\n\nThe (Local) Hour Angle is the inversely measured right ascension from the intersection of the celestial equator and the meridian above horizon. When the hour angle of a celestial object is zero, we call that the celestial object is at its upper culmination.\nThe (Local) Sidereal Time is the hour angle of the vernal equinox. By definition, sidereal time \\Theta of a celestial object is equal to H+\\alpha, where H is hour angle, and \\alpha is right ascension of the celestial object.\n\n\\begin{align*}\n\\text{Sidereal time} = \\Theta & = H_{\\text{vernal equinox}} \\\\\n& = H + \\alpha \\\\\n& = \\alpha_{\\text{celestial object at its upper culmination}}\n\\end{align*}\n\nThe (Local) Solar Time is calculated by adding the hour angle of the Sun to 12 hours.\n\n\\text{Solar time} = S = H_{\\odot} + \\text{12 h}\n\nTherefore, we get the following equation.\n\n\\begin{align*}\n\\text{Sidereal time} = \\Theta & = H_{\\odot} + \\alpha_{\\odot} \\\\\n& = S - \\text{12 h} + \\alpha_{\\odot}\n\\end{align*}\n\nThe right ascension of the Sun \\alpha_{\\odot} increases about +\\text{4 min} per day.\n\n\n\nTime\n\\alpha_{\\odot}\n\n\n\n\nVernal Equinox\n\\text{0 h}\n\n\nSummer Solstice\n\\text{6 h}\n\n\nAutumnal Equinox\n\\text{12 h}\n\n\nWinter Solstice\n\\text{18 h}\n\n\n\nThis solar time is actually local solar time. In Korea, the local solar time of Seoul is that of 127 E but our clock uses the local solar time of 135 E (UTC+9). As a result, there is a difference of about 8 (equivalent to 32 minutes). Therefore, in Seoul, the Sun is at its upper culmination at approximately 12:32 KST."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#spherical-coordinate-system",
    "href": "posts/spherical-astronomy/index.html#spherical-coordinate-system",
    "title": "Spherical Astronomy",
    "section": "Spherical Coordinate System",
    "text": "Spherical Coordinate System\n\nHorizontal System vs.Equatorial System\n\nDefinition of (Az, Alt) in Horizontal System\nAzimuth (Az, A) of a celestial object is the angle (or angular distance) measured commonly clockwise from the south point along the horizon. Azimuth values are typically within the range of [0, 360].\nAltitude (Alt, a) of a celestial object is the angle (or angular distance) measured from the horizon along the great circle passing through the celestial object and the zenith. Altitude values fall within the range [-90, +90]. A positive altitude indicates that the object is above the horizon, while a negative altitude indicates that the object is below the horizon.\n\n\nDefinition of (RA, Dec) in Equatorial System\nRight Ascension (RA, \\alpha) is a celestial longitude, equivalent to Earths longtiude, except that it is measured from the vernal equinox instead of Greenwich. It is measured only in the eartward direction.\nDeclination (Dec, \\delta) is a celestial latitude, equivalent to Earths latitude, except that N and S are respectively replaced by + and -.\n\n\nA formula to convert (RA, Dec) to (Az, Alt)\n\n\n\n\n\nIn the figure above, the point P has coordinates (x, y, z) in xyz Cartesian coordinates system, and (x', y', z') in x'y'z' Cartesian coordinates system. Each of these coordinates has the following relations with spherical coordinates in its respective frame.\n\n\\begin{align*}\nx & = \\cos \\theta \\cos \\psi \\\\\ny & = \\cos \\theta \\sin \\psi \\\\\nz & = \\sin \\theta\n\\end{align*}\n\n\n\\begin{align*}\nx' & = \\cos \\theta' \\cos \\psi' \\\\\ny' & = \\cos \\theta' \\sin \\psi' \\\\\nz' & = \\sin \\theta'\n\\end{align*}\n\nThe primed coordinates are related to the unprimed coordinates by the following equations.\n\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos \\chi & -\\sin \\chi \\\\\n0 & \\sin \\chi & \\cos \\chi\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n\n\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos \\chi & \\sin \\chi \\\\\n0 & -\\sin \\chi & \\cos \\chi\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n\n\n\n\n\n\nIn the figure above, we have \\chi = 90^{\\circ} - \\psi. The values of (x, y, z) and (x', y', z') are determined by the following equations.\n\n\\begin{align*}\nx & = \\cos a \\cos(90^{\\circ} - A) \\\\\n& = \\cos a \\sin A \\\\\ny & = \\cos a \\sin(90^{\\circ} - A) \\\\\n& = \\cos a \\cos A \\\\\nz & = \\sin a\n\\end{align*}\n\n\n\\begin{align*}\nx' & = \\cos \\delta \\cos(90^{\\circ} - H) \\\\\n& = \\cos \\delta \\sin H \\\\\ny' & = \\cos \\delta \\sin(90^{\\circ} - H) \\\\\n& = \\cos \\delta \\cos H \\\\\nz' & = \\sin \\delta\n\\end{align*}\n\nThus, we get the following equations.\n\n\\begin{align*}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos (90^{\\circ} - \\phi) & -\\sin (90^{\\circ} - \\phi) \\\\\n0 & \\sin (90^{\\circ} - \\phi) & \\cos (90^{\\circ} - \\phi)\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\sin \\phi & -\\cos \\phi \\\\\n0 & \\cos \\phi & \\sin \\phi\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n\\end{align*}\n\n\n\\begin{align*}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos (90^{\\circ} - \\phi) & \\sin (90^{\\circ} - \\phi) \\\\\n0 & -\\sin (90^{\\circ} - \\phi) & \\cos (90^{\\circ} - \\phi)\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\sin \\phi & \\cos \\phi \\\\\n0 & -\\cos \\phi & \\sin \\phi\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n\\end{align*}\n\nIn conclusion, we get the following two conversion fomulas.\nWhen we know (\\alpha=\\Theta-H, \\delta, \\phi), we can obtain (A, a) by using following equations.\n\n\\begin{align*}\n\\cos a \\sin A & = \\cos \\delta \\sin H \\\\\n\\cos a \\cos A & = \\sin \\phi \\cos \\delta \\cos H - \\cos \\phi \\sin \\delta \\\\\n\\sin a & = \\cos \\phi \\cos \\delta \\cos H + \\sin \\phi \\sin \\delta\n\\end{align*}\n\nWhen we know (A, a, \\phi), we can obtain (\\alpha=\\Theta-H, \\delta) by using following equations.\n\n\\begin{align*}\n\\cos \\delta \\sin H & = \\cos a \\sin A \\\\\n\\cos \\delta \\cos H & = \\sin \\phi \\cos a \\cos A + \\cos \\phi \\sin a \\\\\n\\sin \\delta & = - \\cos \\phi \\cos a \\cos A + \\sin \\phi \\sin a\n\\end{align*}\n\nTherefore, we can conclude that we need to know sidereal time and latitude when performing conversions between (RA, Dec) and (Az, Alt).\n\n\n\nTargets on Meridian\nLet \\alpha' be the right ascension of celestial object at its upper culmination. By definition of sidereal time, the following equation holds.\n\n\\begin{align*}\n\\Theta & = S - \\text{12 h} + \\alpha_\\odot \\\\\n& = \\alpha'\n\\end{align*}\n\nThus we get the following equation.\n\nS = \\text{12 h} - \\alpha_\\odot + \\alpha'\n\n\n\nHour Angle of Targets\nBy definition of sidereal time, the following equation holds.\n\n\\begin{align*}\n\\Theta & = S - \\text{12 h} + \\alpha_\\odot \\\\\n& = H + \\alpha\n\\end{align*}\n\nThus we get the following equation.\n\nH = S - \\text{12 h} + \\alpha_\\odot - \\alpha"
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#references",
    "href": "posts/spherical-astronomy/index.html#references",
    "title": "Spherical Astronomy",
    "section": "References",
    "text": "References\n\nIntroductory Astronomy: The Celestial Sphere, http://astro.wsu.edu/worthey/astro/html/lec-celestial-sph.html\nMeridian (astronomy), https://en.wikipedia.org/wiki/Meridian_(astronomy)\nRight hand screw rule, https://www3.eng.cam.ac.uk/~hemh1/gyroscopes/screwrule.html\nRotation Vs. Revolution: What Are The Differences?, https://differencecamp.com/rotation-vs-revolution\nSUPPLEMENT: MOTIONS IN THE SKY & COORDINATE SYSTEMS, https://rwoconne.github.io/rwoclass/astr1230/motions-coords.html\nHannu Karttunen et al.Fundamental Astronomy. Sixth Edition. Springer (2016)"
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html",
    "href": "posts/draw-1d-scalar-function/index.html",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "",
    "text": "If you know the mathematical formula of a 1D scalar function y=f(x), then I believe the best tool for drawing 1D scalar functions is Desmos. However, you can also plot the functions in Python using various visualization libraries. In this post, I will draw 1D scalar functions y=x^2 and y=\\sin(x) using basic features of these libraries. Keep in mind that there are many advanced features not covered here, so for more information, refer to the official document of the respective library."
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#sympy",
    "href": "posts/draw-1d-scalar-function/index.html#sympy",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "SymPy",
    "text": "SymPy\n\nSymPy is a Python library for symbolic mathematics.\n\nEven though SymPys strength lies in symbolic computations, it can also be used for drawing 1D scalar functions, see Figure1.\n\nfrom sympy import symbols, sin\nfrom sympy.plotting import plot \n\nx = symbols('x')\n\np1 = plot(x**2, (x, -2, 2), legend=True, show=False)\np2 = plot(sin(x), (x, -5, 5), legend=True, show=False)\np1.extend(p2)\np1.show()\n\n\n\n\nFigure1: A plot using sympy\n\n\n\n\nThe graph depicts y=x^2 for x \\in [-2, 2] and y=\\sin(x) for x \\in [-5, 5]. The purpose of using different ranges for x is to display both graphs in a single figure without one being much smaller than the other. This is because the values of x^2 rapidly increases as |x| increases, while |\\sin(x)| \\leq 1 always."
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#data-generation-using-numpy",
    "href": "posts/draw-1d-scalar-function/index.html#data-generation-using-numpy",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "Data generation using NumPy",
    "text": "Data generation using NumPy\n\nNumPy is the fundamental package for scientific computing with Python.\n\nOther visualization libraries usually dont understand symbolic representation of a function. They just draw (x, y) points in a coordinate plane. Therefore, before you use them, you have to generate (x, y) points using NumPy.\n\nimport numpy as np\n\nx1 = np.linspace(-2, 2, 100)\ny1 = x1**2\n\nx2 = np.linspace(-5, 5, 100)\ny2 = np.sin(x2)\n\nnp.linspace(start, stop, num) creates num evenly spaced numbers within a closed interval [start, stop]. So, x1 is an array containing 100 evenly spaced numbers within the interval [-2, 2], and x2 is the same array but within the interval [-5, 5].\n\nx1\n\narray([-2.        , -1.95959596, -1.91919192, -1.87878788, -1.83838384,\n       -1.7979798 , -1.75757576, -1.71717172, -1.67676768, -1.63636364,\n       -1.5959596 , -1.55555556, -1.51515152, -1.47474747, -1.43434343,\n       -1.39393939, -1.35353535, -1.31313131, -1.27272727, -1.23232323,\n       -1.19191919, -1.15151515, -1.11111111, -1.07070707, -1.03030303,\n       -0.98989899, -0.94949495, -0.90909091, -0.86868687, -0.82828283,\n       -0.78787879, -0.74747475, -0.70707071, -0.66666667, -0.62626263,\n       -0.58585859, -0.54545455, -0.50505051, -0.46464646, -0.42424242,\n       -0.38383838, -0.34343434, -0.3030303 , -0.26262626, -0.22222222,\n       -0.18181818, -0.14141414, -0.1010101 , -0.06060606, -0.02020202,\n        0.02020202,  0.06060606,  0.1010101 ,  0.14141414,  0.18181818,\n        0.22222222,  0.26262626,  0.3030303 ,  0.34343434,  0.38383838,\n        0.42424242,  0.46464646,  0.50505051,  0.54545455,  0.58585859,\n        0.62626263,  0.66666667,  0.70707071,  0.74747475,  0.78787879,\n        0.82828283,  0.86868687,  0.90909091,  0.94949495,  0.98989899,\n        1.03030303,  1.07070707,  1.11111111,  1.15151515,  1.19191919,\n        1.23232323,  1.27272727,  1.31313131,  1.35353535,  1.39393939,\n        1.43434343,  1.47474747,  1.51515152,  1.55555556,  1.5959596 ,\n        1.63636364,  1.67676768,  1.71717172,  1.75757576,  1.7979798 ,\n        1.83838384,  1.87878788,  1.91919192,  1.95959596,  2.        ])\n\n\n\\Delta x for this array is (2 - (-2)) / (100 - 1) = 0.\\overline{04}\n\nnp.isclose(np.diff(x1)[0], (2 - (-2)) / (100 - 1))\n\nTrue\n\n\n\nx2\n\narray([-5.        , -4.8989899 , -4.7979798 , -4.6969697 , -4.5959596 ,\n       -4.49494949, -4.39393939, -4.29292929, -4.19191919, -4.09090909,\n       -3.98989899, -3.88888889, -3.78787879, -3.68686869, -3.58585859,\n       -3.48484848, -3.38383838, -3.28282828, -3.18181818, -3.08080808,\n       -2.97979798, -2.87878788, -2.77777778, -2.67676768, -2.57575758,\n       -2.47474747, -2.37373737, -2.27272727, -2.17171717, -2.07070707,\n       -1.96969697, -1.86868687, -1.76767677, -1.66666667, -1.56565657,\n       -1.46464646, -1.36363636, -1.26262626, -1.16161616, -1.06060606,\n       -0.95959596, -0.85858586, -0.75757576, -0.65656566, -0.55555556,\n       -0.45454545, -0.35353535, -0.25252525, -0.15151515, -0.05050505,\n        0.05050505,  0.15151515,  0.25252525,  0.35353535,  0.45454545,\n        0.55555556,  0.65656566,  0.75757576,  0.85858586,  0.95959596,\n        1.06060606,  1.16161616,  1.26262626,  1.36363636,  1.46464646,\n        1.56565657,  1.66666667,  1.76767677,  1.86868687,  1.96969697,\n        2.07070707,  2.17171717,  2.27272727,  2.37373737,  2.47474747,\n        2.57575758,  2.67676768,  2.77777778,  2.87878788,  2.97979798,\n        3.08080808,  3.18181818,  3.28282828,  3.38383838,  3.48484848,\n        3.58585859,  3.68686869,  3.78787879,  3.88888889,  3.98989899,\n        4.09090909,  4.19191919,  4.29292929,  4.39393939,  4.49494949,\n        4.5959596 ,  4.6969697 ,  4.7979798 ,  4.8989899 ,  5.        ])\n\n\n\\Delta x for this array is (5 - (-5)) / (100 - 1) = 0.\\overline{10}\n\nnp.isclose(np.diff(x2)[0], (5 - (-5)) / (100 - 1))\n\nTrue\n\n\n(x1, y1) points are used to draw the graph of y=x^2, while (x2, y2) points are used for the graph of y=\\sin(x)."
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#matplotlib",
    "href": "posts/draw-1d-scalar-function/index.html#matplotlib",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nMatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n\nMatplotlib is one of the most popular visualization libraries in Python, see Figure2.\n\nimport matplotlib.pyplot as plt\n\nplt.plot(x1, y1, label=r'$x^2$')\nplt.plot(x2, y2, label=r'$\\sin x$')\nplt.legend()\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.show()\n\n\n\n\nFigure2: A plot using matplotlib"
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#pandas",
    "href": "posts/draw-1d-scalar-function/index.html#pandas",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "pandas",
    "text": "pandas\n\npandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nSince pandas is a great data analysis tool in Python, it can used for drawing graphs, see Figure3.\n\nimport pandas as pd \n\ndf1 = pd.DataFrame(data={'x':x1, 'y':y1})\ndf2 = pd.DataFrame(data={'x':x2, 'y':y2})\n\nax = df1.plot(x='x', y='y', label=r'$x^2$')\ndf2.plot(ax=ax, x='x', y='y', label=r'$\\sin x$')\nax.axvline(0, color='k')\nax.axhline(0, color='k')\nplt.show()\n\n\n\n\nFigure3: A plot using pandas"
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#plotly",
    "href": "posts/draw-1d-scalar-function/index.html#plotly",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "Plotly",
    "text": "Plotly\n\nPlotly is a technical computing company headquartered in Montreal, Quebec, that develops online data analytics and visualization tools. Plotly provides online graphing, analytics, and statistics tools for individuals and collaboration, as well as scientific graphing libraries for Python, R, MATLAB, Perl, Julia, Arduino, JavaScript and REST. #\n\nPlotly is a useful tool for creating interactive plots, see Figure4.\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x1, y=y1, mode='lines', name='x'))\nfig.add_trace(go.Scatter(x=x2, y=y2, mode='lines', name='sin(x)'))\nfig.show()\n\n\n\n                                                \nFigure4: A plot using plotly"
  },
  {
    "objectID": "posts/conservation-law/index.html",
    "href": "posts/conservation-law/index.html",
    "title": "",
    "section": "",
    "text": "2018    "
  },
  {
    "objectID": "posts/conservation-law/index.html#---",
    "href": "posts/conservation-law/index.html#---",
    "title": "",
    "section": "   ",
    "text": "   \n    .          .        ,          .    -()   .             (\\mathbf{F} = m \\mathbf{a} \\mathbf{F} (, net force) ). -     .\n\nW_{\\text{ }} = \\Delta K\n\n     ,       .           .\n\n-\\Delta U = \\Delta K\n\n     , K  U   .     ,            , U () .         .      .\n\nW_{\\text{}} = -\\Delta U_{\\text{}}\n\n       ,        .     ,      -      .\n\nW_{\\text{}} + W_{\\text{}} = \\Delta K\n \nW_{\\text{}} = \\Delta K + \\Delta U_{\\text{}}\n\n          .\n\nW_{\\text{}} = \\Delta E_{\\text{}}\n\n         .            .       .\n\n (Potential Energy) \n\n    \\mathbf{r}(t) (a\\leq t \\leq b) ,  a\\leq t \\leq b      C     \\mathbf{F}(\\mathbf{r}) (   \\mathbf{F}(\\mathbf{r})    C   ),  \\mathbf{F}    W   .  \\rm{d}\\mathbf{l} = \\rm{d}\\mathbf{r} = \\mathbf{r}'\\rm{d}t .\n\n\n\\begin{align*}\nW & = \\int_{C} \\mathbf{F} \\cdot \\rm{d}\\mathbf{l} \\\\\n& = \\int_{a}^{b} \\mathbf{F}(\\mathbf{r}(t)) \\cdot \\mathbf{r}'(t)\\rm{d}t\n\\end{align*}\n\n\n  \\mathbf{F} ,   \\mathbf{r}(t)   \\mathbf{F}(\\mathbf{r}(t)) ,     U(\\mathbf{r}(t))  \\mathbf{F}    .\n\n\n\\mathbf{F}(\\mathbf{r}) = -\\nabla U(\\mathbf{r})\n\n\n  \\mathbf{F} \\mathbf{F}(\\mathbf{r}) = - \\nabla U(\\mathbf{r})  ,  \\mathbf{r}(t)    a \\leq t \\leq b  \\mathbf{F}   C   ,     .\n\n\nU(\\mathbf{r}(b)) - U(\\mathbf{r}(a)) = - \\int_{C} \\mathbf{F} \\cdot \\rm{d}\\mathbf{l}\n\n\n  \\mathbf{F}   \\mathbf{r}   U(\\mathbf{r})       .  \\mathbf{r}^{*}   0   .\n\n\nU(\\mathbf{r}) = - \\int_{\\mathbf{r}^{*}}^{\\mathbf{r}} \\mathbf{F} \\cdot \\rm{d} \\mathbf{l}"
  },
  {
    "objectID": "posts/conservation-law/index.html#--",
    "href": "posts/conservation-law/index.html#--",
    "title": "",
    "section": "  ",
    "text": "  \n    \\mathbf{r}_{1}, \\mathbf{r}_{2}, \\cdots, \\mathbf{r}_{n} ,   m_{1}, m_{2}, \\cdots, m_{n} n    .      1, 2, \\cdots, n  .     , i     2   .  \\mathbf{F}_{\\text{}, i} i     n-1  i    , \\mathbf{F}_{\\text{}, i}     i    .  \\mathbf{v}_{i} = \\displaystyle \\frac{d}{dt} \\mathbf{r}_{i}.\n\n\\mathbf{F}_{\\text{}, i} + \\mathbf{F}_{\\text{}, i} = m_{i} \\frac{d}{dt} \\mathbf{v}_{i}\n\n   3 \n\n\\sum_{i=1}^{n} \\mathbf{F}_{\\text{}, i} = \\mathbf{0}\n\n ,        \n\nm_{i} \\frac{d}{dt} \\mathbf{v}_{i} = \\frac{d}{dt} (m_{i} \\mathbf{v}_{i})\n\n .  i         .  \\mathbf{F}_{\\text{}} = \\displaystyle \\sum_{i=1}^{n} \\mathbf{F}_{\\text{}, i}.\n\n\\mathbf{F}_{\\text{}} = \\frac{d}{dt} \\left( \\sum_{i=1}^{n} m_{i} \\mathbf{v}_{i} \\right)\n\n   \\mathbf{p} \\equiv m\\mathbf{v} ,  () .  i   \\mathbf{p}_{i} = m_{i}\\mathbf{v}_{i},          .      \\mathbf{P} = \\displaystyle \\sum_{i=1}^{n} m_{i} \\mathbf{v}_{i}    .\n\n\\mathbf{F}_{\\text{}} = \\frac{d}{dt} \\mathbf{P}\n\n \\mathbf{F}_{\\text{}} = \\mathbf{0} \\mathbf{P} = \\text{}   .       0,       .      ."
  },
  {
    "objectID": "posts/conservation-law/index.html#--",
    "href": "posts/conservation-law/index.html#--",
    "title": "",
    "section": "  ",
    "text": "  \n  \\mathbf{F}    \\mathbf{r} ,      ()   .\n\n\\boldsymbol{\\tau} = \\mathbf{r} \\times \\mathbf{F}\n\n   \\mathbf{F}      ,   2      .\n\n\\boldsymbol{\\tau} = \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt}\n\n  \n\n\\begin{align*}\n\\frac{d}{dt} (\\mathbf{r} \\times \\mathbf{p}) & = \\frac{d\\mathbf{r}}{dt} \\times \\mathbf{p} + \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt} \\\\\n& = \\mathbf{v} \\times m\\mathbf{v} + \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt} \\\\\n& = \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt}\n\\end{align*}\n\n       .  \\mathbf{l} = \\mathbf{r} \\times \\mathbf{p} .\n\n\\begin{align*}\n\\boldsymbol{\\tau} & = \\frac{d}{dt} (\\mathbf{r} \\times \\mathbf{p}) \\\\\n& = \\frac{d\\mathbf{l}}{dt}\n\\end{align*}\n\n      ,   2    .  \\mathbf{F}_{\\text{ext}}    , \\mathbf{P}     .\n\n\\mathbf{F}_{\\text{ext}} = \\frac{d\\mathbf{P}}{dt}\n\n \\mathbf{F}_{\\text{ext}}     \\boldsymbol{\\tau}_{\\text{ext}}   .  \\mathbf{L}     .\n\n\\boldsymbol{\\tau}_{\\text{ext}} = \\frac{d\\mathbf{L}}{dt}\n\n    \\mathbf{0}       ,     ."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Calculating Derivatives of a 1D Scalar Function in Python\n\n\n\n\n\n\n\nmath\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\n \n\n\n\n\n\n\n\nastronomy\n\n\nkorean\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nSpherical Astronomy\n\n\n\n\n\n\n\nastronomy\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nphysics\n\n\nkorean\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nHow to Draw 1D Scalar Functions in Python\n\n\n\n\n\n\n\nmath\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nMotion of Charged Particles in Magnetic Dipole Fields\n\n\n\n\n\n\n\nphysics\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion Inference with Diffusers (high-level)\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion Inference with Diffusers (low-level)\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Masters student at Kyung Hee University, studying solar physics and deep learning. My research interest includes magnetohydrodynamics, simulation, and scientific machine learning."
  },
  {
    "objectID": "about.html#mingyu-jeon-",
    "href": "about.html#mingyu-jeon-",
    "title": "About",
    "section": "",
    "text": "I am a Masters student at Kyung Hee University, studying solar physics and deep learning. My research interest includes magnetohydrodynamics, simulation, and scientific machine learning."
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html",
    "href": "posts/charged-particle-motion-in-dipole/index.html",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "",
    "text": "This post is based on the homework report that was written during the solar terrestrial physics class in 2022.\n\n\n\n\\mathbf{B} = -\\frac{\\mu_0}{4\\pi}\\left( \\frac{\\mathbf{m}}{r^3} - \\frac{3(\\mathbf{m}\\cdot\\mathbf{r})\\mathbf{r}}{r^5} \\right)\n\nIn (r, \\lambda, \\phi) coordinates, we describe the magnetic field as follows:\n\nB_r = Z = -\\frac{\\mu_0 m}{2\\pi}\\frac{\\sin\\lambda}{r^3}\n\n\nB_\\lambda = H = \\frac{\\mu_0 m}{4\\pi}\\frac{\\cos\\lambda}{r^3}\n\n\nB_\\phi = 0\n\nTherefore, the magnetic field \\mathbf{B} does not depend on longitude \\phi.\n\n\\mathbf{B} = \\mathbf{B}(r, \\lambda)\n\nThe strength B is given by:\n\nB = \\frac{\\mu_0 m}{4\\pi r^3}(1+3\\sin^2\\lambda)^{\\textstyle \\frac{1}{2}}\n\nFor the Earths magnetic field at the equator, denoted as B_E, it can be expressed as:\n\nB_E = \\frac{\\mu_0 m}{4\\pi R_E^3}\n\nThe actual value for B_E is approximately 0.31 Gauss (G).\nThe components of the magnetic field can be re-express in terms of B_E as follows:\n\nB_r = -\\frac{2B_E}{(r/R_E)^3}\\sin\\lambda\n\n\nB_\\lambda = \\frac{B_E}{(r/R_E)^3}\\cos\\lambda\n\n\nB_\\phi = 0\n\nThe magnetic field line in the meridian (when \\phi=\\text{const} or in the (r,\\lambda)-plane) is given by:\n\nr = r_\\text{eq} \\cos^2\\lambda\n\nHere, r_\\text{eq} = LR_E and L is called L-parameter.\n\nL-parameter describes the set of magnetic field lines which cross the Earths magnetic equator at a number of Earth-radii equal to the L-parameter. For example, L=2 describes the set of the Earths magnetic field lines which cross the Earths magnetic equator two earth radii from the center of the Earth.1\n\n\nThe dipole model of the Earths magnetic field is a first order approximation of the rather complex true Earths magnetic field. Due to effects of the interplanetary magnetic field (IMF), and the solar wind, the dipole model is particularly inaccurate at high L-shells (e.g., above L=3), but may be a good approximation for lower L-shells. For more precise work, or for any work at higher L-shells, a more accurate model that incorporates solar effects, such as the Tsyganenko magnetic field model, is recommended.2\n\n\n\\begin{align*}\n\\mathbf{B}(\\mathbf{r}) & = B_r(r, \\lambda) \\hat{r} + B_\\lambda (r, \\lambda) \\hat{\\lambda} \\\\\n& = B_x(x, y)\\hat{x} + B_y(x, y)\\hat{y}\n\\end{align*}\n\n\n\n\n\n\\frac{d^2\\mathbf{r}}{dt^2} = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}\n\n\n\\frac{d^2x}{dt^2}\\hat{x} + \\frac{d^2y}{dt^2}\\hat{y} + \\frac{d^2z}{dt^2}\\hat{z} = \\frac{q}{m}(\\hat{x}(v_yB_z - v_zB_y) + \\hat{y}(v_zB_x - v_xB_z) + \\hat{z}(v_xB_y - v_yB_x))\n\n\n\\frac{d^2x}{dt^2} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{d^2y}{dt^2} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{d^2z}{dt^2} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\n\\frac{dx}{dt} = v_x\n\n\n\\frac{dy}{dt} = v_y\n\n\n\\frac{dz}{dt} = v_z\n\n\n\\frac{dv_x}{dt} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{dv_y}{dt} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{dv_z}{dt} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\nS = (x, y, z, v_x, v_y, v_z)\n\n\n\\frac{dS}{dt} = (\\frac{dx}{dt}, \\frac{dy}{dt}, \\frac{dz}{dt}, \\frac{dv_x}{dt}, \\frac{dv_y}{dt},\\frac{dv_z}{dt})"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#theory",
    "href": "posts/charged-particle-motion-in-dipole/index.html#theory",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "",
    "text": "This post is based on the homework report that was written during the solar terrestrial physics class in 2022.\n\n\n\n\\mathbf{B} = -\\frac{\\mu_0}{4\\pi}\\left( \\frac{\\mathbf{m}}{r^3} - \\frac{3(\\mathbf{m}\\cdot\\mathbf{r})\\mathbf{r}}{r^5} \\right)\n\nIn (r, \\lambda, \\phi) coordinates, we describe the magnetic field as follows:\n\nB_r = Z = -\\frac{\\mu_0 m}{2\\pi}\\frac{\\sin\\lambda}{r^3}\n\n\nB_\\lambda = H = \\frac{\\mu_0 m}{4\\pi}\\frac{\\cos\\lambda}{r^3}\n\n\nB_\\phi = 0\n\nTherefore, the magnetic field \\mathbf{B} does not depend on longitude \\phi.\n\n\\mathbf{B} = \\mathbf{B}(r, \\lambda)\n\nThe strength B is given by:\n\nB = \\frac{\\mu_0 m}{4\\pi r^3}(1+3\\sin^2\\lambda)^{\\textstyle \\frac{1}{2}}\n\nFor the Earths magnetic field at the equator, denoted as B_E, it can be expressed as:\n\nB_E = \\frac{\\mu_0 m}{4\\pi R_E^3}\n\nThe actual value for B_E is approximately 0.31 Gauss (G).\nThe components of the magnetic field can be re-express in terms of B_E as follows:\n\nB_r = -\\frac{2B_E}{(r/R_E)^3}\\sin\\lambda\n\n\nB_\\lambda = \\frac{B_E}{(r/R_E)^3}\\cos\\lambda\n\n\nB_\\phi = 0\n\nThe magnetic field line in the meridian (when \\phi=\\text{const} or in the (r,\\lambda)-plane) is given by:\n\nr = r_\\text{eq} \\cos^2\\lambda\n\nHere, r_\\text{eq} = LR_E and L is called L-parameter.\n\nL-parameter describes the set of magnetic field lines which cross the Earths magnetic equator at a number of Earth-radii equal to the L-parameter. For example, L=2 describes the set of the Earths magnetic field lines which cross the Earths magnetic equator two earth radii from the center of the Earth.1\n\n\nThe dipole model of the Earths magnetic field is a first order approximation of the rather complex true Earths magnetic field. Due to effects of the interplanetary magnetic field (IMF), and the solar wind, the dipole model is particularly inaccurate at high L-shells (e.g., above L=3), but may be a good approximation for lower L-shells. For more precise work, or for any work at higher L-shells, a more accurate model that incorporates solar effects, such as the Tsyganenko magnetic field model, is recommended.2\n\n\n\\begin{align*}\n\\mathbf{B}(\\mathbf{r}) & = B_r(r, \\lambda) \\hat{r} + B_\\lambda (r, \\lambda) \\hat{\\lambda} \\\\\n& = B_x(x, y)\\hat{x} + B_y(x, y)\\hat{y}\n\\end{align*}\n\n\n\n\n\n\\frac{d^2\\mathbf{r}}{dt^2} = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}\n\n\n\\frac{d^2x}{dt^2}\\hat{x} + \\frac{d^2y}{dt^2}\\hat{y} + \\frac{d^2z}{dt^2}\\hat{z} = \\frac{q}{m}(\\hat{x}(v_yB_z - v_zB_y) + \\hat{y}(v_zB_x - v_xB_z) + \\hat{z}(v_xB_y - v_yB_x))\n\n\n\\frac{d^2x}{dt^2} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{d^2y}{dt^2} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{d^2z}{dt^2} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\n\\frac{dx}{dt} = v_x\n\n\n\\frac{dy}{dt} = v_y\n\n\n\\frac{dz}{dt} = v_z\n\n\n\\frac{dv_x}{dt} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{dv_y}{dt} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{dv_z}{dt} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\nS = (x, y, z, v_x, v_y, v_z)\n\n\n\\frac{dS}{dt} = (\\frac{dx}{dt}, \\frac{dy}{dt}, \\frac{dz}{dt}, \\frac{dv_x}{dt}, \\frac{dv_y}{dt},\\frac{dv_z}{dt})"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#code",
    "href": "posts/charged-particle-motion-in-dipole/index.html#code",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "Code",
    "text": "Code\n\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom matplotlib.animation import FuncAnimation\n\n\n# Be = 0.31 G = 0.31 * 10^-4 T\nBe = 0.31 * 1e-4\n# Re = 6371 km = 6371 * 10^3 m\nRe = 6371 * 1e3\nC = Be * (Re**3)\n\n\ndef B(x, y, z):\n    \"\"\"dipole field at (x, y, z)\"\"\"\n    r = np.sqrt(x**2 + y**2 + z**2)\n    Bx = -1 * C * (3 * x * z) / (r**5)\n    By = -1 * C * (3 * y * z) / (r**5)\n    Bz = C * (r**2 - 3 * z**2) / (r**5)\n    return Bx, By, Bz\n\n\ndef field_line_3D(phi, L=6.6):\n    \"\"\"dipole field line (3D)\"\"\"\n    phi = np.deg2rad(phi)\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    rf = L * np.sin(theta) ** 2\n    xf = rf * np.sin(theta) * np.cos(phi)\n    yf = rf * np.sin(theta) * np.sin(phi)\n    zf = rf * np.cos(theta)\n    return xf, yf, zf\n\n\ndef field_line_2D(L=6.6):\n    \"\"\"dipole field line (2D)\"\"\"\n    lamb = np.linspace(0, 2 * np.pi, 1000)\n    rf2 = L * np.cos(lamb) ** 2\n    xf2 = rf2 * np.cos(lamb)\n    zf2 = rf2 * np.sin(lamb)\n    return xf2, zf2\n\n\ndef dSdt(S, t, q_over_m):\n    \"\"\"dS/dt for odeint\"\"\"\n    x, y, z, vx, vy, vz = S\n    Bx, By, Bz = B(x, y, z)\n    dvxdt = q_over_m * (vy * Bz - vz * By)\n    dvydt = q_over_m * (vz * Bx - vx * Bz)\n    dvzdt = q_over_m * (vx * By - vy * Bx)\n    return [vx, vy, vz, dvxdt, dvydt, dvzdt]\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nxmin, xmax = -7, 7\nymin, ymax = -7, 7\n\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.legend()\nax.axhline(y=0, color=\"black\", linewidth=0.5)\nax.xaxis.grid(True, which=\"both\")\nax.set_xlim(xmin, xmax)\nax.set_ylim(ymin, ymax)\nmajors = np.arange(xmin + 1, xmax, 1)\nax.xaxis.set_major_locator(ticker.FixedLocator(majors))\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\n\nLvalues = [2, 4, 6, 8, 10, 20, 100]\ncolors = [\"r\", \"darkorange\", \"gold\", \"green\", \"blue\", \"magenta\", \"purple\"]\nlamb = np.linspace(0, 2 * np.pi, 1000)\nfor i, L in enumerate(Lvalues):\n    x, z = field_line_2D(L)\n    ax.plot(x, z, label=f\"L={L}\", color=colors[i])\n\nax.legend()\nax.set_title(\"Magnetic Dipole Field line (2D)\")\nplt.show()\n\n\n\n\n\nspecies = \"Proton\"\n\ne = 1.602e-19\nq = e  # C\n\nmH = 1.67e-27\nm = mH  # kg\nq_over_m = q / m\n\nE_keV = 2000  # keV\n\n\n# L-parameter\nL = 6.6\n\n# start at equator\nx0, y0, z0 = L * Re, 0, 0\n\nkeV_to_J = 1e3 * e  # J\n\n# particle energy (keV) and pitch angle\nE = E_keV * keV_to_J  # J\npitch_angle_deg = 30\nalpha = np.deg2rad(pitch_angle_deg)\n\n# particle velocity\nv0 = np.sqrt(2 * E / m)\n\n# vx = v_perp\n# vy = 0\n# vz = v_para\nvx0 = v0 * np.sin(alpha)\nvy0 = 0\nvz0 = v0 * np.cos(alpha)\n\n# bounce time_scale\nt_B = 290 * (np.pi * L / 10) * np.sqrt(m / (mH * E_keV))\nprint(f\"bounce time scale ~ {t_B} s\")\n\nS0 = [x0, y0, z0, vx0, vy0, vz0]\n\n# number of bounce\nn = 3\n\ntmin = 0\ntmax = n * t_B\n\nt = np.linspace(tmin, tmax, 1000)\n\n# solve ODE\nsol = odeint(dSdt, S0, t, args=(q_over_m,))\nx, y, z, vx, vy, vz = sol.T\nx, y, z = x / Re, y / Re, z / Re\n\nprint(f\"t_max ~ {tmax:.4f} s\")\n\nbounce time scale ~ 13.44549539521195 s\nt_max ~ 40.3365 s\n\n\n\ntrajectory_linewidth = 0.8\nfieldline_linewidth = 0.5\n\n\n# xyzrange = 10\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(projection=\"3d\")\nax.set_aspect(\"equal\")\n# ax.plot([-xyzrange,xyzrange], [0,0], [0, 0], color='black')\n# ax.plot([0,0], [-xyzrange,xyzrange], [0, 0], color='black')\n# ax.plot([0,0], [0,0], [-xyzrange, xyzrange], color='black')\nax.plot(\n    x,\n    y,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"y ($R_E$)\")\nax.set_zlabel(\"z ($R_E$)\")\nax.set_xlim(-L, L)\nax.set_ylim(-L, L)\nax.set_zlim(-L, L)\nfor az in np.arange(0, 361, 20):\n    xf, yf, zf = field_line_3D(az, L)\n    if az == 360:\n        ax.plot(\n            xf,\n            yf,\n            zf,\n            color=\"blue\",\n            linewidth=fieldline_linewidth,\n            zorder=-1,\n            label=f\"Magnetic Field (L={L})\",\n        )\n    else:\n        ax.plot(xf, yf, zf, color=\"blue\", linewidth=fieldline_linewidth, zorder=-1)\n\n# Sphere with radius Re\nu = np.linspace(0, 2 * np.pi, 1000)\nv = np.linspace(0, np.pi, 1000)\nxs = 1 * np.outer(np.cos(u), np.sin(v))\nys = 1 * np.outer(np.sin(u), np.sin(v))\nzs = 1 * np.outer(np.ones(np.size(u)), np.cos(v))\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.plot_surface(xs, ys, zs, color=\"white\", alpha=1)\nax.legend()\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.set_aspect(\"equal\")\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    y,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"y ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\ntheta = np.linspace(0, 2 * np.pi, 100)\nrc = L\nxc = rc * np.cos(theta)\nyc = rc * np.sin(theta)\nplt.plot(xc, yc, color=\"green\", zorder=-1, label=f\"Circle with radius L={L}\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend(loc=1)\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\n\nxf2, zf2 = field_line_2D(L)\nax.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend()\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\n\nxf2, zf2 = field_line_2D(L)\nax.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend()\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(16, 8))\nax1 = plt.subplot(121)\nax2 = plt.subplot(122, projection=\"3d\")\n\nxdata = x\nydata = y\nzdata = z\n\nsc1 = ax1.scatter([], [], color=\"green\")\n(ln1,) = ax1.plot([], [], \"r-\", zorder=99)\nsc2 = ax2.scatter([], [], [], color=\"green\")\n(ln2,) = ax2.plot([], [], [], \"r-\", zorder=99)\n\n# Field line (2D)\nxf2, zf2 = field_line_2D(L)\nax1.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\n\n# Earth (2D)\nax1.add_patch(plt.Circle((0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\"))\n\n# Field line (3D)\nfor az in np.arange(0, 361, 20):\n    xf, yf, zf = field_line_3D(az, L)\n    if az == 360:\n        ax2.plot(\n            xf,\n            yf,\n            zf,\n            color=\"blue\",\n            linewidth=fieldline_linewidth,\n            zorder=-1,\n            label=f\"Magnetic Field (L={L})\",\n        )\n    else:\n        ax2.plot(xf, yf, zf, color=\"blue\", linewidth=fieldline_linewidth, zorder=-1)\n\n# Earth (3D)\nu = np.linspace(0, 2 * np.pi, 1000)\nv = np.linspace(0, np.pi, 1000)\nxs = 1 * np.outer(np.cos(u), np.sin(v))\nys = 1 * np.outer(np.sin(u), np.sin(v))\nzs = 1 * np.outer(np.ones(np.size(u)), np.cos(v))\nax2.plot_surface(xs, ys, zs, color=\"white\", alpha=1)\n\n\ndef init():\n    ax1.set_aspect(\"equal\")\n    ax1.plot([-10, 10], [0, 0], color=\"black\")\n    ax1.plot([0, 0], [-10, 10], color=\"black\")\n    ax1.set_xlabel(\"x ($R_E$)\")\n    ax1.set_ylabel(\"z ($R_E$)\")\n    ax1.set_xlim(-L - 1, L + 1)\n    ax1.set_ylim(-L - 1, L + 1)\n\n    ax2.set_aspect(\"equal\")\n    ax2.plot([-10, 10], [0, 0], [0, 0], color=\"black\")\n    ax2.plot([0, 0], [-10, 10], [0, 0], color=\"black\")\n    ax2.plot([0, 0], [0, 0], [-10, 10], color=\"black\")\n    ax2.set_xlabel(\"x ($R_E$)\")\n    ax2.set_ylabel(\"y ($R_E$)\")\n    ax2.set_zlabel(\"z ($R_E$)\")\n    ax2.set_xlim(-L, L)\n    ax2.set_ylim(-L, L)\n    ax2.set_zlim(-L, L)\n\n    return ln1, ln2\n\n\ndef update(frame):\n    sc1.set_offsets([xdata[frame - 1], zdata[frame - 1]])\n    ln1.set_data(xdata[:frame], zdata[:frame])\n    ln1.set_label(f\"t={frame}\")\n    ax1.legend()\n\n    sc2._offsets3d = ([xdata[frame - 1]], [ydata[frame - 1]], [zdata[frame - 1]])\n    ln2.set_data(xdata[:frame], ydata[:frame])\n    ln2.set_3d_properties(zdata[:frame])\n    ln2.set_label(f\"t={frame}\")\n    ax2.legend()\n    return ln1, ln2\n\n\nani = FuncAnimation(\n    fig, update, frames=np.arange(1, len(xdata)), init_func=init, blit=True\n)\n\nani.save(\"simulation.gif\", fps=30)\n\n\n\nani.save(\"simulation.mp4\", fps=30)"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#footnotes",
    "href": "posts/charged-particle-motion-in-dipole/index.html#footnotes",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/L-shell\nhttps://en.wikipedia.org/wiki/Dipole_model_of_the_Earth%27s_magnetic_field"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html",
    "href": "posts/derivative-of-1d-scalar-function/index.html",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "",
    "text": "There are four techniques to compute derivatives: hand-coded analytical derivative, finite differentiation, symbolic differentiation, and automatic differentiation (Margossian 2019). In this post, I will demonstrate how to find the derivative of a simple 1D scalar function, f(x) = x^2 + \\sin(3x), using each of these four methods in Python within the interval x \\in [0, \\pi]."
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#hand-coded-analytical-derivative",
    "href": "posts/derivative-of-1d-scalar-function/index.html#hand-coded-analytical-derivative",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "1 Hand-coded analytical derivative",
    "text": "1 Hand-coded analytical derivative\nYou can find the analytical derivative of the fucntion f(x) = x^2 + \\sin(3x) using the table of derivatives learned in your Calculus course.\n\n\\begin{align*}\n\\frac{d}{dx} f(x) & = \\frac{d}{dx} (x^2 + \\sin(3x)) \\\\\n& = \\frac{d}{dx} x^2 + \\frac{d}{dx} \\sin(3x) \\\\\n& = 2x + 3\\cos(3x)\n\\end{align*}\n\nThus, for every x,\n\nf'(x) = 2x + 3\\cos(3x)\n\nAccording to the previous post where I explained how to draw a 1D scalar function in Python, I will plot f(x) and its derivative f'(x) using Matplotlib.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"Helvetica\",\n    \"font.size\": 15,\n    \"figure.figsize\": (8, 6)\n})\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfunc = lambda x: x**2 + np.sin(3*x)\nd_func = lambda x: 2*x + 3*np.cos(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\ndf_dx = d_func(x)\n\nplt.plot(x, f, label=r\"$f(x)$\")\nplt.plot(x, df_dx, label=r\"$f'(x)$\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#finite-differentiation",
    "href": "posts/derivative-of-1d-scalar-function/index.html#finite-differentiation",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "2 Finite differentiation",
    "text": "2 Finite differentiation\nYou can derive the following finite differentiation formulae using Taylors theorem. I will explain how to derive these formulae using SymPy in the another post.\n\n2.1 Finite differentiation formulae\n\nForward difference\n\n1st order accuracy \n  f'(x) = \\frac{f(x+h) - f(x)}{h} + \\mathcal{O}(h)\n  \n2nd order accuracy \n  f'(x) = \\frac{-3f(x) + 4f(x+h) - f(x+2h)}{2h} + \\mathcal{O}(h^2)\n  \n\nBackward difference\n\n1st order accuracy \n  f'(x) = \\frac{f(x) - f(x-h)}{h} + \\mathcal{O}(h)\n  \n2nd order accuracy \n  f'(x) = \\frac{3f(x) - 4f(x-h) + f(x-2h)}{2h} + \\mathcal{O}(h^2)\n  \n\nCentral difference\n\n2nd order accuracy \n  f'(x) = \\frac{f(x+h) - f(x-h)}{2h} + \\mathcal{O}(h^2)\n  \n\n\n\n\n2.2 Implementation using NumPy\nNumPy arrays make the implementation of finite differentitation very straightforward. Its important to note that at the left boundary (x=0), I use the forward difference, while at the right boundary (x=\\pi), I use the backward difference. Within the domain (0&lt;x&lt;\\pi), I use the central difference.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\n\n# calculate the spacing dx \ndx = x[1]-x[0]\n# you can use np.diff\n# dx = np.diff(x)[0] or\n# dx = np.mean(np.diff(x))\n\n# create an array with the same shape as `f`\ndf_dx = np.zeros_like(f)\n\n# forward difference (1st order) at the left boundary\ndf_dx[0] = (f[1] - f[0]) / dx\n\n# backward difference (1st order) at the right boundary\ndf_dx[-1] = (f[-1] - f[-2]) / dx\n\n# central difference (2nd accuracy) within the domain\ndf_dx[1:-1] = (f[2:] - f[:-2]) / (2*dx)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx, 'r--', lw=3, label=\"finite diff.\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\nIn the case of using 1st order accuracy formulae, the errors at the boundaries are\n\nprint('1st order')\nprint(f'dx   : {dx:.4f}')\nprint(f'left : {np.abs(df_dx[0] - df_dx_exact[0]):.4f}')\nprint(f'right: {np.abs(df_dx[-1] - df_dx_exact[-1]):.4f}')\n\n1st order\ndx   : 0.0317\nleft : 0.0272\nright: 0.0272\n\n\nIf we use the 2nd order accuracy formulae at the boundaries instead, we get the following errors:\n\n# forward difference (2nd order) at the left boundary\ndf_dx[0] = (-3*f[0] + 4*f[1] - f[2]) / (2*dx)\n\n# backward difference (2nd order) at the right boundary\ndf_dx[-1] = (3*f[-1] - 4*f[-2] + f[-3]) / (2*dx)\n\nprint('2nd order')\nprint(f'dx^2 : {dx**2:.4f}')\nprint(f'left : {np.abs(df_dx[0] - df_dx_exact[0]):.4f}')\nprint(f'right: {np.abs(df_dx[-1] - df_dx_exact[-1]):.4f}')\n\n2nd order\ndx^2 : 0.0010\nleft : 0.0090\nright: 0.0090\n\n\n\nnp.allclose(df_dx_exact, df_dx)\n\nFalse\n\n\n\nnp.allclose(df_dx_exact, df_dx, atol=1e-2)\n\nTrue\n\n\n\n\n2.3 Implementation using findiff\nThere is a convenient library for finite differentiation in Python: findiff\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom findiff import FinDiff\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\n\n# calculate the spacing dx \ndx = x[1]-x[0]\n\n# construct the differential operator: FinDiff(axis, spacing, degree)\nd_dx = FinDiff(0, dx, 1) \n\ndf_dx_findiff = d_dx(f)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx_findiff, 'r--', lw=3, label=\"finite diff. (findiff)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_findiff)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\nBy default, findiff uses 2nd order accuray.\n\nnp.allclose(df_dx, df_dx_findiff)\n\nTrue\n\n\nYou can also find finite difference coefficients using this library. (see Section2.1)\n\nimport findiff\n# coefficients of 2nd order accuracy formulae for 1st derivative\nfindiff.coefficients(deriv=1, acc=2)\n\n{'center': {'coefficients': array([-0.5,  0. ,  0.5]),\n  'offsets': array([-1,  0,  1]),\n  'accuracy': 2},\n 'forward': {'coefficients': array([-1.5,  2. , -0.5]),\n  'offsets': array([0, 1, 2]),\n  'accuracy': 2},\n 'backward': {'coefficients': array([ 0.5, -2. ,  1.5]),\n  'offsets': array([-2, -1,  0]),\n  'accuracy': 2}}\n\n\n\nnp.allclose(df_dx_exact, df_dx_findiff)\n\nFalse\n\n\n\nnp.allclose(df_dx_exact, df_dx_findiff, atol=1e-2)\n\nTrue\n\n\n\n\n2.4 Implementation using numdifftools\nThere is another convenient library for automatic numerical differentiation in Python: numdifftools\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numdifftools as nd\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\n# construct a derivative function (FD)\nd_func = nd.Derivative(func, n=1)\n\nx = np.linspace(0, np.pi, 100)\ndf_dx_numdifftools = d_func(x)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx_numdifftools, 'r--', lw=3, label=\"finite diff. (numdifftools)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_numdifftools)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\nSince this library uses an adaptive finite differences with a Richardson extrapolation methodology, the result is maximally accurate.\n\nnp.allclose(df_dx_exact, df_dx_numdifftools)\n\nTrue"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#symbolic-differentiation",
    "href": "posts/derivative-of-1d-scalar-function/index.html#symbolic-differentiation",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "3 Symbolic differentiation",
    "text": "3 Symbolic differentiation\n\n3.1 Implementation using Sympy\nBy using SymPy, we can symbolically differentiate f(x) = x^2 + \\sin(3x)\n\nfrom sympy import symbols, sin, diff\nx = symbols('x')\nf = x**2 + sin(3*x)\nf\n\n\\displaystyle x^{2} + \\sin{\\left(3 x \\right)}\n\n\n\ndf_dx_sympy = diff(f, x)\ndf_dx_sympy\n\n\\displaystyle 2 x + 3 \\cos{\\left(3 x \\right)}\n\n\nAs I mentioned in the previous post, SymPy also supports plotting of a function.\n\nfrom sympy.plotting import plot\np1 = plot(f, (x, 0, np.pi), legend=True, show=False, label=\"$f(x)$\", ylabel='')\np2 = plot(df_dx_sympy, (x, 0, np.pi), legend=True, show=False, label=r\"$f'(x)$\", ylabel='')\np1.extend(p2)\np1.show()"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#automatic-differentiation",
    "href": "posts/derivative-of-1d-scalar-function/index.html#automatic-differentiation",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "4 Automatic differentiation",
    "text": "4 Automatic differentiation\n\nIn mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program. #\n\nThe efficient implementation of automatic differentiation is quite challenging. However, since the backpropagation is used to minimize loss in neural networks and is essentially a reverse-mode automatic differentiation, most deep learning libraries natively support automatic differentiation tools. In this post, I will demonstrate how to use automatic differentiation in Python with TensorFlow, PyTorch, and JAX.\n\n4.1 Implementation using TensorFlow\n\nTensorFlow is an end-to-end open source platform for machine learning.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfunc = lambda x: x**2 + tf.math.sin(3*x)\n\nx = tf.linspace(0.0, tf.constant(np.pi), 100)\n\n# calculate derivatives of `f`\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    f = func(x)\n    df_dx_tf = tape.gradient(f, x)\n\ndf_dx_tf = df_dx_tf.numpy()\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\nx_numpy = x.numpy()\ndf_dx_exact = exact_d_func(x_numpy)\nplt.plot(x_numpy, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x_numpy, df_dx_tf, 'r--', lw=3, label=\"finite diff. (TensorFlow)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_tf)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show();\n\n\n\n\n\nnp.allclose(df_dx_exact, df_dx_tf)\n\nTrue\n\n\n\n\n4.2 Implementation using PyTorch\n\nPyTorch is a Python package that provides two high-level features: - Tensor computation (like NumPy) with strong GPU acceleration - Deep neural networks built on a tape-based autograd system\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\nfunc = lambda x: x**2 + torch.sin(3*x)\n\nx = torch.linspace(0, np.pi, 100)\n\n# calculate derivatives of `f`\nx.requires_grad = True\nf = func(x)\ndf_dx_torch = torch.autograd.grad(f, x, grad_outputs=torch.ones_like(f), \n                            retain_graph=True, create_graph=True, allow_unused=True)[0]\n\ndf_dx_torch = df_dx_torch.detach().numpy()\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\nx_numpy = x.detach().numpy()\ndf_dx_exact = exact_d_func(x_numpy)\nplt.plot(x_numpy, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x_numpy, df_dx_torch, 'r--', lw=3, label=\"finite diff. (PyTorch)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_torch)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\nnp.allclose(df_dx_exact, df_dx_torch)\n\nTrue\n\n\n\n\n4.3 Implementation using JAX\n\nJAX is Autograd and XLA, brought together for high-performance numerical computing, including large-scale machine learning research.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\nimport jax\n\nfunc = lambda x: x**2 + jnp.sin(3*x)\n\nx = jnp.linspace(0, jnp.pi, 100)\n\n# calculate derivatives of `f`\ndf_dx_jax = jax.vmap(jax.grad(func))(x)\n\ndf_dx_jax = np.array(df_dx_jax)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\nx_numpy = np.array(x)\ndf_dx_exact = exact_d_func(x_numpy)\nplt.plot(x_numpy, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x_numpy, df_dx_jax, 'r--', lw=3, label=\"finite diff. (JAX)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_jax)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\nnp.allclose(df_dx_exact, df_dx_jax)\n\nTrue"
  },
  {
    "objectID": "posts/explain-words/index.html",
    "href": "posts/explain-words/index.html",
    "title": " ",
    "section": "",
    "text": "2019   2020  09.     "
  },
  {
    "objectID": "posts/explain-words/index.html#-",
    "href": "posts/explain-words/index.html#-",
    "title": " ",
    "section": " ",
    "text": " \n\nTT, TCG, TCB, TDB\n\n\\text{TCG} = \\text{TT} + L_G \\times (\\text{JD} - 2443144.5) \\times 86400 \\ \\text{s}\n\n\n(\\text{TCB}-\\text{TCG})_{\\text{secular}} = L_C \\times (\\text{JD} - 2443144.5) \\times 86400 \\ \\text{s}\n\n\n\\text{TDB} = \\text{TCB} - L_B \\times (\\text{JD}_{\\text{TCB}} - T_0) \\times 86400 \\ \\text{s} + \\text{TDB}_0\n\n\n\\text{TDB}(T_0) - \\text{TCB}(T_0) = (\\text{TDB} - \\text{TCB} \\quad \\text{at} \\quad \\text{JD}_{\\text{TCB}} = T_0)\n\nSI(International System of Units) 1 (second)  -133        \\Delta \\nu_{\\text{Cs}} Hz      9 192 631 770  .  Hz s^{-1} .\nTAI(International Atomic Time)   , BIPM(International Bureau of Weights and Measures)    . TAI    SI.\n   (coordinate time) TT(Terrestrial Time) .       . TT    TAI    , TAI 1977 1 1 0 0 0 TT 1977 1 1 0 0 32.184 . ET(Ephemeris Time) TDT(Terrestrial Dynamic Time)  TDT TT  32.184 .\nGCRS(Geocentric Celestial Reference System)     , (near-Earth object, NEO)     . GCRS  TCG(Geocentric Coordinate Time) . TCG        ,      (proper time).\nBCRS(Barycentric Celestial Reference System)     ,       . BCRS  TCB(Barycentric Coordinate Time) . TCB       ,     (proper time).\nTDB(Barycentric Dynamical Time) 2006  TCB  .\n          .    \\rm{d}(\\text{TT}), \\rm{d}(\\text{TCG}), \\rm{d}(\\text{TCB}), \\rm{d}(\\text{TDB})  .\n\n\n(ERA)\n(ERA, Earth Rotation Angle) (Sidereal time)  ,      . rev revolutions() .\n\n1 \\text{ rev} = 1 \\text{ } = 360^\\circ\n\n\n\n(), \nGM     , G      .  GM  G, M     GM   .\n\n\n\nSI m() SI s()  , m    [\\text{TT}]  . \na_E = a_e\n\n\n\n,  \n  (spheroid)  .    ,     (spherical coordinate system)   (potential) \\phi   \\mu = GM_E    . \n\\phi = - \\frac{\\mu}{r} + \\sum_{n=2}^{\\infty} \\frac{J_n P_n(\\sin \\theta)}{r^{n+1}}\n\n P_n  (Legendre polynomial), (Rodrigues formula)  . \nP_n(x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2 - 1)^n\n\n\\phi   (dominating term) n=2 J_2 term, n \\geq 3   . J_2    .\n\\phi_{J_2 \\  \\text{term}} = \\frac{J_2 P_2(\\sin \\theta)}{r^3} = J_2 \\frac{3 \\sin^2 \\theta - 1}{2r^3}\n J_2   J_2, (Dynamical form-factor for the Earth) .   cy century(1, 100) .\nJ_2      .\n\nJ_2 = \\frac{2}{3}f - \\frac{a^3 \\omega^2}{3GM_E}\n\n f  , a  , \\omega   , GM_E  .\n\n\n \n(gravitational field) \\mathbf{g}  \\mathbf{g} = - \\nabla \\phi  (scalar field) \\phi  (potential) .\n  (equipotential surface) (geoid) .      .    ,      .    (  ,     )  .   W_0    . \nW_0 = c^2 L_G\n\n\n\n   (IERS 2010)\n     .     a,  b   .   (flattening) f    \nf = \\frac{a-b}{a}\n\nIERS International Earth Rotation and Reference Systems Service .\n\n\n ,  ,  ,  , \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(inertial frame of reference)      (Rotation), (Precession), (Nutation) .                      .   1.                     .    26000.                        .    18.61.\n (Hipparchus of Nicaea)    BC 127     ,     (precession of the equinoxes) .       , ,              .           (Lunisolar precession),            (Planetary precession) ,        (General Precession) .       ,       ,       2006 IAU   .  (Lunisolar precession)  (Precession of the equator),  (Planetary precession)  (Precession of the ecliptic)  .\n\n\n\n\n\n                 (  )    .  (path of mean celestial pole)   ,  (path of instantaneous celestial pole)          .         (General precession in longitude) p_A .        ,   (semi-major axis) (constant of nutation) N .\n\n\n\n\n\nEquinoctial colure  (Celestial North Pole),  (Celestial South Pole), (Vernal Equinox), (Autumnal Equinox)  . Solstitial colure  (Celestial North Pole),  (Celestial South Pole), (Summer Solstice), (Winter Solstice)  .\n  \\epsilon_A   (ecliptic of date)    (mean equator of date)  .   \\psi_A   solstital colure(solstital colure of date)  solstital colure(solstitial colure of epoch)  .   \\omega_A   (mean equator of date)   (fixed ecliptic of epoch)  . P03 precession model     t  2    .\n\n\\epsilon_A = 84381''.406 - 46''.836769t - 0''.0001831t^2\n\n\n\\psi_A = 5038''.481507t - 1''.0790069t^2\n\n\n\\omega_A = 84381''.406 - 0''.025754t + 0''.0512623t^2\n\n t = (\\text{TT} - \\text{2000 January 1d 12h TT})/36525 ( day ) J2000.0 TT    Julian century   .\nJ2000.0  (Rate of change in obliquity) \\dot{\\epsilon} \\epsilon_A t   t=0  , J2000.0 \\epsilon_A   .\nJ2000.0  (Precession of the equator in longitude) \\dot{\\psi} \\psi_A t   t=0  , J2000.0 \\psi_A   .\nJ2000.0  (Precession of the equator in obliquity) \\dot{\\omega} \\omega_A t   t=0  , J2000.0 \\omega_A   .\n\n\n \n   (apparent radius, angular radius)  (solar parallax) .\n\n\n \n\n\n\n\n\n(aberration)        .  v   .       ,            \\theta,            \\phi .  v/c \\ll 1   .\n\n\\kappa = \\theta - \\phi \\approx v/c\n\nv     \\kappa  (constant of aberration) ."
  },
  {
    "objectID": "posts/explain-words/index.html#-",
    "href": "posts/explain-words/index.html#-",
    "title": " ",
    "section": " ",
    "text": " \n\n, , , , , \n M   R      .\n\n\\text{} = \\frac{GM}{R^2}\n\n 1 \\text{ au}       . , r=1 \\text{ au}, L = \\text{}   .\n\n\\text{} = \\frac{L}{4 \\pi r^2}\n\n M   R      . \n\\text{} = \\sqrt{\\frac{2GM}{R}}\n\n             . \n\\text{} = T_e = \\left( \\frac{F_R}{\\sigma} \\right)^{1/4}\n\n      ( ) . \n\\text{} = L = \\text{}\n\n         . \n\\text{} = F_R = \\frac{L}{4 \\pi R^2}\n\n\n\n\n    ,   a,  b   (eccentricity) e   . \ne = \\frac{\\sqrt{a^2 - b^2}}{a}\n\n\n\n  \n      .     90^\\circ       .         (equatorial horizontal parallax) .\n     \\pi    R,     r    .\n\n\\pi = \\arcsin \\left( \\frac{R}{r} \\right)\n\n\n\n ()\n\n\n\n\n\n     2    (orbital node) ,      ,      (ascending node),     (descending node) .\n  ,      (Lunar node) .        ,       .             18.61.   (nodal period)  (draconic period) .\n\n\n \n(saros) , ,      ,  223 (synodic month).\n\n\n \n(galactic midplane) , (galactic longitude) \\ell      d .        (radial component)   (tangential component)   .\n\nv_r \\approx Ad\\sin(2\\ell)\n\n\nv_t \\approx Ad\\sin(2\\ell) + Bd\n\n A, B  (Oort constant)."
  },
  {
    "objectID": "posts/explain-words/index.html#",
    "href": "posts/explain-words/index.html#",
    "title": " ",
    "section": "",
    "text": "\n\n  \n\n\n\\text{  } \\rightarrow \\text{  }\n\n      .\n\n\n-(+) \n \n(S/E)(l+\\mu) \\rightarrow (S/E)(1+\\mu)\n\n\n\\because \\frac{S}{E+M_M} = \\frac{S}{E+\\mu E} = \\frac{S}{E}\\frac{1}{1+\\mu} = (S/E)(1+\\mu)\n\n\n\n\n \neV = \\frac{e}{c} J \\rightarrow \\text{eV} = (e/\\text{C}) \\text{ J}\n\n \\text{J}   (joule), (e/\\text{C}) (coulomb)    (elementary charge) ."
  },
  {
    "objectID": "posts/explain-words/index.html#",
    "href": "posts/explain-words/index.html#",
    "title": " ",
    "section": "",
    "text": "\n\nCAPITAINE, Nicole; WALLACE, Patrick T.; CHAPRONT, Jean. Expressions for IAU 2000 precession quantities. Astronomy & Astrophysics, 2003, 412.2: 567-586.\nCARROLL, Bradley W.; OSTLIE, Dale A. An introduction to modern astrophysics. Cambridge University Press, 2017.\nLIESKE, J.H., et al.Expressions for the precession quantities based upon the IAU/1976/system of astronomical constants. Astronomy and Astrophysics, 1977, 58: 1-16.\nMCCARTHY, Dennis D.; SEIDELMANN, P.Kenneth. Time: from Earth rotation to atomic physics. Cambridge University Press, 2018.\nVLGYESI, L. Physical backgrounds of Earths rotation, revision of the terminology. Acta Geodaetica et Geophysica Hungarica, 2006, 41.1: 31-44.\nBIPM - Time, https://www.bipm.org/en/bipm/tai/\nFIG Article of the Month - December 2004, https://www.fig.net/resources/monthly_articles/2004/beutLer_july_2004.asp\nFile:Lunar eclipse diagram-en.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Lunar_eclipse_diagram-en.svg\nFile:Praezession.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Praezession.svg#mw-jump-to-license\nFile:Simple stellar aberration diagram.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Simple_stellar_aberration_diagram.svg\nFundamental Physical Constants from NIST, https://pml.nist.gov/cuu/Constants/\nReference Earth Model - WGS84, https://topex.ucsd.edu/geodynamics/14gravity1_2.pdf\nThe Astronomical Almanac Online, http://asa.hmnao.com/index.html\n , https://www.kriss.re.kr/standard/view.do?pg=explanation_tab_02"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "Create an image using Diffusers library.\n\n\n\n!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)\n\n\n\n\n\npil_images = pipe(\n    prompt=prompt * batch_size,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images\n\n\nfor pil_image in pil_images:\n    display(pil_image)\n\n\n\n\n\n\n\n\n\n\n\nPatil et al.(2022) Stable Diffusion with  Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#install-and-import-libraries",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#install-and-import-libraries",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#high-level",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#high-level",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "pil_images = pipe(\n    prompt=prompt * batch_size,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images\n\n\nfor pil_image in pil_images:\n    display(pil_image)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#references",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#references",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "Patil et al.(2022) Stable Diffusion with  Diffusers, https://huggingface.co/blog/stable_diffusion"
  }
]