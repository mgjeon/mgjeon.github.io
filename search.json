[
  {
    "objectID": "posts/double-pendulum-python/index.html",
    "href": "posts/double-pendulum-python/index.html",
    "title": "The double pendulum problem in Python",
    "section": "",
    "text": "This post is based on an example of matplotlib."
  },
  {
    "objectID": "posts/double-pendulum-python/index.html#the-double-pendulum-problem",
    "href": "posts/double-pendulum-python/index.html#the-double-pendulum-problem",
    "title": "The double pendulum problem in Python",
    "section": "",
    "text": "This post is based on an example of matplotlib."
  },
  {
    "objectID": "posts/double-pendulum-python/index.html#code",
    "href": "posts/double-pendulum-python/index.html#code",
    "title": "The double pendulum problem in Python",
    "section": "Code",
    "text": "Code\n\nimport scipy\nimport numpy as np\nfrom numpy import cos, sin\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\ndef pendulum(t, u, M1, M2, L1, L2, G):\n    # u[0] = theta1\n    # u[1] = omega1\n    # u[2] = theta2\n    # u[3] = omega2\n    \n    du = np.zeros_like(u)\n\n    du[0] = u[1]\n    \n    delta = u[2] - u[0]\n    den1 = (M1 + M2) * L1 - M2 * L1 * cos(delta) * cos(delta)\n    du[1] = (\n        (\n            M2 * L1 * u[1] * u[1] * sin(delta) * cos(delta) + \n            M2 * G * sin(u[2]) * cos(delta) + \n            M2 * L2 * u[3] * u[3] * sin(delta) - (M1 + M2) * G * sin(u[0])\n        ) / den1\n    )\n    \n    du[2] = u[3]\n\n    den2 = (L2 / L1) * den1\n    du[3] = (\n        (\n            -M2 * L2 * u[3] * u[3] * sin(delta) * cos(delta) + \n            (M1 + M2) * G * sin(u[0]) * cos(delta) - \n            (M1 + M2) * L1 * u[1] * u[1] * sin(delta) - (M1 + M2) * G * sin(u[2])\n        ) / den2\n    )\n    \n    return du\n\n\nG = 9.8      # acceleration due to gravity, in m/s^2\nL1 = 1.0     # length of pendulum 1 in m\nL2 = 1.0     # length of pendulum 2 in m\nL = L1 + L2  # maximal length of the combined pendulum\nM1 = 1.0     # mass of pendulum 1 in kg\nM2 = 1.0     # mass of pendulum 2 in kg\nt_stop = 5.0   # how many seconds to simulate\n\n\n# th1 and th2 are the initial angles (degrees)\n# w10 and w20 are the initial angular velocities (degrees per second)\nth1 = 120.0\nw1 = 0.0\nth2 = -10.0\nw2 = 0.0\n\n\nu0 = np.radians([th1, w1, th2, w2])\nu0\n\narray([ 2.0943951 ,  0.        , -0.17453293,  0.        ])\n\n\n\np = (M1, M2, L1, L2, G)\np\n\n(1.0, 1.0, 1.0, 1.0, 9.8)\n\n\n\ntspan = (0.0, t_stop)\ntspan\n\n(0.0, 5.0)\n\n\n\nres = %timeit -o scipy.integrate.solve_ivp(pendulum, tspan, u0, args=p, dense_output=True, \\\n                                        method='DOP853', rtol=1e-10, atol=1e-10)\n\n63.3 ms ± 839 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nprint(f\"{res.best:.6f} seconds\")\n\n0.062364 seconds\n\n\n\nsol = scipy.integrate.solve_ivp(pendulum, tspan, u0, args=p, dense_output=True, \\\n                                        method='DOP853', rtol=1e-10, atol=1e-10)\n\n\nsol\n\n  message: The solver successfully reached the end of the integration interval.\n  success: True\n   status: 0\n        t: [ 0.000e+00  2.553e-02 ...  4.985e+00  5.000e+00]\n        y: [[ 2.094e+00  2.091e+00 ... -3.871e-01 -4.402e-01]\n            [ 0.000e+00 -2.555e-01 ... -3.738e+00 -3.521e+00]\n            [-1.745e-01 -1.761e-01 ...  7.104e+00  7.050e+00]\n            [ 0.000e+00 -1.201e-01 ... -3.549e+00 -3.909e+00]]\n      sol: &lt;scipy.integrate._ivp.common.OdeSolution object&gt;\n t_events: None\n y_events: None\n     nfev: 4004\n     njev: 0\n      nlu: 0\n\n\n\nt = np.linspace(0, t_stop, 500)\nt.shape\n\n(500,)\n\n\n\ns = sol.sol(t)\ns\n\narray([[ 2.0943951 ,  2.09389273,  2.09238573, ..., -0.36666802,\n        -0.40417351, -0.44021975],\n       [ 0.        , -0.10027181, -0.20051768, ..., -3.81247888,\n        -3.6719432 , -3.52113898],\n       [-0.17453293, -0.17477032, -0.1754812 , ...,  7.12298826,\n         7.08752942,  7.04957895],\n       [ 0.        , -0.04736169, -0.09446566, ..., -3.41150939,\n        -3.66460357, -3.90882817]])\n\n\n\ns.shape\n\n(4, 500)\n\n\n\n# theta1\ns[0, :] \n\narray([ 2.0943951 ,  2.09389273,  2.09238573,  2.08987451,  2.08635972,\n        2.08184231,  2.07632352,  2.0698049 ,  2.06228836,  2.05377617,\n        2.04427099,  2.03377593,  2.02229454,  2.00983085,  1.99638943,\n        1.98197535,  1.96659427,  1.95025238,  1.93295643,  1.91471374,\n        1.89553213,  1.8754199 ,  1.85438575,  1.83243875,  1.80958814,\n        1.78584331,  1.76121353,  1.73570785,  1.70933481,  1.68210224,\n        1.6540169 ,  1.62508422,  1.59530785,  1.5646893 ,  1.53322745,\n        1.500918  ,  1.46775295,  1.43371989,  1.39880128,  1.36297364,\n        1.32620657,  1.28846171,  1.24969137,  1.20983715,  1.1688281 ,\n        1.12657866,  1.08298619,  1.03792806,  0.99125831,  0.94280406,\n        0.89236193,  0.83969567,  0.78453699,  0.72659457,  0.66557999,\n        0.60126609,  0.53359691,  0.46285337,  0.38981596,  0.31576919,\n        0.2422171 ,  0.17044198,  0.10122448,  0.03485021, -0.02872957,\n       -0.08973502, -0.14844932, -0.20516057, -0.26013525, -0.31360838,\n       -0.36578172, -0.41682532, -0.46688021, -0.51606127, -0.56445994,\n       -0.6121465 , -0.65917205, -0.70557024, -0.75135872, -0.79654052,\n       -0.84110525, -0.88503034, -0.92828229, -0.97081783, -1.01258522,\n       -1.05352556, -1.0935741 , -1.13266158, -1.17071557, -1.20766181,\n       -1.24342545, -1.27793226, -1.31110977, -1.34288826, -1.37320165,\n       -1.40198827, -1.42919143, -1.45475989, -1.47864818, -1.50081672,\n       -1.52123181, -1.53986552, -1.55669543, -1.57170418, -1.58487908,\n       -1.59621142, -1.60569592, -1.61332996, -1.6191128 , -1.62304481,\n       -1.62512658, -1.62535807, -1.62373765, -1.62026121, -1.6149211 ,\n       -1.60770518, -1.59859567, -1.58756812, -1.57459027, -1.55962099,\n       -1.54260932, -1.52349393, -1.50220318, -1.47865646, -1.45276789,\n       -1.42445376, -1.39364601, -1.36031435, -1.32449866, -1.28634925,\n       -1.24616428, -1.20440261, -1.16164927, -1.11853126, -1.07561487,\n       -1.03333011, -0.99194588, -0.95158759, -0.91227401, -0.87395459,\n       -0.83653893, -0.7999169 , -0.76397119, -0.72858459, -0.69364384,\n       -0.65904154, -0.62467677, -0.59045515, -0.55628865, -0.52209513,\n       -0.48779804, -0.45332606, -0.4186129 , -0.38359716, -0.3482223 ,\n       -0.31243678, -0.27619425, -0.23945387, -0.20218073, -0.16434633,\n       -0.12592914, -0.0869152 , -0.04729873, -0.00708275,  0.03372036,\n        0.07508832,  0.11698858,  0.15937799,  0.20220264,  0.24539788,\n        0.28888842,  0.33258868,  0.37640325,  0.4202275 ,  0.46394834,\n        0.50744509,  0.55059048,  0.59325168,  0.63529154,  0.67656977,\n        0.71694425,  0.75627242,  0.7944127 ,  0.83122582,  0.86657636,\n        0.90033402,  0.93237501,  0.96258314,  0.99085089,  1.01708017,\n        1.04118281,  1.06308089,  1.08270659,  1.10000181,  1.1149175 ,\n        1.12741261,  1.1374528 ,  1.14500888,  1.15005494,  1.15256639,\n        1.15251759,  1.14987941,  1.14461641,  1.1366838 ,  1.12602401,\n        1.1125629 ,  1.0962055 ,  1.07683152,  1.05429082,  1.02839993,\n        0.99894175,  0.9656734 ,  0.92835223,  0.88679762,  0.84101338,\n        0.79138105,  0.73885916,  0.68499537,  0.63158026,  0.58011461,\n        0.53149531,  0.48605199,  0.44374855,  0.40436928,  0.36763489,\n        0.33326142,  0.30098534,  0.27057186,  0.24181546,  0.21453737,\n        0.18858212,  0.16381405,  0.1401142 ,  0.11737756,  0.0955109 ,\n        0.07443082,  0.05406232,  0.03433748,  0.01519455, -0.00342288,\n       -0.02156646, -0.03928357, -0.05661762, -0.07360829, -0.0902916 ,\n       -0.10669994, -0.12286203, -0.13880274, -0.15454298, -0.17009946,\n       -0.18548448, -0.20070577, -0.21576636, -0.23066455, -0.24539396,\n       -0.25994378, -0.27429912, -0.28844156, -0.3023498 , -0.31600048,\n       -0.32936909, -0.34243085, -0.35516168, -0.36753904, -0.37954268,\n       -0.39115531, -0.40236298, -0.41315547, -0.42352641, -0.43347327,\n       -0.44299728, -0.45210323, -0.4607992 , -0.4690963 , -0.47700832,\n       -0.48455145, -0.491744  , -0.49860613, -0.50515959, -0.51142757,\n       -0.51743452, -0.52320599, -0.52876861, -0.53414994, -0.53937846,\n       -0.5444836 , -0.54949568, -0.55444597, -0.55936673, -0.56429129,\n       -0.56925413, -0.57429096, -0.57943887, -0.58473648, -0.59022408,\n       -0.59594385, -0.60194007, -0.6082594 , -0.61495118, -0.62206774,\n       -0.62966485, -0.63780211, -0.6465435 , -0.65595792, -0.66611976,\n       -0.67710953, -0.68901444, -0.70192871, -0.71595368, -0.73119705,\n       -0.74777081, -0.76578689, -0.78534865, -0.80653616, -0.82938205,\n       -0.85383544, -0.87971524, -0.90666289, -0.93411881, -0.96135316,\n       -0.98756199, -1.0119963 , -1.03406466, -1.05337146, -1.06969796,\n       -1.08295641, -1.09314302, -1.1003016 , -1.1044994 , -1.10581283,\n       -1.1043198 , -1.10009618, -1.09321472, -1.08374525, -1.07175553,\n       -1.05731244, -1.04048307, -1.02133584, -0.99994134, -0.97637299,\n       -0.95070745, -0.92302469, -0.89340787, -0.86194288, -0.82871768,\n       -0.79382135, -0.75734304, -0.7193706 , -0.67998921, -0.63927981,\n       -0.59731756, -0.55417017, -0.50989631, -0.46454401, -0.41814905,\n       -0.37073346, -0.32230404, -0.27285094, -0.22234635, -0.17074336,\n       -0.11797526, -0.0639554 , -0.00857858,  0.04827514,  0.10673204,\n        0.16690872,  0.22888411,  0.29265532,  0.3580776 ,  0.42480159,\n        0.4922419 ,  0.55962136,  0.62610289,  0.69095463,  0.75366322,\n        0.81395537,  0.87175247,  0.92710381,  0.98012854,  1.03097537,\n        1.07979849,  1.12674457,  1.17194683,  1.21552287,  1.2575747 ,\n        1.29818975,  1.33744224,  1.37539479,  1.41209987,  1.44760125,\n        1.48193522,  1.51513175,  1.54721545,  1.5782065 ,  1.60812134,\n        1.6369734 ,  1.66477366,  1.69153112,  1.71725328,  1.74194649,\n        1.76561623,  1.78826743,  1.80990466,  1.83053226,  1.85015457,\n        1.86877597,  1.88640095,  1.90303423,  1.9186807 ,  1.93334552,\n        1.94703404,  1.95975182,  1.97150461,  1.9822983 ,  1.99213885,\n        2.00103229,  2.00898462,  2.01600181,  2.02208969,  2.02725394,\n        2.03150002,  2.03483311,  2.03725809,  2.03877947,  2.03940138,\n        2.03912749,  2.03796103,  2.03590473,  2.03296079,  2.02913088,\n        2.02441611,  2.01881703,  2.01233359,  2.00496515,  1.99671047,\n        1.98756769,  1.97753431,  1.96660722,  1.95478263,  1.9420561 ,\n        1.92842246,  1.91387583,  1.89840954,  1.88201609,  1.86468708,\n        1.84641313,  1.82718378,  1.80698732,  1.7858107 ,  1.76363931,\n        1.74045677,  1.71624468,  1.6909823 ,  1.66464622,  1.63720998,\n        1.60864353,  1.57891279,  1.547979  ,  1.5157981 ,  1.48232003,\n        1.44748799,  1.41123784,  1.37349749,  1.33418677,  1.29321779,\n        1.25049656,  1.20592627,  1.15941378,  1.11088037,  1.06027866,\n        1.00761653,  0.95298659,  0.89659479,  0.83877539,  0.77997726,\n        0.72071458,  0.66149456,  0.60274982,  0.54479927,  0.48784309,\n        0.43198138,  0.37724196,  0.32360712,  0.27103489,  0.21947412,\n        0.16887442,  0.11919239,  0.07039531,  0.0224631 , -0.02461066,\n       -0.07081848, -0.11613911, -0.16053801, -0.20396806, -0.24637036,\n       -0.28767532, -0.32780373, -0.36666802, -0.40417351, -0.44021975])\n\n\n\n# u[0] = theta1\n# u[1] = omega1\n# u[2] = theta2\n# u[3] = omega2\n\nx1 = +L1 * sin(s[0, :])\ny1 = -L1 * cos(s[0, :])\n\nx2 = +L2 * sin(s[2, :]) + x1\ny2 = -L2 * cos(s[2, :]) + y1\n\n\nplt.plot(x2, y2)\nplt.grid(linewidth=1)\nplt.gca().set_aspect(\"equal\")\nplt.xlabel(\"x2\")\nplt.ylabel(\"y2\")\nplt.xlim((-2.0, 2.0))\nplt.ylim((-2.0, 2.0))\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(5, 4))\nax = fig.add_subplot(autoscale_on=False, xlim=(-L, L), ylim=(-L, 1.))\nax.set_aspect('equal')\nax.grid()\n\nline, = ax.plot([], [], 'o-', lw=2, markerfacecolor=\"red\", markeredgecolor=\"black\")\ntrace, = ax.plot([], [], '.-', lw=1, ms=2)\ntime_template = 'time = %.1f s'\ntime_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n\ndef animate(i):\n    this_x = [0, x1[i], x2[i]]\n    this_y = [0, y1[i], y2[i]]\n    \n    history_x = x2[:i]\n    history_y = y2[:i]\n    \n    line.set_data(this_x, this_y)\n    trace.set_data(history_x, history_y)\n    time_text.set_text(time_template % (t[i]))\n    return line, trace, time_text\n\nn = len(x1)\nframes = np.arange(0, n, 10)\nani = animation.FuncAnimation(fig, animate, frames, blit=True)\n\n\nani.save(\"ani_python.mp4\", fps=10)"
  },
  {
    "objectID": "posts/low_lou_force_free/index.html",
    "href": "posts/low_lou_force_free/index.html",
    "title": "Low and Lou (1990) force free magnetic fields",
    "section": "",
    "text": "Force free magnetic fields are defined as magnetic fields without Lorentz force\n\\begin{align*}\n\\mathbf{J} \\times \\mathbf{B} & = \\mathbf{0} \\\\\n\\nabla \\cdot \\mathbf{B} & = 0\n\\end{align*}\nIn magnetohydrodynamics (MHD), the current density \\mathbf{J} is given by (in SI units)\n\\mathbf{J} = \\frac{1}{\\mu_0} \\nabla \\times \\mathbf{B}\nTherefore, force free magnetic fields are determined by the following partial differential equations (PDEs)\n\\begin{align*}\n\\mathbf{(\\nabla \\times \\mathbf{B})} \\times \\mathbf{B} & = \\mathbf{0} \\\\\n\\nabla \\cdot \\mathbf{B} & = 0\n\\end{align*}\nThe analytical solution of these PDEs in a general case is unknown. But Low and Lou (1990) shows that we can calculate “axisymmetric” force free fields. If we rotate the plane perpendicular to the axis of symmetry, we can generate a quite general force free fields. In this post, I try to calculate this Low and Lou fields, referencing this code."
  },
  {
    "objectID": "posts/low_lou_force_free/index.html#low-lou-ode",
    "href": "posts/low_lou_force_free/index.html#low-lou-ode",
    "title": "Low and Lou (1990) force free magnetic fields",
    "section": "Low-Lou ODE",
    "text": "Low-Lou ODE\nTo calculate Low and Lou fields, we have to solve the following ordinary differential equation (ODE).\n\\begin{cases}\n\\displaystyle (1-\\mu^2)\\frac{d^2 P}{d\\mu^2} + n(n+1)P + a^2 \\frac{1+n}{n}P^{1 + \\frac{2}{n}} = 0 \\\\\n\\mu = \\cos\\theta \\in [-1, 1] \\\\\nP(-1) = 0 \\\\\nP(1) = 0 \\\\\n\\\\\nP'(-1) = 10 \\text{ for numerical normalization}\n\\end{cases}\n\nFor fixed n, a serves as a eignvalue for this homogenous BVP.\nFor n=1, we can list the positive eigenvalues in ascending order with m=0, 1, 2, ....\nWe denote a eigenvalue as a^2 _{n, m} and the corresponding eigenfunction as P_{n, m}.\nFor n=1, m=0, 1, 2,\n\n\\begin{align}\na^2 _{1, 0} &= 0 \\\\\na^2 _{1, 1} &= 0.425 \\\\\na^2 _{1, 2} &= 2.55 \\\\\n\\end{align}\n\nIf you carefully see the Figure 1 by Low and Lou (1990), we can notice that\nP_{1,0} (\\mu) \\sim \\cos\\left(\\displaystyle\\frac{\\pi}{2} \\mu\\right)\nP_{1,1} (\\mu) \\sim -\\sin\\left(\\displaystyle\\pi \\mu\\right)\nP_{1,2} (\\mu) \\sim -\\cos\\left(\\displaystyle\\frac{3\\pi}{2} \\mu\\right)\nThen we can generally say that\nFor m=0, 2 (even m)\nP_{1,m} (\\mu) \\sim \\cos\\left(\\displaystyle\\frac{(m + 1)\\pi}{2} \\mu\\right)\nFor m=1 (odd m) P_{1,m} (\\mu) \\sim \\sin\\left(\\displaystyle\\frac{(m + 1)\\pi}{2} \\mu\\right)\nThis is the initial guess of the solution.\n\nRewrite Low-Lou ODE using \\mathbf{S}(\\mu)\n\n(1-\\mu^2)P'' + n(n+1)P + a^2 \\frac{1+n}{n}P^{1 + \\frac{2}{n}} = 0\n\n\n\\rightarrow P'' =  \\frac{1}{1-\\mu^2}\\left[- n(n+1)P - a^2 \\frac{1+n}{n}P^{1 + \\frac{2}{n}}\\right]\n\n\n\\rightarrow P'' =  \\frac{-1}{1-\\mu^2 + \\epsilon}\\left[n(n+1)P + a^2 \\frac{1+n}{n}P^{1 + \\frac{2}{n}}\\right]\n\nwhere \\epsilon = 10^{-6} for numerical stability.\nTarget y \n\\mathbf{S}(\\mu) = \\begin{bmatrix}\n                     P(\\mu) \\\\\n                     P'(\\mu)\n                 \\end{bmatrix}\n              = \\begin{bmatrix}\n                     y[0] \\\\\n                     y[1]\n                 \\end{bmatrix}\n\nODE system F(x, y) \n\\frac{d\\mathbf{S}}{d\\mu} = \\mathbf{F}(\\mu, \\mathbf{S}(\\mu))\n                         = \\begin{bmatrix}\n                             P'(\\mu) \\\\\n                             P''(\\mu)\n                           \\end{bmatrix}\n                         = \\begin{bmatrix}\n                             y[1] \\\\\n                            \\displaystyle \\frac{-1}{1-\\mu^2 + \\epsilon}\\left[n(n+1)P + a^2 \\frac{1+n}{n}P^{1 + \\frac{2}{n}}\\right]\n                           \\end{bmatrix}   \n\n\\begin{cases}\n\\mu = \\cos\\theta \\in [-1, 1] \\\\\nP(-1) = 0 \\\\\nP(1) = 0 \\\\\n\\end{cases}\n\n\nP'(-1) = 10\n\nDomain\nmu_span = [-1, 1]\nN = 100 # number of points\nmu = np.linspace(mu_span[0], mu_span[1], N)\nBoundary condition function bc defined from ya, yb\n\n\\text{ya} = \\mathbf{S}(-1) = \\begin{bmatrix}\n                                 P(-1) \\\\\n                                 P'(-1)\n                             \\end{bmatrix}\n                          = \\begin{bmatrix}\n                                 0 \\\\\n                                 10\n                             \\end{bmatrix}\n                          = \\begin{bmatrix}\n                                 \\text{ya}[0] \\\\\n                                 \\text{ya}[1]\n                             \\end{bmatrix}\n\n\n\\text{yb} = \\mathbf{S}(1) = \\begin{bmatrix}\n                                 P(1) \\\\\n                                 P'(1)\n                             \\end{bmatrix}\n                            = \\begin{bmatrix}\n                                 0 \\\\\n                                 ?\n                              \\end{bmatrix}\n                            = \\begin{bmatrix}\n                                 \\text{yb}[0] \\\\\n                                 \\text{yb}[1]\n                              \\end{bmatrix}\n\n\n\\text{bc} = \\begin{bmatrix}\n                 \\text{ya} - \\mathbf{S}(-1) \\\\\n                 \\text{yb} - \\mathbf{S}(1)\n            \\end{bmatrix}\n          = \\begin{bmatrix}\n                 \\text{ya}[0] - 0 \\\\\n                 \\text{ya}[1] - 10 \\\\\n                 \\text{yb}[0] - 0\n            \\end{bmatrix}\n          = \\begin{bmatrix}\n                 \\text{ya}[0] \\\\\n                 \\text{ya}[1] - 10\\\\\n                 \\text{yb}[0]\n            \\end{bmatrix}\n\nInitial guess y0\nFor the given spacing h,\n\n\\begin{align}\n\\text{y0} & = [\\mathbf{S}(-1), \\mathbf{S}(-1 + h), \\mathbf{S}(-1 + 2h), \\cdots, \\mathbf{S}(1)] \\\\\n          & = \\begin{bmatrix}\n                  P(\\mu=-1) & P(\\mu=-1+h) & P(\\mu=-1+2h) & \\cdots & P(\\mu=1) \\\\\n                  P'(\\mu=-1) & P'(\\mu=-1+h) & P'(\\mu=-1+2h) & \\cdots & P'(\\mu=1) \\\\\n               \\end{bmatrix}\n\\end{align}\n\nFor m=0, 2 (even m)\nP_{1,m} (\\mu) \\sim \\cos\\left(\\displaystyle\\frac{(m + 1)\\pi}{2} \\mu\\right)\nFor m=1 (odd m) P_{1,m} (\\mu) \\sim \\sin\\left(\\displaystyle\\frac{(m + 1)\\pi}{2} \\mu\\right)\nif m % 2 == 0:\n    P_init = np.cos(mu * (m + 1) * np.pi / 2)\nelse:\n    P_init = np.sin(mu * (m + 1) * np.pi / 2)\nSince P'(-1) = 10,\ndP_init = 10*np.ones_like(mu)\nThen, together,\nS_init = np.vstack([P_init, dP_init])\n\nimport numpy as np \nfrom scipy.integrate import solve_bvp\nimport matplotlib.pyplot as plt \n\n\ndef find_P_and_a2(n, m):\n\n    # ODE system\n    # Define BVP (Low and Lou 1990)\n    # a2 -&gt; eigenvalue\n    # S = [P, dP/dmu]\n    # F = dSdmu\n    # \n    # dP/dmu = 10 at mu = -1\n    def F(x, y, p):\n        mu = x\n        P = y[0]\n        dP = y[1]\n        a2 = p[0]\n\n        ddP = (-1)*(n*(n+1)*P + a2*((1+n)/n)*P**(1+2/n)) / (1-mu**2 + 1e-6)\n\n        return [dP, ddP] \n\n    # Boundary Condition\n    def bc(ya, yb, p):\n        return [ya[0], ya[1]-10, yb[0]]\n\n    # Domain\n    mu_span = [-1, 1]\n    N = 100\n    mu = np.linspace(mu_span[0], mu_span[1], N)\n\n    # Initial guess\n    # For given m, use different initial guess\n    if m % 2 == 0:\n        P_guess = np.cos(mu * (m + 1) * np.pi / 2)\n    else:\n        P_guess = np.sin(mu * (m + 1) * np.pi / 2)\n\n    # For initial guess of dP/dmu, just use BC value\n    dP_guess = 10*np.ones_like(mu)\n\n    y_guess = np.vstack([P_guess, dP_guess])\n\n    # For each initial eigenvalue, solve the problem.\n    # If it is successful, return that otherwise do not return.\n    # np.vectorize -&gt; for loop & return type : array\n    @np.vectorize\n    def solve_eigenvalue_problem(a2_0):\n        sol = solve_bvp(F, bc, mu, y_guess, p=[a2_0], tol=1e-6)\n        if sol.success == True:\n            return sol\n        else:\n            return None \n\n    a2_0_list = np.linspace(0.0, 10.0, 100)\n\n    results = solve_eigenvalue_problem(a2_0_list)\n    eigenvalues = np.array([sol.p for sol in results if sol is not None])\n\n\n     # round & unique value & sorting\n    eigenvalues = np.sort(np.unique(np.round(eigenvalues, 4)))\n    \n    # The smallest value for given m is desired eigenvalue\n    eigenvalue = eigenvalues[0]\n    # If this eigenvalue is zero for nonzero m, choose the next big eigenvalue\n    if m &gt; 0:\n        if not (eigenvalue &gt; 0):\n            eigenvalue = eigenvalues[1]\n\n    # Solve again with that eigenvalue\n    sol = solve_eigenvalue_problem([eigenvalue])[0]\n    \n    return sol.sol, sol.p[0]\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.grid(True)\nax.axhline(0, color='k', lw=2)\nax.axvline(0, color='k', lw=2)\nax.set_xlabel(r'$\\mu$')\nax.set_ylabel(r'P($\\mu$)')\n\nmu_plot = np.linspace(-1, 1, 1000)\n\nn = 1\nfor m in [0, 1, 2]:\n    \n    S, a2 = find_P_and_a2(n, m)\n    P_plot = S(mu_plot)[0]\n    \n    if a2 &lt; 1e-3:\n        P_label = 'P' r'$_{' f'{n}, {m}' r'}(\\mu)$ with $a^2' r'_{' f'{n}, {m}' r'}$ = 0'\n    else:\n        P_label = 'P' r'$_{' f'{n}, {m}' r'}(\\mu)$ with $a^2' r'_{' f'{n}, {m}' r'}$ = ' f'{a2:.3g}'\n    ax.plot(mu_plot, P_plot, label=P_label)\n\nfig.legend()\n\n\n\n\n\n\n\n\nWe successfuly solve Low-Lou ODE for n=1, m=0, 1, 2. (see Figure 1 by Low and Lou (1990))"
  },
  {
    "objectID": "posts/low_lou_force_free/index.html#parameters",
    "href": "posts/low_lou_force_free/index.html#parameters",
    "title": "Low and Lou (1990) force free magnetic fields",
    "section": "Parameters",
    "text": "Parameters\n\nbounds\nbounds=[x_min, x_max, y_min, y_max, z_min, z_max]\n\n\nresolutions\nresolutions=[Nx, Ny, Nz] where Nx, Ny, Nz respectively mean that the number of points in x-, y-, z-axis.\n\n\nn & m\nP_{n,m}(\\mu) & a^2 _{n,m} are eigenfunction and eigenvalues for Low-Lou ODE with fixed n. And m is just used for denoting differenct eigenfunction and eigenvalues.\n\n\\displaystyle (1-\\mu^2)\\frac{d^2 P_{n,m}}{d\\mu^2} + n(n+1)P_{n,m} + a^2 _{n,m} \\frac{1+n}{n}P_{n,m}^{1 + \\frac{2}{n}} = 0\n\n\n\nl & \\Phi\nSee Figure 2 by Low and Lou (1990)"
  },
  {
    "objectID": "posts/low_lou_force_free/index.html#algorithm",
    "href": "posts/low_lou_force_free/index.html#algorithm",
    "title": "Low and Lou (1990) force free magnetic fields",
    "section": "Algorithm",
    "text": "Algorithm\nFor each physical coordinate (x, y, z),\nCalculate corresponding\n\nlocal Cartesian coordinate (X, Y, Z)\nlocal spherical coordinate (r, \\theta, \\phi)\n\\mu = \\cos\\theta\nP'_{n,m}(\\mu) and P_{n,m}(\\mu) by solving Low-Lou ODE\n(B_r, B_\\theta, B_\\phi)\n(B_X, B_Y, B_Z)\n(B_x, B_y, B_z)\n\nThen, we get a vector (B_x, B_y, B_z) at the point (x, y, z)\nThe local spherical coordinates are\nr = \\sqrt{X^2 + Y^2 + Z^2}\n\\theta= \\arccos\\frac{Z}{r}\n\\phi = \\arctan2\\frac{Y}{X}\nS refers to \\mathbf{S}_{n,m}(\\mu) and a2 refers to a^2 _{n,m}\n\n\\mathbf{S}_{n,m}(\\mu) = \\begin{bmatrix}\n                     P_{n,m}(\\mu) \\\\\n                     P'_{n,m}(\\mu)\n                 \\end{bmatrix}\n\nThe magnetic field is\n\n\\mathbf{B}(r, \\theta, \\phi) = B_r \\hat{r} + B_\\theta \\hat{\\theta} + B_\\phi \\hat{\\phi}\n\nwhere\n\nB_r = \\frac{1}{r^2 \\sin\\theta} \\frac{\\partial A}{\\partial \\theta}\n\n\nB_\\theta = -\\frac{1}{r \\sin\\theta} \\frac{\\partial A}{\\partial r}\n\n\nB_\\phi = \\frac{Q}{r \\sin\\theta}\n\nHere A is\n\nA = (r^{-n})P_{n,m}(\\mu)\n\n\n\\frac{\\partial A}{\\partial \\theta} = (- r^{-n}\\sin\\theta) P'_{n,m}(\\mu)\n\n\n\\frac{\\partial A}{\\partial r} = (-n r^{-n-1}) P_{n,m}(\\mu)\n\nFor n=1,\n\nQ(A) = \\sqrt{a^2 _{n,m}} A^{1 + \\frac{1}{n}}\n\n\n\\alpha = \\sqrt{a^2 _{n,m}} \\left(1 + \\frac{1}{n} \\right) A^{\\frac{1}{n}}\n\n\nimport pyvista as pv\npv.set_jupyter_backend('static')\npv.global_theme.notebook = True\npv.start_xvfb()\n\n\nclass LowLouMag:\n    \"A Low and Lou (1990) NLFFF\"\n\n    \n    def __init__(self, \n                 bounds=[-1,1,-1,1,0,2],\n                 resolutions=[64,64,64],\n                 n=1, m=1,\n                 l=0.3, Phi=np.pi/2):\n        self.bounds = bounds\n        self.resolutions = resolutions\n        self.n = n\n        self.m = m\n        self.l = l\n        self.Phi = Phi\n        \n\n    def __str__(self): \n        return (\n            \"### Low and Lou (1990) NLFFF\\n\"\n            f\"bounds = {self.bounds}&lt;br&gt;\\n\"\n            f\"resolutions = {self.resolutions}&lt;br&gt;\\n\"\n            f\"n = {self.n}&lt;br&gt;\\n\"\n            f\"m = {self.m}&lt;br&gt;\\n\"\n            f\"l = {self.l}&lt;br&gt;\\n\"\n            f\"Phi = {self.Phi/np.pi}π&lt;br&gt;\\n\"\n        )\n    _repr_markdown_ = __str__\n\n\n    def create_physical_coordinates(self):\n        x_1D = np.linspace(self.bounds[0], self.bounds[1], self.resolutions[0])\n        y_1D = np.linspace(self.bounds[2], self.bounds[3], self.resolutions[1])\n        z_1D = np.linspace(self.bounds[4], self.bounds[5], self.resolutions[2])\n        x_spacing = np.diff(x_1D)[0]\n        y_spacing = np.diff(y_1D)[0]\n        z_spacing = np.diff(z_1D)[0]\n        spacing = (x_spacing, y_spacing, z_spacing)\n        origin = (x_1D[0], y_1D[0], z_1D[0]) # The bottom left corner of the data set\n        self.grid = pv.ImageData(dimensions=self.resolutions, spacing=spacing, origin=origin)\n        self.x_1D = x_1D\n        self.y_1D = y_1D\n        self.z_1D = z_1D\n        return self.grid\n    \n\n    def calculate_local_Cartesian_coordinates(self):\n        # information of point source & Z-axis\n        l = self.l\n        Phi = self.Phi\n        \n        # physical coordinates (x, y, z)\n        x = self.grid.x\n        y = self.grid.y\n        z = self.grid.z\n\n        # local Cartesian coordinates (X, Y, Z)\n        X = x*np.cos(Phi) - (z+l)*np.sin(Phi)\n        Y = y\n        Z = x*np.sin(Phi) + (z+l)*np.cos(Phi)\n\n        self.X, self.Y, self.Z = X, Y, Z\n\n\n    def calculate_local_spherical_coordinates(self):\n        # local Cartesian coordinates (X, Y, Z)\n        X = self.X\n        Y = self.Y\n        Z = self.Z\n\n        # local spherical coordinates (r, theta, phi)\n        r = np.sqrt(X**2 + Y**2 + Z**2)\n        theta = np.arccos(Z/r)\n        phi = np.arctan2(Y, X) \n\n        self.r, self.theta, self.phi = r, theta, phi\n\n\n    def calculate_eigenfunctions(self):\n        # calculate mu=cos(theta)\n        mu = np.cos(self.theta)\n\n        # eigenfunction parameter n, m\n        n = self.n\n        m = self.m\n\n        # calculate eigenfunction & its derivates and eigenvalues\n        # S = [P, dP]\n        S, a2 = find_P_and_a2(n, m)\n        P, dP = S(mu)\n\n        self.P, self.dP, self.a2 = P, dP, a2\n\n\n    def calculate_local_spherical_magnetic_fields(self):\n        # eigenfunctions and eigenvalue\n        n, m = self.n, self.m\n        P, dP, a2 = self.P, self.dP, self.a2\n\n        # r, theta info\n        r, theta = self.r, self.theta\n        \n        A = (r**(-n)) * P\n        dA_dtheta = -(r**(-n)) * np.sin(theta) * dP\n        dA_dr = -(n*(r**(-n-1))) * P\n        Q = np.sqrt(a2) * A * np.abs(A)**(1/n)\n        \n        alpha = np.sqrt(a2) * (1 + 1/n) * A**(1/n)\n        \n        Br = (r**2 * np.sin(theta))**(-1) * dA_dtheta\n        Btheta = -1 * (r*np.sin(theta))**(-1) * dA_dr\n        Bphi = (r*np.sin(theta))**(-1) * Q\n        \n        self.Br, self.Btheta, self.Bphi, self.alpha = Br, Btheta, Bphi, alpha\n\n\n    def calculate_local_Cartesian_magnetic_fields(self):\n        Br, Btheta, Bphi = self.Br, self.Btheta, self.Bphi\n        r, theta, phi = self.r, self.theta, self.phi\n        \n        BX = Br * np.sin(theta) * np.cos(phi) + Btheta * np.cos(theta) * np.cos(phi) - Bphi * np.sin(phi)\n        BY = Br * np.sin(theta) * np.sin(phi) + Btheta * np.cos(theta) * np.sin(phi) + Bphi * np.cos(phi)\n        BZ = Br * np.cos(theta) - Bphi * np.sin(theta)\n        \n        self.BX, self.BY, self.BZ = BX, BY, BZ\n\n\n    def calculate_physical_magnetic_fields(self):\n        BX, BY, BZ = self.BX, self.BY, self.BZ\n        Phi = self.Phi\n\n        Bx = BX * np.cos(Phi) + BZ * np.sin(Phi)\n        By = BY\n        Bz = - BX * np.sin(Phi) + BZ * np.cos(Phi)\n        \n        self.Bx, self.By, self.Bz = Bx, By, Bz\n\n    def calculate_final_magnetic_fields(self):\n        bx = self.Bx.reshape(self.resolutions).transpose(2, 1, 0)\n        by = self.By.reshape(self.resolutions).transpose(2, 1, 0)\n        bz = self.Bz.reshape(self.resolutions).transpose(2, 1, 0)\n        return bx, by, bz\n\n    ##------ All in one ------##\n    def calculate(self):\n        self.create_physical_coordinates()\n        self.calculate_local_Cartesian_coordinates()\n        self.calculate_local_spherical_coordinates()\n        self.calculate_eigenfunctions()\n        self.calculate_local_spherical_magnetic_fields()\n        self.calculate_local_Cartesian_magnetic_fields()\n        self.calculate_physical_magnetic_fields()\n        bx, by, bz = self.calculate_final_magnetic_fields()\n        return bx, by, bz\n\n\nb = LowLouMag()\nb\n\nLow and Lou (1990) NLFFF\nbounds = [-1, 1, -1, 1, 0, 2] resolutions = [64, 64, 64] n = 1 m = 1 l = 0.3 Phi = 0.5π\n\n\n\nb.create_physical_coordinates()\n\n\n\n\n\n\nImageData\nInformation\n\n\nN Cells\n250047\n\n\nN Points\n262144\n\n\nX Bounds\n-1.000e+00, 1.000e+00\n\n\nY Bounds\n-1.000e+00, 1.000e+00\n\n\nZ Bounds\n0.000e+00, 2.000e+00\n\n\nDimensions\n64, 64, 64\n\n\nSpacing\n3.175e-02, 3.175e-02, 3.175e-02\n\n\nN Arrays\n0\n\n\n\n\n\n\n\n\nb.calculate_local_Cartesian_coordinates()\n\n\nb.grid.x[0], b.grid.y[0], b.grid.z[0]\n\n(-1.0, -1.0, 0.0)\n\n\n\nb.X[0], b.Y[0], b.Z[0]\n\n(-0.30000000000000004, -1.0, -1.0)\n\n\n\nb.calculate_local_spherical_coordinates()\n\n\nb.r[0], b.theta[0], b.phi[0]\n\n(1.445683229480096, 2.3346567297775978, -1.8622531212727638)\n\n\n\nb.calculate_eigenfunctions()\n\n\nb.P[0], b.dP[0], b.a2\n\n(2.3671821639686197, 4.145376627987454, 0.4274037235234833)\n\n\n\nb.calculate_local_spherical_magnetic_fields()\n\n\nb.Br[0], b.Btheta[0], b.Bphi[0], b.alpha[0]\n\n(-1.3719698429432043,\n 1.0848561237109233,\n 1.6788928338597737,\n 2.140955710310132)\n\n\n\nb.calculate_local_Cartesian_magnetic_fields()\n\n\nb.BX[0], b.BY[0], b.BZ[0]\n\n(2.108420021547058, 1.185348144787035, -0.26343650351723624)\n\n\n\nb.calculate_physical_magnetic_fields()\n\n\nb.Bx[0], b.By[0], b.Bz[0]\n\n(-0.26343650351723613, 1.185348144787035, -2.108420021547058)\n\n\n\nbx, by, bz = b.calculate_final_magnetic_fields()\n\n\n!pip install -q git+https://github.com/mgjeon/magnetic_field_line.git\n\n\nfrom magplot.base import create_mesh, mag_plotter\n\n\nmesh = create_mesh(bx, by, bz)\n\n\nb_plot = mag_plotter(mesh)\nb_tube, b_bottom, b_dargs = b_plot.create_mesh(i_siz=32, j_siz=32, \n                                               i_resolution=8, j_resolution=8, \n                                               vmin=-100, vmax=100, \n                                               max_time=10000)\n\n\np = pv.Plotter()\np.add_mesh(b_plot.grid.outline())\np.add_mesh(b_bottom, cmap='gray', **b_dargs)\np.add_mesh(b_tube, lighting=False, color='blue')\np.camera_position = 'xy'\np.show_bounds()\np.show()\n\n\n\n\n\n\n\n\n\np = pv.Plotter(off_screen=False)\np.add_mesh(b_plot.grid.outline())\np.add_mesh(pv.Plane(center=(mesh.center[0], mesh.center[1], -1), direction=(0, 0, 1), i_size=64, j_size=64), color='gray')\np.add_mesh(b_bottom.contour(scalars=b_bottom['vector'][:, 2]), cmap='bwr', **b_dargs)\np.add_mesh(b_tube, lighting=False, color='black')\np.camera_position = 'xy'\np.show_bounds()\np.show()\n\n\n\n\n\n\n\n\nIt seems to correspond to Figure 8, not Figure 3. I don’t know why.\n\nmesh_new = create_mesh(*LowLouMag(Phi=np.pi/4).calculate()).reflect((0, 1, 0))\n\nb_plot_new = mag_plotter(mesh_new)\nb_tube_new, b_bottom_new, b_dargs_new = b_plot_new.create_mesh(i_siz=32, j_siz=32, \n                                                   i_resolution=8, j_resolution=8, \n                                                   vmin=-100, vmax=100, \n                                                   max_time=10000)\n\np = pv.Plotter(off_screen=False)\np.add_mesh(b_plot_new.grid.outline())\np.add_mesh(pv.Plane(center=(mesh_new.center[0], mesh_new.center[1], -1), direction=(0, 0, 1), i_size=64, j_size=64), color='gray')\np.add_mesh(b_bottom_new.contour(scalars=b_bottom_new['vector'][:, 2]), cmap='bwr', **b_dargs_new)\np.add_mesh(b_tube_new, lighting=False, color='black')\np.camera_position = 'xy'\np.show_bounds()\np.show()\n\n\n\n\n\n\n\n\n\nmesh_new = create_mesh(*LowLouMag(Phi=0.47).calculate()).reflect((0, 1, 0))\n\nb_plot_new = mag_plotter(mesh_new)\nb_tube_new, b_bottom_new, b_dargs_new = b_plot_new.create_mesh(i_siz=40, j_siz=40, \n                                                   i_resolution=8, j_resolution=8, \n                                                   vmin=-100, vmax=100, \n                                                   max_time=10000)\n\np = pv.Plotter(off_screen=False)\np.add_mesh(b_plot_new.grid.outline())\np.add_mesh(pv.Plane(center=(mesh_new.center[0], mesh_new.center[1], -1), direction=(0, 0, 1), i_size=64, j_size=64), color='gray')\np.add_mesh(b_bottom_new.contour(scalars=b_bottom_new['vector'][:, 2]), cmap='bwr', **b_dargs_new)\np.add_mesh(b_tube_new, lighting=False, color='black')\np.camera_position = 'xy'\np.show_bounds()\np.show()\n\n\n\n\n\n\n\n\n\nmesh_new = create_mesh(*LowLouMag(Phi=0.27).calculate()).reflect((0, 1, 0))\n\nb_plot_new = mag_plotter(mesh_new)\nb_tube_new, b_bottom_new, b_dargs_new = b_plot_new.create_mesh(i_siz=40, j_siz=40, \n                                                   i_resolution=8, j_resolution=8, \n                                                   vmin=-100, vmax=100, \n                                                   max_time=10000)\n\np = pv.Plotter(off_screen=False)\np.add_mesh(b_plot_new.grid.outline())\np.add_mesh(pv.Plane(center=(mesh_new.center[0], mesh_new.center[1], -1), direction=(0, 0, 1), i_size=64, j_size=64), color='gray')\np.add_mesh(b_bottom_new.contour(scalars=b_bottom_new['vector'][:, 2]), cmap='bwr', **b_dargs_new)\np.add_mesh(b_tube_new, lighting=False, color='black')\np.camera_position = 'xy'\np.show_bounds()\np.show()\n\n\n\n\n\n\n\n\nWith reflect((0, 1, 0)), these seems to correspond to Figure 4, 5, and 6, respectively. However, I don’t know why I need to use reflect((0, 1, 0))."
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html",
    "href": "posts/charged-particle-motion-in-dipole/index.html",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "",
    "text": "This post is based on the homework report that was written during the solar terrestrial physics class in 2022.\n\n\n\n\\mathbf{B} = -\\frac{\\mu_0}{4\\pi}\\left( \\frac{\\mathbf{m}}{r^3} - \\frac{3(\\mathbf{m}\\cdot\\mathbf{r})\\mathbf{r}}{r^5} \\right)\n\nIn (r, \\lambda, \\phi) coordinates, we describe the magnetic field as follows:\n\nB_r = Z = -\\frac{\\mu_0 m}{2\\pi}\\frac{\\sin\\lambda}{r^3}\n\n\nB_\\lambda = H = \\frac{\\mu_0 m}{4\\pi}\\frac{\\cos\\lambda}{r^3}\n\n\nB_\\phi = 0\n\nTherefore, the magnetic field \\mathbf{B} does not depend on longitude \\phi.\n\n\\mathbf{B} = \\mathbf{B}(r, \\lambda)\n\nThe strength B is given by:\n\nB = \\frac{\\mu_0 m}{4\\pi r^3}(1+3\\sin^2\\lambda)^{\\textstyle \\frac{1}{2}}\n\nFor the Earth’s magnetic field at the equator, denoted as B_E, it can be expressed as:\n\nB_E = \\frac{\\mu_0 m}{4\\pi R_E^3}\n\nThe actual value for B_E is approximately 0.31 Gauss (G).\nThe components of the magnetic field can be re-express in terms of B_E as follows:\n\nB_r = -\\frac{2B_E}{(r/R_E)^3}\\sin\\lambda\n\n\nB_\\lambda = \\frac{B_E}{(r/R_E)^3}\\cos\\lambda\n\n\nB_\\phi = 0\n\nThe magnetic field line in the meridian (when \\phi=\\text{const} or in the (r,\\lambda)-plane) is given by:\n\nr = r_\\text{eq} \\cos^2\\lambda\n\nHere, r_\\text{eq} = LR_E and L is called L-parameter.\n\nL-parameter describes the set of magnetic field lines which cross the Earth’s magnetic equator at a number of Earth-radii equal to the L-parameter. For example, L=2 describes the set of the Earth’s magnetic field lines which cross the Earth’s magnetic equator two earth radii from the center of the Earth.1\n\n\nThe dipole model of the Earth’s magnetic field is a first order approximation of the rather complex true Earth’s magnetic field. Due to effects of the interplanetary magnetic field (IMF), and the solar wind, the dipole model is particularly inaccurate at high L-shells (e.g., above L=3), but may be a good approximation for lower L-shells. For more precise work, or for any work at higher L-shells, a more accurate model that incorporates solar effects, such as the Tsyganenko magnetic field model, is recommended.2\n\n\n\\begin{align*}\n\\mathbf{B}(\\mathbf{r}) & = B_r(r, \\lambda) \\hat{r} + B_\\lambda (r, \\lambda) \\hat{\\lambda} \\\\\n& = B_x(x, y)\\hat{x} + B_y(x, y)\\hat{y}\n\\end{align*}\n\n\n\n\n\n\\frac{d^2\\mathbf{r}}{dt^2} = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}\n\n\n\\frac{d^2x}{dt^2}\\hat{x} + \\frac{d^2y}{dt^2}\\hat{y} + \\frac{d^2z}{dt^2}\\hat{z} = \\frac{q}{m}(\\hat{x}(v_yB_z - v_zB_y) + \\hat{y}(v_zB_x - v_xB_z) + \\hat{z}(v_xB_y - v_yB_x))\n\n\n\\frac{d^2x}{dt^2} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{d^2y}{dt^2} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{d^2z}{dt^2} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\n\\frac{dx}{dt} = v_x\n\n\n\\frac{dy}{dt} = v_y\n\n\n\\frac{dz}{dt} = v_z\n\n\n\\frac{dv_x}{dt} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{dv_y}{dt} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{dv_z}{dt} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\nS = (x, y, z, v_x, v_y, v_z)\n\n\n\\frac{dS}{dt} = (\\frac{dx}{dt}, \\frac{dy}{dt}, \\frac{dz}{dt}, \\frac{dv_x}{dt}, \\frac{dv_y}{dt},\\frac{dv_z}{dt})"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#theory",
    "href": "posts/charged-particle-motion-in-dipole/index.html#theory",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "",
    "text": "This post is based on the homework report that was written during the solar terrestrial physics class in 2022.\n\n\n\n\\mathbf{B} = -\\frac{\\mu_0}{4\\pi}\\left( \\frac{\\mathbf{m}}{r^3} - \\frac{3(\\mathbf{m}\\cdot\\mathbf{r})\\mathbf{r}}{r^5} \\right)\n\nIn (r, \\lambda, \\phi) coordinates, we describe the magnetic field as follows:\n\nB_r = Z = -\\frac{\\mu_0 m}{2\\pi}\\frac{\\sin\\lambda}{r^3}\n\n\nB_\\lambda = H = \\frac{\\mu_0 m}{4\\pi}\\frac{\\cos\\lambda}{r^3}\n\n\nB_\\phi = 0\n\nTherefore, the magnetic field \\mathbf{B} does not depend on longitude \\phi.\n\n\\mathbf{B} = \\mathbf{B}(r, \\lambda)\n\nThe strength B is given by:\n\nB = \\frac{\\mu_0 m}{4\\pi r^3}(1+3\\sin^2\\lambda)^{\\textstyle \\frac{1}{2}}\n\nFor the Earth’s magnetic field at the equator, denoted as B_E, it can be expressed as:\n\nB_E = \\frac{\\mu_0 m}{4\\pi R_E^3}\n\nThe actual value for B_E is approximately 0.31 Gauss (G).\nThe components of the magnetic field can be re-express in terms of B_E as follows:\n\nB_r = -\\frac{2B_E}{(r/R_E)^3}\\sin\\lambda\n\n\nB_\\lambda = \\frac{B_E}{(r/R_E)^3}\\cos\\lambda\n\n\nB_\\phi = 0\n\nThe magnetic field line in the meridian (when \\phi=\\text{const} or in the (r,\\lambda)-plane) is given by:\n\nr = r_\\text{eq} \\cos^2\\lambda\n\nHere, r_\\text{eq} = LR_E and L is called L-parameter.\n\nL-parameter describes the set of magnetic field lines which cross the Earth’s magnetic equator at a number of Earth-radii equal to the L-parameter. For example, L=2 describes the set of the Earth’s magnetic field lines which cross the Earth’s magnetic equator two earth radii from the center of the Earth.1\n\n\nThe dipole model of the Earth’s magnetic field is a first order approximation of the rather complex true Earth’s magnetic field. Due to effects of the interplanetary magnetic field (IMF), and the solar wind, the dipole model is particularly inaccurate at high L-shells (e.g., above L=3), but may be a good approximation for lower L-shells. For more precise work, or for any work at higher L-shells, a more accurate model that incorporates solar effects, such as the Tsyganenko magnetic field model, is recommended.2\n\n\n\\begin{align*}\n\\mathbf{B}(\\mathbf{r}) & = B_r(r, \\lambda) \\hat{r} + B_\\lambda (r, \\lambda) \\hat{\\lambda} \\\\\n& = B_x(x, y)\\hat{x} + B_y(x, y)\\hat{y}\n\\end{align*}\n\n\n\n\n\n\\frac{d^2\\mathbf{r}}{dt^2} = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}\n\n\n\\frac{d^2x}{dt^2}\\hat{x} + \\frac{d^2y}{dt^2}\\hat{y} + \\frac{d^2z}{dt^2}\\hat{z} = \\frac{q}{m}(\\hat{x}(v_yB_z - v_zB_y) + \\hat{y}(v_zB_x - v_xB_z) + \\hat{z}(v_xB_y - v_yB_x))\n\n\n\\frac{d^2x}{dt^2} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{d^2y}{dt^2} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{d^2z}{dt^2} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\n\\frac{dx}{dt} = v_x\n\n\n\\frac{dy}{dt} = v_y\n\n\n\\frac{dz}{dt} = v_z\n\n\n\\frac{dv_x}{dt} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{dv_y}{dt} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{dv_z}{dt} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\nS = (x, y, z, v_x, v_y, v_z)\n\n\n\\frac{dS}{dt} = (\\frac{dx}{dt}, \\frac{dy}{dt}, \\frac{dz}{dt}, \\frac{dv_x}{dt}, \\frac{dv_y}{dt},\\frac{dv_z}{dt})"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#code",
    "href": "posts/charged-particle-motion-in-dipole/index.html#code",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "Code",
    "text": "Code\n\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom matplotlib.animation import FuncAnimation\n\n\n# Be = 0.31 G = 0.31 * 10^-4 T\nBe = 0.31 * 1e-4\n# Re = 6371 km = 6371 * 10^3 m\nRe = 6371 * 1e3\nC = Be * (Re**3)\n\n\ndef B(x, y, z):\n    \"\"\"dipole field at (x, y, z)\"\"\"\n    r = np.sqrt(x**2 + y**2 + z**2)\n    Bx = -1 * C * (3 * x * z) / (r**5)\n    By = -1 * C * (3 * y * z) / (r**5)\n    Bz = C * (r**2 - 3 * z**2) / (r**5)\n    return Bx, By, Bz\n\n\ndef field_line_3D(phi, L=6.6):\n    \"\"\"dipole field line (3D)\"\"\"\n    phi = np.deg2rad(phi)\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    rf = L * np.sin(theta) ** 2\n    xf = rf * np.sin(theta) * np.cos(phi)\n    yf = rf * np.sin(theta) * np.sin(phi)\n    zf = rf * np.cos(theta)\n    return xf, yf, zf\n\n\ndef field_line_2D(L=6.6):\n    \"\"\"dipole field line (2D)\"\"\"\n    lamb = np.linspace(0, 2 * np.pi, 1000)\n    rf2 = L * np.cos(lamb) ** 2\n    xf2 = rf2 * np.cos(lamb)\n    zf2 = rf2 * np.sin(lamb)\n    return xf2, zf2\n\n\ndef dSdt(S, t, q_over_m):\n    \"\"\"dS/dt for odeint\"\"\"\n    x, y, z, vx, vy, vz = S\n    Bx, By, Bz = B(x, y, z)\n    dvxdt = q_over_m * (vy * Bz - vz * By)\n    dvydt = q_over_m * (vz * Bx - vx * Bz)\n    dvzdt = q_over_m * (vx * By - vy * Bx)\n    return [vx, vy, vz, dvxdt, dvydt, dvzdt]\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nxmin, xmax = -7, 7\nymin, ymax = -7, 7\n\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.legend()\nax.axhline(y=0, color=\"black\", linewidth=0.5)\nax.xaxis.grid(True, which=\"both\")\nax.set_xlim(xmin, xmax)\nax.set_ylim(ymin, ymax)\nmajors = np.arange(xmin + 1, xmax, 1)\nax.xaxis.set_major_locator(ticker.FixedLocator(majors))\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\n\nLvalues = [2, 4, 6, 8, 10, 20, 100]\ncolors = [\"r\", \"darkorange\", \"gold\", \"green\", \"blue\", \"magenta\", \"purple\"]\nlamb = np.linspace(0, 2 * np.pi, 1000)\nfor i, L in enumerate(Lvalues):\n    x, z = field_line_2D(L)\n    ax.plot(x, z, label=f\"L={L}\", color=colors[i])\n\nax.legend()\nax.set_title(\"Magnetic Dipole Field line (2D)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nspecies = \"Proton\"\n\ne = 1.602e-19\nq = e  # C\n\nmH = 1.67e-27\nm = mH  # kg\nq_over_m = q / m\n\nE_keV = 2000  # keV\n\n\n# L-parameter\nL = 6.6\n\n# start at equator\nx0, y0, z0 = L * Re, 0, 0\n\nkeV_to_J = 1e3 * e  # J\n\n# particle energy (keV) and pitch angle\nE = E_keV * keV_to_J  # J\npitch_angle_deg = 30\nalpha = np.deg2rad(pitch_angle_deg)\n\n# particle velocity\nv0 = np.sqrt(2 * E / m)\n\n# vx = v_perp\n# vy = 0\n# vz = v_para\nvx0 = v0 * np.sin(alpha)\nvy0 = 0\nvz0 = v0 * np.cos(alpha)\n\n# bounce time_scale\nt_B = 290 * (np.pi * L / 10) * np.sqrt(m / (mH * E_keV))\nprint(f\"bounce time scale ~ {t_B} s\")\n\nS0 = [x0, y0, z0, vx0, vy0, vz0]\n\n# number of bounce\nn = 3\n\ntmin = 0\ntmax = n * t_B\n\nt = np.linspace(tmin, tmax, 1000)\n\n# solve ODE\nsol = odeint(dSdt, S0, t, args=(q_over_m,))\nx, y, z, vx, vy, vz = sol.T\nx, y, z = x / Re, y / Re, z / Re\n\nprint(f\"t_max ~ {tmax:.4f} s\")\n\nbounce time scale ~ 13.44549539521195 s\nt_max ~ 40.3365 s\n\n\n\ntrajectory_linewidth = 0.8\nfieldline_linewidth = 0.5\n\n\n# xyzrange = 10\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(projection=\"3d\")\nax.set_aspect(\"equal\")\n# ax.plot([-xyzrange,xyzrange], [0,0], [0, 0], color='black')\n# ax.plot([0,0], [-xyzrange,xyzrange], [0, 0], color='black')\n# ax.plot([0,0], [0,0], [-xyzrange, xyzrange], color='black')\nax.plot(\n    x,\n    y,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"y ($R_E$)\")\nax.set_zlabel(\"z ($R_E$)\")\nax.set_xlim(-L, L)\nax.set_ylim(-L, L)\nax.set_zlim(-L, L)\nfor az in np.arange(0, 361, 20):\n    xf, yf, zf = field_line_3D(az, L)\n    if az == 360:\n        ax.plot(\n            xf,\n            yf,\n            zf,\n            color=\"blue\",\n            linewidth=fieldline_linewidth,\n            zorder=-1,\n            label=f\"Magnetic Field (L={L})\",\n        )\n    else:\n        ax.plot(xf, yf, zf, color=\"blue\", linewidth=fieldline_linewidth, zorder=-1)\n\n# Sphere with radius Re\nu = np.linspace(0, 2 * np.pi, 1000)\nv = np.linspace(0, np.pi, 1000)\nxs = 1 * np.outer(np.cos(u), np.sin(v))\nys = 1 * np.outer(np.sin(u), np.sin(v))\nzs = 1 * np.outer(np.ones(np.size(u)), np.cos(v))\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.plot_surface(xs, ys, zs, color=\"white\", alpha=1)\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.set_aspect(\"equal\")\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    y,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"y ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\ntheta = np.linspace(0, 2 * np.pi, 100)\nrc = L\nxc = rc * np.cos(theta)\nyc = rc * np.sin(theta)\nplt.plot(xc, yc, color=\"green\", zorder=-1, label=f\"Circle with radius L={L}\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend(loc=1)\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\n\nxf2, zf2 = field_line_2D(L)\nax.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\n\nxf2, zf2 = field_line_2D(L)\nax.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(16, 8))\nax1 = plt.subplot(121)\nax2 = plt.subplot(122, projection=\"3d\")\n\nxdata = x\nydata = y\nzdata = z\n\nsc1 = ax1.scatter([], [], color=\"green\")\n(ln1,) = ax1.plot([], [], \"r-\", zorder=99)\nsc2 = ax2.scatter([], [], [], color=\"green\")\n(ln2,) = ax2.plot([], [], [], \"r-\", zorder=99)\n\n# Field line (2D)\nxf2, zf2 = field_line_2D(L)\nax1.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\n\n# Earth (2D)\nax1.add_patch(plt.Circle((0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\"))\n\n# Field line (3D)\nfor az in np.arange(0, 361, 20):\n    xf, yf, zf = field_line_3D(az, L)\n    if az == 360:\n        ax2.plot(\n            xf,\n            yf,\n            zf,\n            color=\"blue\",\n            linewidth=fieldline_linewidth,\n            zorder=-1,\n            label=f\"Magnetic Field (L={L})\",\n        )\n    else:\n        ax2.plot(xf, yf, zf, color=\"blue\", linewidth=fieldline_linewidth, zorder=-1)\n\n# Earth (3D)\nu = np.linspace(0, 2 * np.pi, 1000)\nv = np.linspace(0, np.pi, 1000)\nxs = 1 * np.outer(np.cos(u), np.sin(v))\nys = 1 * np.outer(np.sin(u), np.sin(v))\nzs = 1 * np.outer(np.ones(np.size(u)), np.cos(v))\nax2.plot_surface(xs, ys, zs, color=\"white\", alpha=1)\n\n\ndef init():\n    ax1.set_aspect(\"equal\")\n    ax1.plot([-10, 10], [0, 0], color=\"black\")\n    ax1.plot([0, 0], [-10, 10], color=\"black\")\n    ax1.set_xlabel(\"x ($R_E$)\")\n    ax1.set_ylabel(\"z ($R_E$)\")\n    ax1.set_xlim(-L - 1, L + 1)\n    ax1.set_ylim(-L - 1, L + 1)\n\n    ax2.set_aspect(\"equal\")\n    ax2.plot([-10, 10], [0, 0], [0, 0], color=\"black\")\n    ax2.plot([0, 0], [-10, 10], [0, 0], color=\"black\")\n    ax2.plot([0, 0], [0, 0], [-10, 10], color=\"black\")\n    ax2.set_xlabel(\"x ($R_E$)\")\n    ax2.set_ylabel(\"y ($R_E$)\")\n    ax2.set_zlabel(\"z ($R_E$)\")\n    ax2.set_xlim(-L, L)\n    ax2.set_ylim(-L, L)\n    ax2.set_zlim(-L, L)\n\n    return ln1, ln2\n\n\ndef update(frame):\n    sc1.set_offsets([xdata[frame - 1], zdata[frame - 1]])\n    ln1.set_data(xdata[:frame], zdata[:frame])\n    ln1.set_label(f\"t={frame}\")\n    ax1.legend()\n\n    sc2._offsets3d = ([xdata[frame - 1]], [ydata[frame - 1]], [zdata[frame - 1]])\n    ln2.set_data(xdata[:frame], ydata[:frame])\n    ln2.set_3d_properties(zdata[:frame])\n    ln2.set_label(f\"t={frame}\")\n    ax2.legend()\n    return ln1, ln2\n\n\nani = FuncAnimation(\n    fig, update, frames=np.arange(1, len(xdata)), init_func=init, blit=True\n)\n\nani.save(\"simulation.gif\", fps=30)\n\n\n\nani.save(\"simulation.mp4\", fps=30)"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#footnotes",
    "href": "posts/charged-particle-motion-in-dipole/index.html#footnotes",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/L-shell↩︎\nhttps://en.wikipedia.org/wiki/Dipole_model_of_the_Earth%27s_magnetic_field↩︎"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "Create an image using Diffusers library.\n\n\n\n!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)\n\n\n\n\n\npil_images = pipe(\n    prompt=prompt * batch_size,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images\n\n\nfor pil_image in pil_images:\n    display(pil_image)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#install-and-import-libraries",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#install-and-import-libraries",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#high-level",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#high-level",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "pil_images = pipe(\n    prompt=prompt * batch_size,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images\n\n\nfor pil_image in pil_images:\n    display(pil_image)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#references",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#references",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "Patil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/double-pendulum-julia/index.html",
    "href": "posts/double-pendulum-julia/index.html",
    "title": "The double pendulum problem in Julia",
    "section": "",
    "text": "This post is based on an example of Plots.jl."
  },
  {
    "objectID": "posts/double-pendulum-julia/index.html#the-double-pendulum-problem",
    "href": "posts/double-pendulum-julia/index.html#the-double-pendulum-problem",
    "title": "The double pendulum problem in Julia",
    "section": "",
    "text": "This post is based on an example of Plots.jl."
  },
  {
    "objectID": "posts/double-pendulum-julia/index.html#code",
    "href": "posts/double-pendulum-julia/index.html#code",
    "title": "The double pendulum problem in Julia",
    "section": "Code",
    "text": "Code\n\nusing OrdinaryDiffEq, Plots; gr()\nusing Printf\n\n\nfunction pendulum!(du, u, p, t)\n    # u[1] = theta1\n    # u[2] = omega1\n    # u[3] = theta2\n    # u[4] = omega2\n    \n    (; M1, M2, L1, L2, G) = p\n\n    du[1] = u[2]\n\n    delta = u[3] - u[1]\n    den1 = (M1 + M2) * L1 - M2 * L1 * cos(delta) * cos(delta)\n    du[2] = (\n        (\n            M2 * L1 * u[2] * u[2] * sin(delta) * cos(delta) +\n            M2 * G * sin(u[3]) * cos(delta) +\n            M2 * L2 * u[4] * u[4] * sin(delta) - (M1 + M2) * G * sin(u[1])\n        ) / den1\n    )\n\n    du[3] = u[4]\n\n    den2 = (L2 / L1) * den1\n    du[4] = (\n        (\n            -M2 * L2 * u[4] * u[4] * sin(delta) * cos(delta) +\n            (M1 + M2) * G * sin(u[1]) * cos(delta) -\n            (M1 + M2) * L1 * u[2] * u[2] * sin(delta) - (M1 + M2) * G * sin(u[3])\n        ) / den2\n    )\n    nothing\nend\n\npendulum! (generic function with 1 method)\n\n\n\nG = 9.8       # acceleration due to gravity, in m/s^2\nL1 = 1.0      # length of pendulum 1 in m\nL2 = 1.0      # length of pendulum 2 in m\nL = L1 + L2   # maximal length of the combined pendulum\nM1 = 1.0      # mass of pendulum 1 in kg\nM2 = 1.0      # mass of pendulum 2 in kg\nt_stop = 5.0  # how many seconds to simulate\n\n5.0\n\n\n\n# th1 and th2 are the initial angles (degrees)\n# w10 and w20 are the initial angular velocities (degrees per second)\nth1 = 120.0\nw1 = 0.0\nth2 = -10.0\nw2 = 0.0\n\n0.0\n\n\n\nu0 = deg2rad.([th1, w1, th2, w2])\n\n4-element Vector{Float64}:\n  2.0943951023931953\n  0.0\n -0.17453292519943295\n  0.0\n\n\n\np = (; M1, M2, L1, L2, G)\n\n(M1 = 1.0, M2 = 1.0, L1 = 1.0, L2 = 1.0, G = 9.8)\n\n\n\ntspan = (0.0, t_stop)\n\n(0.0, 5.0)\n\n\n\nprob = ODEProblem(pendulum!, u0, tspan, p)\n\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 5.0)\nu0: 4-element Vector{Float64}:\n  2.0943951023931953\n  0.0\n -0.17453292519943295\n  0.0\n\n\n\n\n@time solve(prob, dense=true, alg=Vern7(), reltol=1e-10, abstol=1e-10);\n\n  4.801410 seconds (20.07 M allocations: 1.311 GiB, 6.50% gc time, 99.99% compilation time)\n\n\n\n@time sol = solve(prob, dense=true, \n    alg=Vern7(), reltol=1e-10, abstol=1e-10)\n\n  0.000413 seconds (3.97 k allocations: 404.250 KiB)\n\n\nretcode: Success\nInterpolation: specialized 7th order lazy interpolation\nt: 327-element Vector{Float64}:\n 0.0\n 0.015120200135242772\n 0.03461247124860289\n 0.05621034906611724\n 0.08063338298221337\n 0.10655860257718384\n 0.13371069061057467\n 0.16136170833943062\n 0.18910379555029622\n 0.216500191087515\n ⋮\n 4.877658909987774\n 4.891048061088556\n 4.906726954985946\n 4.92395952857032\n 4.940477265391924\n 4.9570673499295275\n 4.973347466199946\n 4.989481530533818\n 5.0\nu: 327-element Vector{Vector{Float64}}:\n [2.0943951023931953, 0.0, -0.17453292519943295, 0.0]\n [2.093251192266732, -0.15130144360225076, -0.17507316706106824, -0.07138583688240614]\n [2.0884020066846594, -0.34620684293642423, -0.17735151407883185, -0.16197940762950855]\n [2.0785960375730372, -0.5617458491336331, -0.18190022469696743, -0.2583291215327368]\n [2.0619088616767502, -0.8045658145300556, -0.18945998215379264, -0.35897310430460627]\n [2.0377259296673795, -1.0606883684801727, -0.20000780534447987, -0.45198941094552436]\n [2.0053127296813607, -1.3263167089129453, -0.21337992707010664, -0.5290751421284102]\n [1.9649409859714275, -1.5930439677685073, -0.22879032047995732, -0.5805198650083305]\n [1.9170909858635226, -1.8556580077531657, -0.24523226705660403, -0.5987532995918393]\n [1.8627674994092769, -2.109011300081415, -0.2614479063701741, -0.5781647279534727]\n ⋮\n [0.08055304727076054, -4.844672221947719, 7.323641407873413, -0.3937092424276102]\n [0.016456248134838113, -4.729974482015508, 7.315465794703606, -0.8259494949202341]\n [-0.056649840079156244, -4.5949029003734845, 7.298626670050392, -1.3197608125366442]\n [-0.13450819146501108, -4.439547315775765, 7.271324047281913, -1.8459341422625748]\n [-0.20652600642233343, -4.277983589438366, 7.236791129717663, -2.3322767387862955]\n [-0.27603285796575844, -4.097988312352799, 7.194181777987756, -2.800971206639784]\n [-0.3411670498167553, -3.899839597788059, 7.144980717318993, -3.2396924617007903]\n [-0.40234149964195665, -3.6791729003067517, 7.08935288274075, -3.652222880783009]\n [-0.44021974519477647, -3.5211389770525976, 7.049578950062814, -3.9088281792608424]\n\n\n\ndt = 0.01\nt = range(0, t_stop, 500)\nt\n\n0.0:0.01002004008016032:5.0\n\n\n\nss = sol(t)\n\nt: 0.0:0.01002004008016032:5.0\nu: 500-element Vector{Vector{Float64}}:\n [2.0943951023931953, 0.0, -0.17453292519943295, 0.0]\n [2.093892727794568, -0.1002718148594462, -0.17477031576780788, -0.047361688467315]\n [2.0923857338065837, -0.20051767898773526, -0.17548119651329958, -0.09446566162367707]\n [2.089874512316763, -0.3007111508060837, -0.17666169217282704, -0.14105372938857924]\n [2.0863597246275583, -0.4008248182697641, -0.17830533609426816, -0.18686676551089001]\n [2.0818423134938038, -0.5008298418502478, -0.18040305865053075, -0.23164427293860568]\n [2.076323519577815, -0.6006955317654576, -0.18294317148887201, -0.2751239894162722]\n [2.069804901918936, -0.7003889715061674, -0.18591134808623772, -0.31704154681648083]\n [2.0622883618913157, -0.7998747002091745, -0.18929060121668145, -0.3571301977165818]\n [2.0537761699948334, -0.8991144670078379, -0.1930612580721276, -0.3951206226371907]\n ⋮\n [-0.11613910894023596, -4.4776867736889425, 7.278673697988922, -1.7218077760104107]\n [-0.16053801455439923, -4.3835492670133664, 7.2599127844500435, -2.0217833125716056]\n [-0.2039680558672177, -4.284106009021184, 7.238179522753177, -2.3150140108684756]\n [-0.2463703579186004, -4.178233875161714, 7.213543772354339, -2.601042788066967]\n [-0.287675318032719, -4.064916713625344, 7.186080007668746, -2.8794106387414]\n [-0.32780373308398425, -3.943255996139626, 7.1558671806928364, -3.1496882975237526]\n [-0.366668020773485, -3.8124788731823114, 7.122988257425244, -3.4115093994105914]\n [-0.404173506838206, -3.6719431947583643, 7.087529421169394, -3.6646035735052447]\n [-0.4402197451947838, -3.5211389770525665, 7.0495789500628065, -3.908828179260892]\n\n\n\ns = reduce(hcat, ss.u)'\n\n500×4 adjoint(::Matrix{Float64}) with eltype Float64:\n  2.0944     0.0       -0.174533   0.0\n  2.09389   -0.100272  -0.17477   -0.0473617\n  2.09239   -0.200518  -0.175481  -0.0944657\n  2.08987   -0.300711  -0.176662  -0.141054\n  2.08636   -0.400825  -0.178305  -0.186867\n  2.08184   -0.50083   -0.180403  -0.231644\n  2.07632   -0.600696  -0.182943  -0.275124\n  2.0698    -0.700389  -0.185911  -0.317042\n  2.06229   -0.799875  -0.189291  -0.35713\n  2.05378   -0.899114  -0.193061  -0.395121\n  ⋮                               \n -0.116139  -4.47769    7.27867   -1.72181\n -0.160538  -4.38355    7.25991   -2.02178\n -0.203968  -4.28411    7.23818   -2.31501\n -0.24637   -4.17823    7.21354   -2.60104\n -0.287675  -4.06492    7.18608   -2.87941\n -0.327804  -3.94326    7.15587   -3.14969\n -0.366668  -3.81248    7.12299   -3.41151\n -0.404174  -3.67194    7.08753   -3.6646\n -0.44022   -3.52114    7.04958   -3.90883\n\n\n\n# theta1\ns[:, 1]\n\n500-element Vector{Float64}:\n  2.0943951023931953\n  2.093892727794568\n  2.0923857338065837\n  2.089874512316763\n  2.0863597246275583\n  2.0818423134938038\n  2.076323519577815\n  2.069804901918936\n  2.0622883618913157\n  2.0537761699948334\n  ⋮\n -0.11613910894023596\n -0.16053801455439923\n -0.2039680558672177\n -0.2463703579186004\n -0.287675318032719\n -0.32780373308398425\n -0.366668020773485\n -0.404173506838206\n -0.4402197451947838\n\n\n\n# u[1] = theta1\n# u[2] = omega1\n# u[3] = theta2\n# u[4] = omega2\n\nx1 = +L1 * sin.(s[:, 1])\ny1 = -L1 * cos.(s[:, 1])\n\nx2 = +L2 * sin.(s[:, 3]) + x1\ny2 = -L2 * cos.(s[:, 3]) + y1\n\n500-element Vector{Float64}:\n -0.48480775301220824\n -0.48520163506531977\n -0.4863838171334237\n -0.4883559068223133\n -0.49112058298184197\n -0.4946815946474795\n -0.499043758925124\n -0.5042129571957659\n -0.5101961288529362\n -0.5170012616341575\n  ⋮\n -1.537356613268544\n -1.5468788057341616\n -1.556884012872978\n -1.567350616246526\n -1.5782459039766115\n -1.5895261435074444\n -1.6011366077255564\n -1.6130116056633983\n -1.625074551166418\n\n\n\nplot(x2, y2,\n    legend = false,\n    grid = true,\n    gridlinewidth = 2,\n    aspect_ratio = :equal;\n    xaxis = \"x2\",\n    yaxis = \"y2\",\n    xlims = (-2.0, 2.0),\n    ylims = (-2.0, 2.0),\n    widen = true,\n    dpi = 300\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsavefig(\"thumbnail.png\")\n\n\nts = [i for i in t]\nani = @animate for i in eachindex(x2)\n\n    x = [0, x1[i], x2[i]]\n    y = [0, y1[i], y2[i]]\n\n    plot(x, y, aspect_ratio=1.0, legend = false)\n    plot!(xlims = (-L, L), xticks = -L:0.5:L)\n    plot!(ylims = (-L, 1), yticks = -L:0.5:1)\n    scatter!(x, y, color = :red)\n\n    x = x2[1:i]\n    y = y2[1:i]\n\n    plot!(x, y, linecolor = :orange)\n    plot!(xlims = (-L, L), xticks = -L:0.5:L)\n    plot!(ylims = (-L, 1), yticks = -L:0.5:1)\n    scatter!(x, y, \n        color = :orange,\n        markersize = 2,\n        markerstrokewidth = 0,\n        markerstrokecolor = :orange,\n    )\n    annotate!(-1.25, 0.5, \"time= $(@sprintf(\"%.1f\", round(ts[i]; digits=2))) s\")\nend every 10\n\n\ngif(ani, \"ani_julia.mp4\", fps = 10)"
  },
  {
    "objectID": "posts/spherical-astronomy/index.html",
    "href": "posts/spherical-astronomy/index.html",
    "title": "Spherical Astronomy",
    "section": "",
    "text": "This post is based on the project report that was written during the astronomical observation class in 2019."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#introduction",
    "href": "posts/spherical-astronomy/index.html#introduction",
    "title": "Spherical Astronomy",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is the purpose of this post?\nThe purpose of this post is to help you grasp the concept of the celestial sphere and the motions of celestial objects by utilizing the celestial sphere model. In detail, you will learn and understand that diurnal motion is the result of Earth’s rotation, and annual motion is the result of Earth’s revolution. Furthermore, you will gain insights into solar time, sidereal time, the celestial coordinate system (horizontal and equatorial), and coordinate conversion.\n\n\nWhy do you learn this?\nAll astronomical studies are based on the results of astronomical observations. The majority of these observations are conducted on Earth, with some exceptions. However, Earth is not an inertial frame of reference because it both rotates on its axis and orbits the Sun. Consequently, when we observe celestial objects from Earth, we inevitably observe apparent motion due to Earth’s own movements. Therefore, it is essential to identify the apparent motion in the motion of celestial objects. Additionally, all celestial objects are projected onto the celestial sphere, and as a result, their movements occur on the surface of this sphere. For these reasons, learning spherical astronomy is crucial for astronomers."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#celestial-sphere-model",
    "href": "posts/spherical-astronomy/index.html#celestial-sphere-model",
    "title": "Spherical Astronomy",
    "section": "Celestial Sphere Model",
    "text": "Celestial Sphere Model\n\nCelestial Sphere\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Celestial Sphere is an imaginary sphere with an infinite radius centered at the Earth’s center. While most motions of celestial objects are a result of Earth’s motion, for the sake of convenience, we assume that the Earth is fixed, and all celetial objects are embedded on the surface of the Celetial Sphere. We attribute the motions of celestial objects to the motion of the Celestial Sphere.\nSince the Celestial Sphere is a sphere defined by a set of points equidistant from its center, we designate certain special points or circles for convenience. There are various methods for naming these points and circles, but we typically use Earth’s Rotation, Earth’s Revolution, and Observer. For each of them, we begin by selecting a reference axis and a reference plane that is perpendicular to the axis. We then extend the axis and plane infinitely, resulting in three intersections (two points and one circle), each of which is given a specific name. If additional points or circles on the Celestial Sphere are required, we can refer to 9 intersections (6 points and 3 circles) for reference.\n\nEarth’s Rotation\n\nReference axis: Earth’s rotation axis\nReference plane: The plane containing Earth’s equator\n\n\nTwo points on the Celestial Sphere\n\nCelestial North Pole: The point near Earth’s North Pole\nCelestial South Pole: The point near Earth’s South Pole\n\n\n\nOne circle on the Celestial Sphere\n\nCelestial Equator: The great circle determined by the reference plane of Earth’s Rotation\n\n\n\n\nEarth’s Revolution\n\nReference axis: The axis parallel to Earth’s revolution axis and passing through Earth’s center\nReference plane: The plane containing Earth’s orbital path around the Sun\n\n\nTwo points on the Celestial Sphere\n\nEcliptic North Pole: The point near Earth’s North Pole\nEcliptic South Pole: The point near Earth’s South Pole\n\n\n\nOne circle on the Celestial Sphere\n\nEcliptic: The great circle determined by the reference plane of Earth’s Revolution\n\n\n\n\nObserver\n\nReference axis: The line perpendicular to the reference plane at the observer’s location\nReference plane: The plane parallel to the tangent plane of the Earth at the observer’s location and passing though Earth’s center\n\n\nTwo points on the Celestial Sphere\n\nZenith: The point near observer’s head\nNadir: The point near observer’s feet\n\n\n\nOne circle on the Celestial Sphere\n\nHorizon: The great circle determined by the reference plane of observer\n\n\n\n\nAdditional Points and Circles\n\nCircle on the Celestial Sphere\n\nMeridian: The great circle passing though the Celestial North Pole, the Celestial South Pole, the Zenith, and the Nadir\n\n\n\n4 points on the Ecliptic\n\nVernal Equinox : The point of intersection of the Ecliptic and the Celestial Equator, where the Sun moves from the celestial southern hemisphere to the celestial northern hemisphere\nSummer Solstice​ : The point on the Ecliptic nearest to the Celestial North Pole\nAutumnal Equinox : The point of intersection of the Ecliptic and the Celestial Equator, where the Sun moves from the celestial northern hemisphere to the celestial southern hemisphere\nWinter Solstice​ : The point on the Ecliptic nearest to the Celestial South Pole\n\n\n\n4 points on the Horizon\n\nNorth point​ : The point of intersection of the Horizon and the Meridian near the Celestial North Pole\nEast point : The point on the Horizon located 90 degree clockwise from the North point\nSouth point​ : The point of intersection of the Horizon and the Meridian near the Celestial South Pole\nWest point : The point on the Horizon located 90 degree clockwise from the South point\n\n\n\n\nAltitude of Celestial Poles\n\n\n\n\n\n\n\n\n\n\n\n(a) Northern hemisphere observer\n\n\n\n\n\n\n\n\n\n\n\n(b) Southern hemisphere observer\n\n\n\n\n\n\n\nFigure 1: Schematic illustrations explaining the altitude of celestial poles\n\n\n\nFigure 1 (a) represents a situation in which the observer is located in the northern hemisphere, while Figure 1 (b) depicts a case in the southern hemisphere. In these diagrams, the blue line represents the horizon, the red line signifies the celestial equator, the black line is perpendicular to the horizon, the green line is perpendicular to the celestial equator, the orange point represents the observer, the green point represents Polaris, and \\phi denotes the observer’s latitude. From these figures, we can conclude that the altitude of celestial poles is equal to the observer’s latitude.\n\n\n\nEarth’s Rotation\nFirst, let’s review the following concept for convenience.\n\n\n\n\n\n\n\n\n\n\n\n(a) Left-hand screw rule\n\n\n\n\n\n\n\n\n\n\n\n(b) Right-hand screw rule\n\n\n\n\n\n\n\nFigure 2: Illustrations explaining the rotation direction\n\n\n\nFigure 2 (a) illustrates the left-hand screw rule, while Figure 2 (b) illustrates the right-hand screw rule. Imagine you’re directly looking at your thumbs and curling your other fingers. If you focus on your left thumb, the rotation direction is clockwise. Conversely, if you focus on your right thumb, the rotation direction is counterclockwise. To determine the rotation direction, simply observe your thumbs, curl your other fingers, and match the finger’s rotation direction with the given rotation direction: clockwise for the left hand and counterclockwise for the right hand.\n\n\n\n\n\nIn 3D space, the direction of a rotation vector is uniqe, but the rotation direction is not. Therefore, when describing a rotation, it is necessary to specify the viewpoint or indicate the direction of the rotation vector.\n\n\n\n\n\nIf your right thumb points in the direction of (Celestial) North Pole, then the Earth’s rotation follows right-hand screw rule. In other words, if you were to observe Earth’s North Pole from space, you would see that it rotates counterclockwise. Therefore, the direction of the Earth’s rotation vector is from South Pole to North Pole. Due to relative motion, the celestial sphere appears to rotate in the opposite direction around the Earth.\nWhen you rotate the celestial sphere model, you can observe that the Sun and all celestial objects rise in the east and set in the west in all hemispheres. Furthermore, you’ll notice that at the Sun’s upper culmination (its highest point), it appears in the southern sky in the northern hemisphere and in the northern sky in the southern hemisphere. At the poles, the Sun either remains either continuously visible or permanently hidden.\n\n\nSidereal Time\n\n\n\n\n\nThe (Local) Hour Angle is the inversely measured right ascension from the intersection of the celestial equator and the meridian above horizon. When the hour angle of a celestial object is zero, we call that the celestial object is at its upper culmination.\nThe (Local) Sidereal Time is the hour angle of the vernal equinox. By definition, sidereal time \\Theta of a celestial object is equal to H+\\alpha, where H is hour angle, and \\alpha is right ascension of the celestial object.\n\n\\begin{align*}\n\\text{Sidereal time} = \\Theta & = H_{\\text{vernal equinox}} \\\\\n& = H + \\alpha \\\\\n& = \\alpha_{\\text{celestial object at its upper culmination}}\n\\end{align*}\n\nThe (Local) Solar Time is calculated by adding the hour angle of the Sun to 12 hours.\n\n\\text{Solar time} = S = H_{\\odot} + \\text{12 h}\n\nTherefore, we get the following equation.\n\n\\begin{align*}\n\\text{Sidereal time} = \\Theta & = H_{\\odot} + \\alpha_{\\odot} \\\\\n& = S - \\text{12 h} + \\alpha_{\\odot}\n\\end{align*}\n\nThe right ascension of the Sun \\alpha_{\\odot} increases about +\\text{4 min} per day.\n\n\n\nTime\n\\alpha_{\\odot}\n\n\n\n\nVernal Equinox\n\\text{0 h}\n\n\nSummer Solstice\n\\text{6 h}\n\n\nAutumnal Equinox\n\\text{12 h}\n\n\nWinter Solstice\n\\text{18 h}\n\n\n\nThis solar time is actually local solar time. In Korea, the local solar time of Seoul is that of 127° E but our clock uses the local solar time of 135° E (UTC+9). As a result, there is a difference of about 8° (equivalent to 32 minutes). Therefore, in Seoul, the Sun is at its upper culmination at approximately 12:32 KST."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#spherical-coordinate-system",
    "href": "posts/spherical-astronomy/index.html#spherical-coordinate-system",
    "title": "Spherical Astronomy",
    "section": "Spherical Coordinate System",
    "text": "Spherical Coordinate System\n\nHorizontal System vs. Equatorial System\n\nDefinition of (Az, Alt) in Horizontal System\nAzimuth (Az, A) of a celestial object is the angle (or angular distance) measured commonly clockwise from the south point along the horizon. Azimuth values are typically within the range of [0°, 360°].\nAltitude (Alt, a) of a celestial object is the angle (or angular distance) measured from the horizon along the great circle passing through the celestial object and the zenith. Altitude values fall within the range [-90°, +90°]. A positive altitude indicates that the object is above the horizon, while a negative altitude indicates that the object is below the horizon.\n\n\nDefinition of (RA, Dec) in Equatorial System\nRight Ascension (RA, \\alpha) is a celestial longitude, equivalent to Earth’s longtiude, except that it is measured from the vernal equinox instead of Greenwich. It is measured only in the eartward direction.\nDeclination (Dec, \\delta) is a celestial latitude, equivalent to Earth’s latitude, except that N and S are respectively replaced by + and -.\n\n\nA formula to convert (RA, Dec) to (Az, Alt)\n\n\n\n\n\nIn the figure above, the point ​P has coordinates (x, y, z) in xyz Cartesian coordinates system, and (x', y', z') in x'y'z' Cartesian coordinates system. Each of these coordinates has the following relations with spherical coordinates in its respective frame.\n\n\\begin{align*}\nx & = \\cos \\theta \\cos \\psi \\\\\ny & = \\cos \\theta \\sin \\psi \\\\\nz & = \\sin \\theta\n\\end{align*}\n\n\n\\begin{align*}\nx' & = \\cos \\theta' \\cos \\psi' \\\\\ny' & = \\cos \\theta' \\sin \\psi' \\\\\nz' & = \\sin \\theta'\n\\end{align*}\n\nThe primed coordinates are related to the unprimed coordinates by the following equations.\n\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos \\chi & -\\sin \\chi \\\\\n0 & \\sin \\chi & \\cos \\chi\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n\n\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos \\chi & \\sin \\chi \\\\\n0 & -\\sin \\chi & \\cos \\chi\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n\n\n\n\n\n\nIn the figure above, we have \\chi = 90^{\\circ} - \\psi. The values of (x, y, z) and (x', y', z') are determined by the following equations.\n\n\\begin{align*}\nx & = \\cos a \\cos(90^{\\circ} - A) \\\\\n& = \\cos a \\sin A \\\\\ny & = \\cos a \\sin(90^{\\circ} - A) \\\\\n& = \\cos a \\cos A \\\\\nz & = \\sin a\n\\end{align*}\n\n\n\\begin{align*}\nx' & = \\cos \\delta \\cos(90^{\\circ} - H) \\\\\n& = \\cos \\delta \\sin H \\\\\ny' & = \\cos \\delta \\sin(90^{\\circ} - H) \\\\\n& = \\cos \\delta \\cos H \\\\\nz' & = \\sin \\delta\n\\end{align*}\n\nThus, we get the following equations.\n\n\\begin{align*}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos (90^{\\circ} - \\phi) & -\\sin (90^{\\circ} - \\phi) \\\\\n0 & \\sin (90^{\\circ} - \\phi) & \\cos (90^{\\circ} - \\phi)\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\sin \\phi & -\\cos \\phi \\\\\n0 & \\cos \\phi & \\sin \\phi\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n\\end{align*}\n\n\n\\begin{align*}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos (90^{\\circ} - \\phi) & \\sin (90^{\\circ} - \\phi) \\\\\n0 & -\\sin (90^{\\circ} - \\phi) & \\cos (90^{\\circ} - \\phi)\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\sin \\phi & \\cos \\phi \\\\\n0 & -\\cos \\phi & \\sin \\phi\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n\\end{align*}\n\nIn conclusion, we get the following two conversion fomulas.\nWhen we know (\\alpha=\\Theta-H, \\delta, \\phi), we can obtain (A, a) by using following equations.\n\n\\begin{align*}\n\\cos a \\sin A & = \\cos \\delta \\sin H \\\\\n\\cos a \\cos A & = \\sin \\phi \\cos \\delta \\cos H - \\cos \\phi \\sin \\delta \\\\\n\\sin a & = \\cos \\phi \\cos \\delta \\cos H + \\sin \\phi \\sin \\delta\n\\end{align*}\n\nWhen we know (A, a, \\phi), we can obtain (\\alpha=\\Theta-H, \\delta) by using following equations.\n\n\\begin{align*}\n\\cos \\delta \\sin H & = \\cos a \\sin A \\\\\n\\cos \\delta \\cos H & = \\sin \\phi \\cos a \\cos A + \\cos \\phi \\sin a \\\\\n\\sin \\delta & = - \\cos \\phi \\cos a \\cos A + \\sin \\phi \\sin a\n\\end{align*}\n\nTherefore, we can conclude that we need to know sidereal time and latitude when performing conversions between (RA, Dec) and (Az, Alt).\n\n\n\nTargets on Meridian\nLet \\alpha' be the right ascension of celestial object at its upper culmination. By definition of sidereal time, the following equation holds.\n\n\\begin{align*}\n\\Theta & = S - \\text{12 h} + \\alpha_\\odot \\\\\n& = \\alpha'\n\\end{align*}\n\nThus we get the following equation.\n\nS = \\text{12 h} - \\alpha_\\odot + \\alpha'\n\n\n\nHour Angle of Targets\nBy definition of sidereal time, the following equation holds.\n\n\\begin{align*}\n\\Theta & = S - \\text{12 h} + \\alpha_\\odot \\\\\n& = H + \\alpha\n\\end{align*}\n\nThus we get the following equation.\n\nH = S - \\text{12 h} + \\alpha_\\odot - \\alpha"
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#references",
    "href": "posts/spherical-astronomy/index.html#references",
    "title": "Spherical Astronomy",
    "section": "References",
    "text": "References\n\nIntroductory Astronomy: The Celestial Sphere, http://astro.wsu.edu/worthey/astro/html/lec-celestial-sph.html\nMeridian (astronomy), https://en.wikipedia.org/wiki/Meridian_(astronomy)\nRight hand screw rule, https://www3.eng.cam.ac.uk/~hemh1/gyroscopes/screwrule.html\nRotation Vs. Revolution: What Are The Differences?, https://differencecamp.com/rotation-vs-revolution\nSUPPLEMENT: MOTIONS IN THE SKY & COORDINATE SYSTEMS, https://rwoconne.github.io/rwoclass/astr1230/motions-coords.html\nHannu Karttunen et al. ​Fundamental Astronomy​. Sixth Edition. Springer (2016)"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "",
    "text": "In this post, I perform binary semantic segmentation in PyTorch using a Fully Convolutional Network (FCN) with a ResNet-50 backbone. The model is pre-trained on a subset of COCO using only the 20 categories from the Pascal VOC dataset, and I fine-tune it on the balloon dataset from the Mask R-CNN repository.\nI referred to the code for balloon dataset of Mask R-CNN repository and the colab tutorial of detectron2 for the balloon dataset; and the torchvision semantic segmentaion reference training scripts for the overall code structure."
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#semantic-segmentation",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#semantic-segmentation",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "",
    "text": "In this post, I perform binary semantic segmentation in PyTorch using a Fully Convolutional Network (FCN) with a ResNet-50 backbone. The model is pre-trained on a subset of COCO using only the 20 categories from the Pascal VOC dataset, and I fine-tune it on the balloon dataset from the Mask R-CNN repository.\nI referred to the code for balloon dataset of Mask R-CNN repository and the colab tutorial of detectron2 for the balloon dataset; and the torchvision semantic segmentaion reference training scripts for the overall code structure."
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#balloon-dataset",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#balloon-dataset",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "Balloon dataset",
    "text": "Balloon dataset\nDownload the balloon dataset from Mask R-CNN repository and transform it into a format that is compatible with the binary semantic segmentation task.\ndataset\n├── train\n│   ├── images\n│   │   ├── &lt;file1&gt;.png\n│   │   ├── &lt;file2&gt;.png\n│   │   └── ...\n│   └── masks\n│       ├── &lt;file1&gt;.png\n│       ├── &lt;file2&gt;.png\n│       └── ...\n└── val\n    ├── images\n    │   ├── &lt;file1&gt;.png\n    │   ├── &lt;file2&gt;.png\n    │   └── ...\n    └── masks\n        ├── &lt;file1&gt;.png\n        ├── &lt;file2&gt;.png\n        └── ...\nDownload the dataset as an zip file.\n\nimport requests\n\nurl = \"https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\"\nfilename = \"balloon_dataset.zip\"\n\nresponse = requests.get(url)\nwith open(filename, \"wb\") as file:\n    file.write(response.content)\n\nExtract the zip file.\n\nimport zipfile\n\nwith zipfile.ZipFile(filename, \"r\") as file:\n    file.extractall()\n\nRemove the unnecessary folder.\n\nimport shutil\n\nshutil.rmtree(\"__MACOSX\")\n\nConvert the dataset into the binary semantic segmentation format.\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport skimage\n\ndef create_mask(dataset_dir, new_dataset_dir):\n    dataset_dir = Path(dataset_dir)\n\n    img_dir = Path(new_dataset_dir) / \"images\"\n    mask_dir = Path(new_dataset_dir) / \"masks\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    mask_dir.mkdir(parents=True, exist_ok=True)\n\n    with open(dataset_dir / \"via_region_data.json\") as file:\n        annotations = json.load(file)\n\n    for idx, v in enumerate(annotations.values()):\n        img = skimage.io.imread(dataset_dir / v[\"filename\"])\n        height, width = img.shape[:2]\n\n        regions = v[\"regions\"]\n\n        mask = np.zeros([height, width], dtype=np.uint8)\n        for region in regions.values():\n            anno = region[\"shape_attributes\"]\n            px = anno[\"all_points_x\"]\n            py = anno[\"all_points_y\"]\n            poly = np.array([[y, x] for x, y in zip(px, py)])\n            mask += skimage.draw.polygon2mask((height, width), poly)\n        mask = mask.astype(np.bool).astype(np.uint8)\n\n        skimage.io.imsave(img_dir / (v[\"filename\"][:-4] + \".png\"), img)\n        skimage.io.imsave(mask_dir / (v[\"filename\"][:-4] + \".png\"), mask)\n\n\ncreate_mask(\"balloon/train\", \"dataset/train\")\ncreate_mask(\"balloon/val\", \"dataset/val\")"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#dataset",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#dataset",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "Dataset",
    "text": "Dataset\n\nfrom pathlib import Path\n\nfrom torch.utils.data import Dataset\nfrom torchvision import tv_tensors\nfrom torchvision.io import read_image\n\nclass BalloonDataset(Dataset):\n    def __init__(self, dataset_dir, transform=None):\n\n        self.dataset_dir = Path(dataset_dir)\n        self.transform = transform\n\n        self.img_dir = self.dataset_dir / \"images\"\n        self.mask_dir = self.dataset_dir / \"masks\"\n\n        self.imgs = list(self.img_dir.glob(\"*.png\"))\n\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n    def __getitem__(self, idx):\n        img_path = self.imgs[idx]\n        mask_path = self.mask_dir / img_path.name\n\n        img = read_image(img_path)\n        mask = read_image(mask_path)[0]\n\n        img = tv_tensors.Image(img)\n        mask = tv_tensors.Mask(mask)\n\n        sample = {\n            \"image\": img,\n            \"mask\": mask\n        }\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\ncmp = ListedColormap(['black','white'])\n\n\ndataset = BalloonDataset(\"dataset/train\")\n\nsample = dataset[0]\n\nprint(type(sample[\"image\"]))\nprint(sample[\"image\"].shape)\nprint(sample[\"image\"].min(), sample[\"image\"].max())\n\nprint()\n\nprint(type(sample[\"mask\"]))\nprint(sample[\"mask\"].shape)\nprint(sample[\"mask\"].min(), sample[\"mask\"].max())\n\nplt.subplot(1,2,1)\nplt.imshow(sample[\"image\"].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(sample[\"mask\"], cmap=cmp)\nplt.title(\"Mask\")\nplt.axis(\"off\")\nplt.show()\n\n&lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntorch.Size([3, 1365, 2048])\ntensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\n\n&lt;class 'torchvision.tv_tensors._mask.Mask'&gt;\ntorch.Size([1365, 2048])\ntensor(0, dtype=torch.uint8) tensor(1, dtype=torch.uint8)\n\n\n\n\n\n\n\n\n\n\ndataset = BalloonDataset(\"dataset/val\")\n\nsample = dataset[0]\n\nprint(type(sample[\"image\"]))\nprint(sample[\"image\"].shape)\nprint(sample[\"image\"].min(), sample[\"image\"].max())\n\nprint()\n\nprint(type(sample[\"mask\"]))\nprint(sample[\"mask\"].shape)\nprint(sample[\"mask\"].min(), sample[\"mask\"].max())\n\nplt.subplot(1,2,1)\nplt.imshow(sample[\"image\"].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(sample[\"mask\"], cmap=cmp)\nplt.title(\"Mask\")\nplt.axis(\"off\")\nplt.show()\n\n&lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntorch.Size([3, 1536, 2048])\ntensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\n\n&lt;class 'torchvision.tv_tensors._mask.Mask'&gt;\ntorch.Size([1536, 2048])\ntensor(0, dtype=torch.uint8) tensor(1, dtype=torch.uint8)"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#transforms",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#transforms",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "Transforms",
    "text": "Transforms\n\nimport torch\nfrom torchvision.transforms import v2\n\n\nmean = (0.485, 0.456, 0.406) # ImageNet\nstd = (0.229, 0.224, 0.225) # ImageNet\n\nbase_size = 520\n\ncrop_size = 480\nhflip_prob = 0.5\n\ntransform_train = v2.Compose(\n    [\n        v2.ToImage(),\n        v2.Resize(size=(base_size, base_size)),\n        v2.RandomHorizontalFlip(p=hflip_prob),\n        v2.RandomCrop(size=(crop_size, crop_size)),\n        v2.ToDtype(dtype={tv_tensors.Image:torch.float32, tv_tensors.Mask:torch.int64, \"others\":None}, scale=True),\n        v2.Normalize(mean=mean, std=std), \n        v2.ToPureTensor()\n    ]\n)\n\ntransform_val = v2.Compose(\n    [\n        v2.ToImage(),\n        v2.Resize(size=base_size),\n        v2.ToDtype(dtype={tv_tensors.Image:torch.float32, tv_tensors.Mask:torch.int64, \"others\":None}, scale=True),\n        v2.Normalize(mean=mean, std=std), # ImageNet mean and std\n        v2.ToPureTensor()\n    ]\n)\n\n\ndataset = BalloonDataset(\"dataset/train\", transform=transform_train)\ndataset_val = BalloonDataset(\"dataset/val\", transform=transform_val)\n\n\nsample = dataset[0]\n\nprint(type(sample[\"image\"]))\nprint(sample[\"image\"].shape)\nprint(sample[\"image\"].min(), sample[\"image\"].max())\n\nprint()\n\nprint(type(sample[\"mask\"]))\nprint(sample[\"mask\"].shape)\nprint(sample[\"mask\"].min(), sample[\"mask\"].max())\n\nplt.subplot(1,2,1)\nplt.imshow(sample[\"image\"].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(sample[\"mask\"], cmap=cmp)\nplt.title(\"Mask\")\nplt.axis(\"off\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9295317..2.622571].\n\n\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([3, 480, 480])\ntensor(-1.9295) tensor(2.6226)\n\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([480, 480])\ntensor(0) tensor(1)\n\n\n\n\n\n\n\n\n\n\nsample = dataset_val[0]\n\nprint(type(sample[\"image\"]))\nprint(sample[\"image\"].shape)\nprint(sample[\"image\"].min(), sample[\"image\"].max())\n\nprint()\n\nprint(type(sample[\"mask\"]))\nprint(sample[\"mask\"].shape)\nprint(sample[\"mask\"].min(), sample[\"mask\"].max())\n\nplt.subplot(1,2,1)\nplt.imshow(sample[\"image\"].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(sample[\"mask\"], cmap=cmp)\nplt.title(\"Mask\")\nplt.axis(\"off\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.7870152..2.4285715].\n\n\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([3, 520, 693])\ntensor(-1.7870) tensor(2.4286)\n\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([520, 693])\ntensor(0) tensor(1)"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#dataloader",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#dataloader",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "DataLoader",
    "text": "DataLoader\n\nnum_workers = 0\nbatch_size = 2\n\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ndef collate_fn(batch):\n    images = torch.stack([sample[\"image\"] for sample in batch])\n    masks = torch.stack([sample[\"mask\"] for sample in batch])\n    return images, masks\n\ndata_loader = DataLoader(dataset, \n                         batch_size=batch_size, \n                         sampler=RandomSampler(dataset),\n                         num_workers=num_workers, \n                         drop_last=True,\n                         collate_fn=collate_fn)\n\ndata_loader_val = DataLoader(dataset_val,\n                             batch_size=1,\n                             sampler=SequentialSampler(dataset_val),\n                             num_workers=num_workers,\n                             drop_last=False,\n                             collate_fn=collate_fn)\n\n\nbatch = next(iter(data_loader))\nprint(batch[0].shape, batch[1].shape)\nprint(batch[0].dtype, batch[1].dtype)\nprint(torch.unique(batch[1]))\n\nplt.subplot(1,2,1)\nplt.imshow(batch[0][0].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(batch[1][0], cmap=cmp)\nplt.title(\"Mask\")\nplt.axis(\"off\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.117904..2.64].\n\n\ntorch.Size([2, 3, 480, 480]) torch.Size([2, 480, 480])\ntorch.float32 torch.int64\ntensor([0, 1])\n\n\n\n\n\n\n\n\n\n\nbatch = next(iter(data_loader_val))\nprint(batch[0].shape, batch[1].shape)\nprint(batch[0].dtype, batch[1].dtype)\nprint(torch.unique(batch[1]))\n\nplt.subplot(1,2,1)\nplt.imshow(batch[0][0].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(batch[1][0], cmap=cmp)\nplt.title(\"Mask\")\nplt.axis(\"off\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.7870152..2.4285715].\n\n\ntorch.Size([1, 3, 520, 693]) torch.Size([1, 520, 693])\ntorch.float32 torch.int64\ntensor([0, 1])"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#model",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#model",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "Model",
    "text": "Model\n\nimport torchvision\nfrom torch.nn import Conv2d\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_classes = 2\n\nmodel_name = \"fcn_resnet50\"\nmodel_weights = \"COCO_WITH_VOC_LABELS_V1\"\naux_loss = True\n\n\nmodel = torchvision.models.get_model(\n    name=model_name,\n    weights=model_weights,\n    aux_loss=aux_loss,\n)\n\nout_in_channels = model.classifier[4].in_channels\nmodel.classifier[4] = Conv2d(out_in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1))\n\naux_in_channels = model.aux_classifier[4].in_channels\nmodel.aux_classifier[4] = Conv2d(aux_in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1))\n\nmodel = model.to(device)\n\n\nmodel\n\nFCN(\n  (backbone): IntermediateLayerGetter(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n  )\n  (classifier): FCNHead(\n    (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (aux_classifier): FCNHead(\n    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n  )\n)\n\n\n\nwith torch.inference_mode():\n    image, target = next(iter(data_loader))\n    image, target = image.to(device), target.to(device)\n    output = model(image)\n\n    print(output.keys())\n\n    print(output[\"out\"].shape)\n    print(output[\"aux\"].shape)\n\n    output = output[\"out\"]\n\n    image = image.detach().cpu()\n    true_mask = target.detach().cpu()\n    pred_mask = output.argmax(1).detach().cpu()\n    probs = output.softmax(1).detach().cpu()\n\nprint(image.shape, target.shape)\nprint(true_mask.shape, pred_mask.shape)\nprint(output.shape, probs.shape)\n\np = 0.5\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1,5,1)\nplt.imshow(image[0].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,2)\nplt.imshow(true_mask[0], cmap=cmp)\nplt.title(\"True Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,3)\nplt.imshow(pred_mask[0], cmap=cmp)\nplt.title(\"Predicted Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,4)\nplt.imshow(probs[0, 1], clim=(0, 1))\nplt.title(\"Probability of class 1\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,5)\nplt.imshow((probs[0, 1] &gt; p).to(torch.bool), cmap=cmp)\nplt.title(f\"Probability of class 1 &gt; {p}\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.0836544..2.64].\n\n\nodict_keys(['out', 'aux'])\ntorch.Size([2, 2, 480, 480])\ntorch.Size([2, 2, 480, 480])\ntorch.Size([2, 3, 480, 480]) torch.Size([2, 480, 480])\ntorch.Size([2, 480, 480]) torch.Size([2, 480, 480])\ntorch.Size([2, 2, 480, 480]) torch.Size([2, 2, 480, 480])\n\n\n\n\n\n\n\n\n\n\nwith torch.inference_mode():\n    image, target = next(iter(data_loader_val))\n    image, target = image.to(device), target.to(device)\n    output = model(image)\n    output = output[\"out\"]\n\n    image = image.detach().cpu()\n    true_mask = target.detach().cpu()\n    pred_mask = output.argmax(1).detach().cpu()\n    probs = output.softmax(1).detach().cpu()\n\nprint(image.shape, target.shape)\nprint(true_mask.shape, pred_mask.shape)\nprint(output.shape, probs.shape)\n\np = 0.5\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1,5,1)\nplt.imshow(image[0].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,2)\nplt.imshow(true_mask[0], cmap=cmp)\nplt.title(\"True Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,3)\nplt.imshow(pred_mask[0], cmap=cmp)\nplt.title(\"Predicted Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,4)\nplt.imshow(probs[0, 1], clim=(0, 1))\nplt.title(\"Probability of class 1\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,5)\nplt.imshow((probs[0, 1] &gt; p).to(torch.bool), cmap=cmp)\nplt.title(f\"Probability of class 1 &gt; {p}\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.7870152..2.4285715].\n\n\ntorch.Size([1, 3, 520, 693]) torch.Size([1, 520, 693])\ntorch.Size([1, 520, 693]) torch.Size([1, 520, 693])\ntorch.Size([1, 2, 520, 693]) torch.Size([1, 2, 520, 693])"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#preparing-training",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#preparing-training",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "Preparing Training",
    "text": "Preparing Training\n\nlr = 0.01\nmomentum = 0.9\nweight_decay = 1e-4\n\nepochs = 10\nprint_freq = 5\n\n\nparams = [\n    {\"params\": [p for p in model.backbone.parameters() if p.requires_grad]},\n    {\"params\": [p for p in model.classifier.parameters() if p.requires_grad]},\n    {\"params\": [p for p in model.aux_classifier.parameters() if p.requires_grad]},\n]\n\n\noptimizer = torch.optim.SGD(params, \n                            lr=lr, \n                            momentum=momentum, \n                            weight_decay=weight_decay)\n\n\niters_per_epoch = len(data_loader)\niters_per_epoch\n\n30\n\n\n\nlr_scheduler = torch.optim.lr_scheduler.PolynomialLR(\n    optimizer, \n    total_iters=iters_per_epoch * epochs, \n    power=0.9\n)\n\n\nclass ConfusionMatrix:\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.mat = None\n\n    def update(self, true, pred):\n        n = self.num_classes\n        if self.mat is None:\n            self.mat = torch.zeros((n, n), dtype=torch.int64, device=true.device)\n\n        with torch.inference_mode():\n            # T, P\n            # 0, 0 =&gt; 2*0 + 0 = 0 True Negative\n            # 0, 1 =&gt; 2*0 + 1 = 1 False Positive\n            # 1, 0 =&gt; 2*1 + 0 = 2 False Negative\n            # 1, 1 =&gt; 2*1 + 1 = 3 True Positive\n            k = (true &gt;= 0) & (true &lt; n)\n            inds = n * true[k].to(torch.int64) + pred[k]\n            self.mat += torch.bincount(inds, minlength=n**2).reshape(n, n)\n\n    def compute(self):\n        # Confusion Matrix\n        # [[TN, FP],\n        #  [FN, TP]]\n        h = self.mat.float()\n\n        # TN, TP\n        # TN -&gt; Correctly predicted as class 0\n        # TP -&gt; Correctly predicted as class 1\n        diag = torch.diag(h)\n\n        # Overall accuracy\n        # (TN + TP) / (TN + FP + FN + TP)\n        acc_global = diag.sum() / h.sum()\n\n        # Actual Negative, Actual Positive\n        # (TN + FP) -&gt; Actual Negative -&gt; # of pixels that are class 0\n        # (FN + TP) -&gt; Actual Positive -&gt; # of pixels that are class 1\n        hsum1 = h.sum(1)\n\n        # Predicted Negative, Predicted Positive\n        # (TN + FN) -&gt; Predicted Negative -&gt; # of pixels predicted as class 0\n        # (FP + TP) -&gt; Predicted Positive -&gt; # of pixels predicted as class 1\n        hsum0 = h.sum(0)\n\n        # Accuracy of class 0, Accuracy of class 1\n        # (TN) / (TN + FP) -&gt; Accuracy of class 0 \n        # (TN + FP = Actual Negative -&gt; # of pixels that are class 0)\n        #\n        # (TP) / (FN + TP) -&gt; Accuracy of class 1  \n        # (FN + TP = Actual Positive -&gt; # of pixels that are class 1)\n        acc = diag / hsum1\n\n        # IoU for class 0, IoU for class 1\n        # (TN) / ((TN + FP) + (TN + FN) - TN) =&gt; TN / (TN + FP + FN) -&gt; IoU for class 0\n        # (TP) / ((FN + TP) + (FP + TP) - TP) =&gt; TP / (FN + FP + TP) -&gt; IoU for class 1\n        iou = diag / (hsum1 + hsum0 - diag)\n        return acc_global, acc, iou\n\n\ndef evaluate(model, data_loader, device, num_classes):\n    model.eval()\n    \n    confmat = ConfusionMatrix(num_classes)\n\n    with torch.inference_mode():\n        for image, target in data_loader:\n            image, target = image.to(device), target.to(device)\n            output = model(image)\n            output = output[\"out\"]\n\n            true_mask = target.flatten()\n            pred_mask = output.argmax(1).flatten()\n\n            confmat.update(true_mask, pred_mask)\n\n    acc_global, acc, iou = confmat.compute()\n    return acc_global, acc, iou\n\n\ndef criterion(outputs, target):\n    \"\"\"\n    outputs: {\"out\": [batch_size, num_classes, H, W], \"aux\": [batch_size, num_classes, H, W]}\n    target : [batch_size, M, M]\n    \"\"\"\n    losses = {}\n    for name, output in outputs.items():\n        losses[name] = torch.nn.functional.cross_entropy(output, target, ignore_index=255)\n\n    return losses[\"out\"] + 0.5 * losses[\"aux\"]\n\n\ndef train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, device, print_freq):\n    model.train()\n    \n    for idx, (image, target) in enumerate(data_loader):\n        image, target = image.to(device), target.to(device)\n\n        output = model(image)\n        loss = criterion(output, target)\n    \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        lr_scheduler.step()\n\n        if idx % print_freq == 0:\n            print(f\"iteration: {idx}, loss: {loss.item()}, lr: {optimizer.param_groups[0]['lr']}\")\n\n\nimport datetime\nfrom time import perf_counter\n\n\noutput_dir = Path(\"output\")\noutput_dir.mkdir(exist_ok=True, parents=True)"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#training-loop",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#training-loop",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "Training Loop",
    "text": "Training Loop\n\nstart_time = perf_counter()\n\nmean_iou_best = 0\n\nfor epoch in range(epochs):\n    print(f\"epoch: {epoch}\")\n    \n    train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, device, print_freq)\n    acc_global, acc, iou = evaluate(model, data_loader_val, device, num_classes)\n\n    print(\"overall accuracy: {:.1f}\".format(acc_global.item() * 100))\n    print(\"per-class accuracy: {}\".format([f\"{i:.1f}\" for i in (acc * 100).tolist()]))\n    print(\"per-class IoU: {}\".format([f\"{i:.1f}\" for i in (iou * 100).tolist()]))\n    mean_iou = iou.mean().item() * 100\n    print(\"mean IoU: {:.1f}\".format(mean_iou))\n\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"lr_scheduler\": lr_scheduler.state_dict(),\n        \"epoch\": epoch,\n    }\n    torch.save(model.state_dict(), output_dir / f\"model_{epoch}.pth\")\n    torch.save(checkpoint, output_dir / \"checkpoint.pth\")\n\n    if mean_iou &gt; mean_iou_best:\n        mean_iou_best = mean_iou\n        torch.save(model.state_dict(), output_dir / \"model_best.pth\")\n\ntotal_time = perf_counter() - start_time\ntotal_time_str = str(datetime.timedelta(seconds=int(total_time)))\nprint(f\"Training time {total_time_str}\")\n\nepoch: 0\niteration: 0, loss: 1.01277494430542, lr: 0.009969994993878174\niteration: 5, loss: 0.7877825498580933, lr: 0.009819818665965752\niteration: 10, loss: 0.5762972831726074, lr: 0.009669386705882587\niteration: 15, loss: 1.0311238765716553, lr: 0.009518694243963476\niteration: 20, loss: 0.2171248197555542, lr: 0.009367736230317173\niteration: 25, loss: 0.176581472158432, lr: 0.009216507424802825\noverall accuracy: 96.6\nper-class accuracy: ['96.5', '98.0']\nper-class IoU: ['96.3', '69.3']\nmean IoU: 82.8\nepoch: 1\niteration: 0, loss: 0.3303796052932739, lr: 0.009065002386255556\niteration: 5, loss: 0.49728652834892273, lr: 0.00891321546089\niteration: 10, loss: 0.14698544144630432, lr: 0.008761140769802247\niteration: 15, loss: 0.25900280475616455, lr: 0.008608772195481283\niteration: 20, loss: 0.23461148142814636, lr: 0.008456103367230421\niteration: 25, loss: 0.15549582242965698, lr: 0.008303127645386885\noverall accuracy: 99.1\nper-class accuracy: ['99.7', '92.5']\nper-class IoU: ['99.0', '89.1']\nmean IoU: 94.1\nepoch: 2\niteration: 0, loss: 0.18801620602607727, lr: 0.008149838104213793\niteration: 5, loss: 0.49533677101135254, lr: 0.007996227513322691\niteration: 10, loss: 0.1928689032793045, lr: 0.00784228831746623\niteration: 15, loss: 0.32615792751312256, lr: 0.007688012614519259\niteration: 20, loss: 0.0866878479719162, lr: 0.007533392131441786\niteration: 25, loss: 0.19141457974910736, lr: 0.007378418197988367\noverall accuracy: 98.7\nper-class accuracy: ['99.0', '94.9']\nper-class IoU: ['98.6', '84.7']\nmean IoU: 91.6\nepoch: 3\niteration: 0, loss: 0.2648457884788513, lr: 0.007223081717895036\niteration: 5, loss: 0.11991502344608307, lr: 0.0070673731372354115\niteration: 10, loss: 0.23792551457881927, lr: 0.006911282409591538\niteration: 15, loss: 0.18067464232444763, lr: 0.006754798957630521\niteration: 20, loss: 0.1413581818342209, lr: 0.006597911630613654\niteration: 25, loss: 0.3505668342113495, lr: 0.006440608657288157\noverall accuracy: 99.1\nper-class accuracy: ['99.6', '92.4']\nper-class IoU: ['99.0', '88.7']\nmean IoU: 93.8\nepoch: 4\niteration: 0, loss: 0.11682246625423431, lr: 0.00628287759352032\niteration: 5, loss: 0.5209360718727112, lr: 0.006124705263919317\niteration: 10, loss: 0.11417791247367859, lr: 0.005966077696569146\niteration: 15, loss: 0.16387423872947693, lr: 0.005806980049826368\niteration: 20, loss: 0.13538804650306702, lr: 0.005647396529947093\niteration: 25, loss: 0.05533313378691673, lr: 0.005487310298068799\noverall accuracy: 99.5\nper-class accuracy: ['99.8', '95.0']\nper-class IoU: ['99.4', '93.1']\nmean IoU: 96.3\nepoch: 5\niteration: 0, loss: 0.10098870098590851, lr: 0.005326703364779764\niteration: 5, loss: 0.047345079481601715, lr: 0.005165556470146127\niteration: 10, loss: 0.06248948723077774, lr: 0.00500384894661411\niteration: 15, loss: 0.07856252789497375, lr: 0.004841558561636509\niteration: 20, loss: 0.09252874553203583, lr: 0.004678661336152997\niteration: 25, loss: 0.05078906565904617, lr: 0.004515131334135518\noverall accuracy: 99.3\nper-class accuracy: ['99.8', '93.2']\nper-class IoU: ['99.2', '91.1']\nmean IoU: 95.2\nepoch: 6\niteration: 0, loss: 0.05498559772968292, lr: 0.004350940417227879\niteration: 5, loss: 0.10427920520305634, lr: 0.00418605795697228\niteration: 10, loss: 0.05495597422122955, lr: 0.004020450495098764\niteration: 15, loss: 0.033951517194509506, lr: 0.0038540813396768115\niteration: 20, loss: 0.21207734942436218, lr: 0.0036869100813333193\niteration: 25, loss: 0.08376838266849518, lr: 0.0035188920088502548\noverall accuracy: 99.4\nper-class accuracy: ['99.7', '94.9']\nper-class IoU: ['99.3', '92.1']\nmean IoU: 95.7\nepoch: 7\niteration: 0, loss: 0.08038052916526794, lr: 0.0033499773967063965\niteration: 5, loss: 0.032610610127449036, lr: 0.003180110627669994\niteration: 10, loss: 0.025917738676071167, lr: 0.003009229100067209\niteration: 15, loss: 0.0338129997253418, lr: 0.0028372618497656244\niteration: 20, loss: 0.03268418461084366, lr: 0.002664127787853039\niteration: 25, loss: 0.04463454335927963, lr: 0.002489733410844936\noverall accuracy: 99.4\nper-class accuracy: ['99.8', '95.3']\nper-class IoU: ['99.4', '92.6']\nmean IoU: 96.0\nepoch: 8\niteration: 0, loss: 0.035370051860809326, lr: 0.002313969771367498\niteration: 5, loss: 0.026227110996842384, lr: 0.0021367083864453854\niteration: 10, loss: 0.05721791461110115, lr: 0.0019577955758817893\niteration: 15, loss: 0.17568983137607574, lr: 0.00177704440273342\niteration: 20, loss: 0.060686126351356506, lr: 0.0015942228040916459\niteration: 25, loss: 0.04033160209655762, lr: 0.0014090353734474502\noverall accuracy: 99.4\nper-class accuracy: ['99.8', '93.6']\nper-class IoU: ['99.3', '91.9']\nmean IoU: 95.6\nepoch: 9\niteration: 0, loss: 0.04921840876340866, lr: 0.001221093920078196\niteration: 5, loss: 0.03143635392189026, lr: 0.0010298666348361786\niteration: 10, loss: 0.06234569475054741, lr: 0.0008345821880550068\niteration: 15, loss: 0.04337213188409805, lr: 0.0006340245914362474\niteration: 20, loss: 0.12797394394874573, lr: 0.0004259995391188707\niteration: 25, loss: 0.03848736360669136, lr: 0.00020532643320784173\noverall accuracy: 99.4\nper-class accuracy: ['99.8', '95.0']\nper-class IoU: ['99.3', '92.4']\nmean IoU: 95.8\nTraining time 0:00:47\n\n\n\nprint(\"Best mean IoU: {:.1f}\".format(mean_iou_best))\n\nBest mean IoU: 96.3"
  },
  {
    "objectID": "posts/semantic-segmentation-fcn-resnet50/index.html#evaluation",
    "href": "posts/semantic-segmentation-fcn-resnet50/index.html#evaluation",
    "title": "Semantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone",
    "section": "Evaluation",
    "text": "Evaluation\n\nmodel = torchvision.models.get_model(\n    name=model_name,\n    weights=model_weights,\n    aux_loss=aux_loss,\n)\n\nout_in_channels = model.classifier[4].in_channels\nmodel.classifier[4] = Conv2d(out_in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1))\n\naux_in_channels = model.aux_classifier[4].in_channels\nmodel.aux_classifier[4] = Conv2d(aux_in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1))\n\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(output_dir / \"model_best.pth\", map_location=device, weights_only=True))\n\n&lt;All keys matched successfully&gt;\n\n\n\nwith torch.inference_mode():\n    image, target = next(iter(data_loader))\n    image, target = image.to(device), target.to(device)\n    output = model(image)\n    output = output[\"out\"]\n\n    image = image.detach().cpu()\n    true_mask = target.detach().cpu()\n    pred_mask = output.argmax(1).detach().cpu()\n    probs = output.softmax(1).detach().cpu()\n\nprint(image.shape, target.shape)\nprint(true_mask.shape, pred_mask.shape)\nprint(output.shape, probs.shape)\n\np = 0.9\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1,5,1)\nplt.imshow(image[0].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,2)\nplt.imshow(true_mask[0], cmap=cmp)\nplt.title(\"True Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,3)\nplt.imshow(pred_mask[0], cmap=cmp)\nplt.title(\"Predicted Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,4)\nplt.imshow(probs[0, 1], clim=(0, 1))\nplt.title(\"Probability of class 1\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,5)\nplt.imshow((probs[0, 1] &gt; p).to(torch.bool), cmap=cmp)\nplt.title(f\"Probability of class 1 &gt; {p}\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.0494049..2.500567].\n\n\ntorch.Size([2, 3, 480, 480]) torch.Size([2, 480, 480])\ntorch.Size([2, 480, 480]) torch.Size([2, 480, 480])\ntorch.Size([2, 2, 480, 480]) torch.Size([2, 2, 480, 480])\n\n\n\n\n\n\n\n\n\n\nidx = 4\n\nwith torch.inference_mode():\n    data_loader_val_iter = iter(data_loader_val)\n    for _ in range(idx):\n        image, target = next(data_loader_val_iter)\n    image, target = image.to(device), target.to(device)\n    output = model(image)\n    output = output[\"out\"]\n\n    image = image.detach().cpu()\n    true_mask = target.detach().cpu()\n    pred_mask = output.argmax(1).detach().cpu()\n    probs = output.softmax(1).detach().cpu()\n\nprint(image.shape, target.shape)\nprint(true_mask.shape, pred_mask.shape)\nprint(output.shape, probs.shape)\n\np = 0.9\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1,5,1)\nplt.imshow(image[0].permute(1,2,0))\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,2)\nplt.imshow(true_mask[0], cmap=cmp)\nplt.title(\"True Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,3)\nplt.imshow(pred_mask[0], cmap=cmp)\nplt.title(\"Predicted Mask\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,4)\nplt.imshow(probs[0, 1], clim=(0, 1))\nplt.title(\"Probability of class 1\")\nplt.axis(\"off\")\n\nplt.subplot(1,5,5)\nplt.imshow((probs[0, 1] &gt; p).to(torch.bool), cmap=cmp)\nplt.title(f\"Probability of class 1 &gt; {p}\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.0665298..2.64].\n\n\ntorch.Size([1, 3, 804, 520]) torch.Size([1, 804, 520])\ntorch.Size([1, 804, 520]) torch.Size([1, 804, 520])\ntorch.Size([1, 2, 804, 520]) torch.Size([1, 2, 804, 520])"
  },
  {
    "objectID": "posts/explain-words/index.html",
    "href": "posts/explain-words/index.html",
    "title": "천문용어 설명",
    "section": "",
    "text": "2019년 우주관측 수업에서 2020 역서 09. 천문상수와 자료 설명용으로 작성한 글"
  },
  {
    "objectID": "posts/explain-words/index.html#국제천문연맹-천문상수",
    "href": "posts/explain-words/index.html#국제천문연맹-천문상수",
    "title": "천문용어 설명",
    "section": "국제천문연맹 천문상수",
    "text": "국제천문연맹 천문상수\n\nTT, TCG, TCB, TDB\n\n\\text{TCG} = \\text{TT} + L_G \\times (\\text{JD} - 2443144.5) \\times 86400 \\ \\text{s}\n\n\n(\\text{TCB}-\\text{TCG})_{\\text{secular}} = L_C \\times (\\text{JD} - 2443144.5) \\times 86400 \\ \\text{s}\n\n\n\\text{TDB} = \\text{TCB} - L_B \\times (\\text{JD}_{\\text{TCB}} - T_0) \\times 86400 \\ \\text{s} + \\text{TDB}_0\n\n\n\\text{TDB}(T_0) - \\text{TCB}(T_0) = (\\text{TDB} - \\text{TCB} \\quad \\text{at} \\quad \\text{JD}_{\\text{TCB}} = T_0)\n\nSI단위계(International System of Units)의 1 초(second)는 현재 세슘-133 원자의 섭동이 없는 바닥상태의 초미세 전이 주파수 \\Delta \\nu_{\\text{Cs}}를 Hz 단위로 나타날 때 그 수치를 9 192 631 770으로 고정함으로써 정의된다. 여기서 Hz는 s^{-1}과 같다.\nTAI(International Atomic Time)는 원자시계에 기반한 시척도이며, BIPM(International Bureau of Weights and Measures)의 분석에 의해 유지되고 있다. TAI의 단위시간 길이는 지오이드에서의 SI초이다.\n지구 표면에서 정의된 좌표시간(coordinate time)을 TT(Terrestrial Time)라고 부른다. 지표면에서 이루어지는 천문관측에서의 시간측정에 주로 사용되는 시척도이다. TT의 단위 시간 길이는 TAI의 단위 시간 길이로 정의하며, TAI 1977년 1월 1일 0시 0분 0초를 TT 1977년 1월 1일 0시 0분 32.184초로 정의한다.역사적으로 ET(Ephemeris Time)를 TDT(Terrestrial Dynamic Time)가 계승하고 TDT를 TT가 계승하기에 32.184초가 붙는다.\nGCRS(Geocentric Celestial Reference System)는 지구의 질량중심에 원점을 둔 좌표계이며, 지구접근천체(near-Earth object, NEO)에 대한 역학적 계산에서 사용되는 좌표계이다. GCRS의 시간을 TCG(Geocentric Coordinate Time)라고 부른다. TCG는 중력에 의한 시간 지연 효과를 모두 무시하였을 때, 지구의 질량중심과 같이 움직이는 좌표계의 고유시간(proper time)이다.\nBCRS(Barycentric Celestial Reference System)는 태양계의 질량중심에 원점을 둔 좌표계이며, 일반적인 천체에 대한 역학적 계산에서 사용되는 좌표계이다. BCRS의 시간을 TCB(Barycentric Coordinate Time)라고 부른다. TCB는 중력에 의한 시간 지연 효과를 모두 무시하였을때,태양계의 질량중심과 같이 움직이는 좌표계의 고유시간(proper time)이다.\nTDB(Barycentric Dynamical Time)는 2006년 이후로 TCB를 사용하여 정의된다.\n각 시스템 시간의 단위 시간 길이는 상대론적 효과에 의해 미세하게 다르다.각각의 단위 시간 길이를 \\rm{d}(\\text{TT}), \\rm{d}(\\text{TCG}), \\rm{d}(\\text{TCB}), \\rm{d}(\\text{TDB}) 처럼 나타낸다.\n\n\n지구자전각(ERA)\n지구자전각(ERA, Earth Rotation Angle)는 항성시(Sidereal time)와 비슷한 개념으로, 지구가 자전에 의해 돌아간 각도를 나타낸다. rev는 revolutions(회전)의 줄임말이다.\n\n1 \\text{ rev} = 1 \\text{ 회전} = 360^\\circ\n\n\n\n태양질량인수(태양중력상수), 지구중력상수\nGM의 경우 천문관측에 의해 측정이 가능하나, G는 매우 정밀한 실험에 의해서만 측정이 가능하다. 따라서 GM의 정확도가 G, M보다 높은 경우가 많으므로 천문상수에 GM 값을 명시한 것이다.\n\n\n지구적도반경\nSI m(미터)는 SI s(초)에 기반하여 정의되므로, m 단위의 상수에도 시척도 [\\text{TT}]를 밝힌 것이다. \na_E = a_e\n\n\n\n지구역학계수, 지구역학계수 시간변화율\n지구를 꽉찬 회전타원면(spheroid)형태의 질량체라고 가정하자. 원점이 지구의 질량중심에 위치하고, 지구의 회전이 관측되지 않는 구면좌표계(spherical coordinate system)에서의 지구 중력 포텐셜(potential) \\phi는 지구 중력상수 \\mu = GM_E에 대해 다음과 같이 주어진다. \n\\phi = - \\frac{\\mu}{r} + \\sum_{n=2}^{\\infty} \\frac{J_n P_n(\\sin \\theta)}{r^{n+1}}\n\n여기서 P_n은 르장드르 다항식(Legendre polynomial)으로, 로드리게스공식(Rodrigues’ formula)은 다음과 같다. \nP_n(x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2 - 1)^n\n\n\\phi에서 첫항을 제외한 지배항(dominating term)은 n=2인 J_2 term으로, n \\geq 3 항들은 일반적으로 무시가능하다. J_2 항은 다음과 같이 주어진다.\n\\phi_{J_2 \\  \\text{term}} = \\frac{J_2 P_2(\\sin \\theta)}{r^3} = J_2 \\frac{3 \\sin^2 \\theta - 1}{2r^3}\n여기서 J_2 항의 계수가 J_2이며, 지구역학계수(Dynamical form-factor for the Earth)라고 부른다. 지구역학계수 시간변화율에서 cy는 century(1세기, 100년)의 줄임말이다.\nJ_2의 값은 다음과 같이 계산할 수 있다.\n\nJ_2 = \\frac{2}{3}f - \\frac{a^3 \\omega^2}{3GM_E}\n\n여기서 f는 지구 편평도, a는 지구 적도반경, \\omega는 지구 평균 각속도, GM_E는 지구 중력상수이다.\n\n\n지오이드 포텐셜\n중력장(gravitational field) \\mathbf{g}에 대해 \\mathbf{g} = - \\nabla \\phi를 만족시키는 스칼라장(scalar field) \\phi를 중력 포텐셜(potential)이라고 부른다.\n지구 중력장의 등퍼텐셜면(equipotential surface)을 지오이드(geoid)라고 부른다. 그러므로 정의에 의해 지오이드는 중력에 수직이다. 또한 지오이드는 무수히 많으며, 각각에 대응되는 지오이드 포텐셜도 무수히 많다. 다만 관습적으로 특정한 등포텐셜면(바다에서는 평균 해수면, 육지에서는 평균 해수면을 연장한 곡면)을 지오이드라고 부른다. 지오이드 포텐셜 W_0는 현재 다음과 같이 정의된다. \nW_0 = c^2 L_G\n\n\n\n지구 편평도 역수 (IERS 2010)\n지구는 자전에 의해 적도반경이 극반경보다 길다. 따라서 지구의 단면을 장반경이 a, 단반경이 b인 타원으로 근사 가능하다. 이때 지구 편평도(flattening) f는 다음과 같이 정의된다 \nf = \\frac{a-b}{a}\n\nIERS는 International Earth Rotation and Reference Systems Service의 약자이다.\n\n\n일반 경도세차, 경사각 변화율, 경도 적도세차, 경사각 적도세차, 장동상수\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n관성좌표계(inertial frame of reference)에서 관측한 지구는 위의 그림 처럼 자전(Rotation), 세차(Precession), 장동(Nutation)을 한다. 자전은 지구의 적도 평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 지구 전체가 반시계 방향으로 회전하는 현상이다. 자전 주기는 1일이다. 세차는 지구의 황도평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 지구의 자전축이 시계 방향으로 회전하는 현상이다. 세차 주기는 약 26000년이다. 장동은 지구의 황도평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 지구의 자전축과 지구의 공전축 사이의 각도가 주기적으로 변하는 현상이다. 장동 주기는 약 18.61년이다.\n그리스의 히파르코스(Hipparchus of Nicaea)가 춘분점이 이동한다는 현상을 BC 127년에 발견함으로써 지구의 세차운동이 알려졌기 때문에, 역사적으로 지구의 세차운동은 분점의 세차운동(precession of the equinoxes)이라고 불렸다. 그러나 뉴턴 이후 천체역학이 발달하면서 세차운동이 태양, 달, 지구 외 행성들이 지구에 작용하는 중력에 의해 발생한다는 사실을 알게 되면서 새로운 용어가 만들어졌다. 주로 태양과 달에 의해 지구의 적도 평면이 변하는 현상을 일월 세차(Lunisolar precession), 주로 지구 외 행성에 의해 지구의 황도 평면이 변하는 현상을 행성 세차(Planetary precession)라고 명명했으며, 이 둘의 결합으로 발생하는 실제 세차운동을 일반 세차(General Precession)라고 명명했다. 그러나 태양과 달이 지구의 황도평면을 변화시키기도 하고, 지구 외 행성이 지구의 적도평면을 변화시키기도 하기에, 용어에 있어 혼선이 존재한다는 판단 하에 2006년 IAU가 각각의 용어를 재정의했다. 즉 일월세차(Lunisolar precession)를 적도 세차(Precession of the equator)로, 행성 세차(Planetary precession)를 황도 세차(Precession of the ecliptic)로 용어를 바꾸었다.\n\n\n\n\n\n위 그림은 지구의 황도평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 천구의 북극(지구 자전축의 끝부분)이 이동하는 경로를 나타낸 것이다. 점선의 운동경로(path of mean celestial pole)는 세차운동에 의한 경로이고, 실선의 운동경로(path of instantaneous celestial pole)는 세차운동에 의한 원운동에 장동운동에 의한 타원운동이 결합된 운동에 의한 경로이다. 천구의 북극이 점선의 운동경로를 따라 이동하는 각속력을 일반 경도세차(General precession in longitude) p_A라고 한다. 장동운동에 의해 천구의 북극은 국소적인 타원운동을 하게 되는데, 이 타원의 장반경(semi-major axis)을 장동장수(constant of nutation) N이라 한다.\n\n\n\n\n\nEquinoctial colure는 천구의 북극(Celestial North Pole), 천구의 남극(Celestial South Pole), 춘분점(Vernal Equinox), 추분점(Autumnal Equinox)을 지나는 대원이다. Solstitial colure는 천구의 북극(Celestial North Pole), 천구의 남극(Celestial South Pole), 하지점(Summer Solstice), 동지점(Winter Solstice)을 지나는 대원이다.\n어느 시점에서의 \\epsilon_A는 그 시점에서의 황도(ecliptic of date)와 그 시점에서의 평균 적도(mean equator of date)가 이루는 각도이다. 어느 시점에서의 \\psi_A는 그 시점에서의 solstital colure(solstital colure of date)와 역기점에서의 solstital colure(solstitial colure of epoch)가 이루는 각도이다. 어느 시점에서의 \\omega_A는 그 시점에서의 평균적도(mean equator of date)와 역기점에서의 고정된 황도(fixed ecliptic of epoch)가 이루는 각도이다. P03 precession model에 의해 주어지는 각각의 값을 t에 대한 2차항 까지만 나타내면 다음과 같다.\n\n\\epsilon_A = 84381''.406 - 46''.836769t - 0''.0001831t^2\n\n\n\\psi_A = 5038''.481507t - 1''.0790069t^2\n\n\n\\omega_A = 84381''.406 - 0''.025754t + 0''.0512623t^2\n\n여기서 t = (\\text{TT} - \\text{2000 January 1d 12h TT})/36525 (값은 day단위로 넣는다)는 J2000.0 TT 로부터 경과된 시간을 Julian century 단위로 나타낸 값이다.\nJ2000.0 경사각 변화율(Rate of change in obliquity) \\dot{\\epsilon}는 \\epsilon_A를 t에 대해 미분하고 t=0을 대입한 값으로, J2000.0에서의 \\epsilon_A의 시간 변화율을 나타낸다.\nJ2000.0 경도 적도세차(Precession of the equator in longitude) \\dot{\\psi}는 \\psi_A를 t에 대해 미분하고 t=0을 대입한 값으로, J2000.0에서의 \\psi_A의 시간 변화율을 나타낸다.\nJ2000.0 경사각 적도세차(Precession of the equator in obliquity) \\dot{\\omega}는 \\omega_A를 t에 대해 미분하고 t=0을 대입한 값으로, J2000.0에서의 \\omega_A의 시간 변화율을 나타낸다.\n\n\n태양 시차\n태양에서 본 지구의 시반경(apparent radius, angular radius)을 태양 시차(solar parallax)라고 부른다.\n\n\n광행차 상수\n\n\n\n\n\n광행차(aberration)란 관측자의 속도에 의존하여 관측대상의 겉보기 위치가 바뀌는 현상이다. 관측자가 v의 속력으로 움직인다고 하자. 관측자가 움직이는 방향을 각도를 측정하는 기준선으로 설정하고, 관측자가 정지했을 때 특정 관측 대상에 대한 시선방향과 기준선이 이루는 각을 \\theta, 관측자가 움직일 때 특정 관측 대상에 대한 시선방향과 기준선이 이루는 각을 \\phi라고 하자. 그러면 v/c \\ll 1일 때 다음이 성립한다.\n\n\\kappa = \\theta - \\phi \\approx v/c\n\nv가 지구의 평균 공전 속도일때의 \\kappa를 광행차 상수(constant of aberration)라고 부른다."
  },
  {
    "objectID": "posts/explain-words/index.html#일반-천문상수",
    "href": "posts/explain-words/index.html#일반-천문상수",
    "title": "천문용어 설명",
    "section": "일반 천문상수",
    "text": "일반 천문상수\n\n표면중력, 태양상수, 표면탈출속도, 표면유효온도, 전복사에너지, 표면복사에너지\n질량이 M인 천체의 반지름이 R이라면 표면중력은 다음과 같이 구할 수 있다.\n\n\\text{표면중력} = \\frac{GM}{R^2}\n\n태양으로부터 1 \\text{ au}만큼 떨어진 단위면적에 단위시간동안 통과하는 총 태양에너지를 태양상수라고한다. 즉, r=1 \\text{ au}, L = \\text{광도} 일때 다음을 얻는다.\n\n\\text{태양상수} = \\frac{L}{4 \\pi r^2}\n\n질량이 M인 천체의 반지름이 R이라면 표면탈출속도는 다음과 같이 구할 수 있다. \n\\text{표면탈출속도} = \\sqrt{\\frac{2GM}{R}}\n\n천체를 그 천체와 같은 광도를 가진 흑체로 가정했을 때 흑체가 가지는 열평형온도를 표면유효온도라고 한다. \n\\text{표면유효온도} = T_e = \\left( \\frac{F_R}{\\sigma} \\right)^{1/4}\n\n천체가 단위 시간 동안 방출하는 총에너지를 전복사에너지(또는 광도)라고 한다. \n\\text{전복사에너지} = L = \\text{광도}\n\n천체의 단위 표면적이 단위 시간 동안 방출하는 총에너지를 표면복사에너지라고 한다. \n\\text{표면복사에너지} = F_R = \\frac{L}{4 \\pi R^2}\n\n\n\n이심률\n닫힌 공전 궤도는 타원 궤도이고, 타원의 장반경 a, 단반경 b에 대해 궤도의 이심률(eccentricity) e는 다음과 같이 계산된다. \ne = \\frac{\\sqrt{a^2 - b^2}}{a}\n\n\n\n적도 수평 시차\n적도상의 한 관측자 천정에 천체가 위치한다고 하자. 그 관측자로부터 적도를 따라 90^\\circ만큼 떨어져 있는 관측자가 같은 천체를 바라본다고 하자. 이때 두 관측자의 시선방향이 이루는 각을 적도 수평 시차(equatorial horizontal parallax)라고 한다.\n따라서 달의 적도 수평 시차 \\pi는 지구 적도 반경 R, 지구 중심에서 달까지의 거리 r에 대해 다음과 같이 주어진다.\n\n\\pi = \\arcsin \\left( \\frac{R}{r} \\right)\n\n\n\n장동주기 (교점주기)\n\n\n\n\n\n천체의 공전궤도가 어떤 기준평면과 만나는 2개의 점을 통틀어서 궤도 교점(orbital node)이라고 하며, 기준평면의 어느 한쪽을 위쪽으로 정의하였을 때, 천체가 아래에서 위로 올라오는 교점을 승교점(ascending node), 위에서 아래로 내려가는 교점을 강교점(descending node)이라고 한다.\n기준평면을 황도평면으로 하였을때, 달의 공전궤도에 대한 궤도교점을 달의 교점(Lunar node)이라고 한다. 지구의 장동 운동을 발생시키는 주 원인이 달이기 때문에, 이에 대한 반작용으로 달의 교점이 변하게 된다. 달의 교점이 황도를 따라 한바퀴 회전하는데 걸리는 시간은 지구의 장동 운동주기와 같은 약18.61년이다. 따라서 이것을 장동주기(nodal period) 또는 교점주기(draconic period)라고 한다.\n\n\n사로스 주기\n사로스주기(saros)는 태양, 달, 지구의 상대적 위치 관계가 반복되는 주기로, 정확히 223 삭망월(synodic month)이다.\n\n\n오오트 상수\n은하면(galactic midplane)에 존재하는, 은경(galactic longitude)이 \\ell인 임의의 천체가 태양으로부터 떨어진 거리가 d라고 하자. 그러면 태양에 대한 그 천체의 상대속도의 시선방향 성분(radial component)과 접선 방향 성분(tangential component)은 다음과 같이 주어진다.\n\nv_r \\approx Ad\\sin(2\\ell)\n\n\nv_t \\approx Ad\\sin(2\\ell) + Bd\n\n여기서 A, B가 오오트 상수(Oort constant)이다."
  },
  {
    "objectID": "posts/explain-words/index.html#정오표",
    "href": "posts/explain-words/index.html#정오표",
    "title": "천문용어 설명",
    "section": "정오표",
    "text": "정오표\n\n지구 평균 각운동량\n\n\n\\text{지구 평균 각운동량} \\rightarrow \\text{지구 평균 각속도}\n\n지구 평균 각속도는 지구의 평균 자전 각속도이다.\n\n\n태양-(지구+달) 질량비\n \n(S/E)(l+\\mu) \\rightarrow (S/E)(1+\\mu)\n\n\n\\because \\frac{S}{E+M_M} = \\frac{S}{E+\\mu E} = \\frac{S}{E}\\frac{1}{1+\\mu} = (S/E)(1+\\mu)\n\n\n\n전자볼트\n \neV = \\frac{e}{c} J \\rightarrow \\text{eV} = (e/\\text{C}) \\text{ J}\n\n여기서 \\text{J}는 에너지의 단위 줄(joule)이고, (e/\\text{C})는 쿨롬(coulomb) 단위로 나타낸 기본 전하(elementary charge)의 값이다."
  },
  {
    "objectID": "posts/explain-words/index.html#참고문헌",
    "href": "posts/explain-words/index.html#참고문헌",
    "title": "천문용어 설명",
    "section": "참고문헌",
    "text": "참고문헌\n\nCAPITAINE, Nicole; WALLACE, Patrick T.; CHAPRONT, Jean. Expressions for IAU 2000 precession quantities. Astronomy & Astrophysics​, 2003, 412.2: 567-586.\nCARROLL, Bradley W.; OSTLIE, Dale A. An introduction to modern astrophysics​. Cambridge University Press, 2017.\nLIESKE, J.H., et al. Expressions for the precession quantities based upon the IAU/1976/system of astronomical constants. Astronomy and Astrophysics​, 1977, 58: 1-16.\nMCCARTHY, Dennis D.; SEIDELMANN, P.Kenneth. Time: from Earth rotation to atomic physics​. Cambridge University Press, 2018.\nVÖLGYESI, L. Physical backgrounds of Earth’s rotation, revision of the terminology. Acta Geodaetica et Geophysica Hungarica​, 2006, 41.1: 31-44.\nBIPM - Time, https://www.bipm.org/en/bipm/tai/\nFIG Article of the Month - December 2004, https://www.fig.net/resources/monthly_articles/2004/beutLer_july_2004.asp\nFile:Lunar eclipse diagram-en.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Lunar_eclipse_diagram-en.svg\nFile:Praezession.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Praezession.svg#mw-jump-to-license\nFile:Simple stellar aberration diagram.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Simple_stellar_aberration_diagram.svg\nFundamental Physical Constants from NIST, https://pml.nist.gov/cuu/Constants/\nReference Earth Model - WGS84, https://topex.ucsd.edu/geodynamics/14gravity1_2.pdf\nThe Astronomical Almanac Online, http://asa.hmnao.com/index.html\n기본단위의 정의, https://www.kriss.re.kr/standard/view.do?pg=explanation_tab_02"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a master’s student advised by Yong-Jae Moon in the Sun and Space Weather Laboratory at Kyung Hee University. My research interests are in solar physics and machine learning."
  },
  {
    "objectID": "about.html#mingyu-jeon-전민규",
    "href": "about.html#mingyu-jeon-전민규",
    "title": "About",
    "section": "",
    "text": "I am a master’s student advised by Yong-Jae Moon in the Sun and Space Weather Laboratory at Kyung Hee University. My research interests are in solar physics and machine learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "A collection of resources on pendulum\n\n\n\n\n\n\nphysics\n\n\nenglish\n\n\n\n\n\n\n\n\n\nNov 30, 2024\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nSemantic Segmentation in PyTorch Using an FCN with a ResNet-50 Backbone\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nAug 30, 2024\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nThe double pendulum problem in Julia\n\n\n\n\n\n\nphysics\n\n\njulia\n\n\nenglish\n\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nThe double pendulum problem in Python\n\n\n\n\n\n\nphysics\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nLow and Lou (1990) force free magnetic fields\n\n\n\n\n\n\nsolar physics\n\n\nenglish\n\n\n\n\n\n\n\n\n\nMar 30, 2024\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nSolar Eruptions & Magnetic Fields\n\n\n\n\n\n\nsolar physics\n\n\nenglish\n\n\n\n\n\n\n\n\n\nMar 9, 2024\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nThe very, very basics of gravitational wave\n\n\n\n\n\n\nphysics\n\n\nenglish\n\n\n\n\n\n\n\n\n\nDec 25, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python\n\n\n\n\n\n\nsolar physics\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Derivatives of a 1D Scalar Function in Python\n\n\n\n\n\n\nmath\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\n천문용어 설명\n\n\n\n\n\n\nastronomy\n\n\nkorean\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nSpherical Astronomy\n\n\n\n\n\n\nastronomy\n\n\nenglish\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Draw 1D Scalar Functions in Python\n\n\n\n\n\n\nmath\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\n보존법칙\n\n\n\n\n\n\nphysics\n\n\nkorean\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nMotion of Charged Particles in Magnetic Dipole Fields\n\n\n\n\n\n\nphysics\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nStable Diffusion Inference with Diffusers (high-level)\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n\n\n\n\n\n\nStable Diffusion Inference with Diffusers (low-level)\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/baisc_GWs/index.html",
    "href": "posts/baisc_GWs/index.html",
    "title": "The very, very basics of gravitational wave",
    "section": "",
    "text": "I use the following sign conventions (Misner, Thorne, and Wheeler 1973) and notations.\n\npartial derivative\n\n\n\\partial_{\\mu} = \\frac{\\partial}{\\partial x^{\\mu}}\n\n\n(- + + +) metric signature\n\nFor example, the Minkowski metric \\eta_{\\mu \\nu} in the Cartesian coordinates is written as the following matrix form. \n[\\eta_{\\mu \\nu}] = [\\eta^{\\mu \\nu}] = \\begin{bmatrix}\n-1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\nThe general metric tensor is denoted by g_{\\mu \\nu}.\n\nChristoffel symbol\n\n\n\\Gamma^{\\sigma}{}_{\\mu \\nu} = \\frac{1}{2} g^{\\sigma \\alpha}(\\partial_{\\mu} g_{\\alpha \\nu} + \\partial_{\\nu} g_{\\alpha \\mu} - \\partial_{\\alpha} g_{\\mu \\nu})\n\n\nRiemann curvature tensor\n\n\nR^{\\mu}{}_{\\nu \\alpha \\beta} = \\partial_{\\alpha} \\Gamma^{\\mu}{}_{\\nu \\beta} - \\partial_{\\beta} \\Gamma^{\\mu}{}_{\\nu \\alpha} + \\Gamma^{\\mu}{}_{\\sigma \\alpha}\\Gamma^{\\sigma}{}_{\\nu \\beta} - \\Gamma^{\\mu}{}_{\\sigma \\beta}\\Gamma^{\\sigma}{}_{\\nu \\alpha}\n\n\nRicci tensor\n\n\nR_{\\mu \\nu} = R^{\\alpha}{}_{\\mu \\alpha \\nu}\n\n\nRicci scalar\n\n\nR = R^{\\alpha}{}_{\\alpha}\n\n\nEinstein tensor\n\n\nG_{\\mu \\nu} = R_{\\mu \\nu} - \\frac{1}{2} R g_{\\mu \\nu}\n\n\nFour-velocity\n\nIn Minkowski space with the Cartesian coordinates, \n[U^{\\mu}] = \\begin{bmatrix}\n\\gamma c \\\\\n\\gamma u_{x} \\\\\n\\gamma u_{y} \\\\\n\\gamma u_{z} \\\\\n\\end{bmatrix}\n\nHere, the Lorentz factor is denoted by \\displaystyle \\gamma = \\frac{1}{\\sqrt{1-\\frac{u^2}{c^2}}} and u^2 = u_{x}^2 + u_{y}^2 + u_{z}^2.\n\nEnergy-momentum tensor\n\nperfect fluid\n\n  T^{\\mu \\nu} = \\left(\\rho_{0} + \\frac{p}{c^2}\\right)U^{\\mu}U^{\\nu} + p g^{\\mu \\nu}\n  \n\nHere, \\rho_{0} is the rest mass denstiy and \\rho = \\gamma^2 \\rho_{0} is the relativistic mass density.\n\ndust (p=0)\n\n  T^{\\mu \\nu} = \\rho_{0} U^{\\mu}U^{\\nu}\n  \n\nIn this case, T^{00} = \\rho c^2."
  },
  {
    "objectID": "posts/baisc_GWs/index.html#sign-conventions-and-notations",
    "href": "posts/baisc_GWs/index.html#sign-conventions-and-notations",
    "title": "The very, very basics of gravitational wave",
    "section": "",
    "text": "I use the following sign conventions (Misner, Thorne, and Wheeler 1973) and notations.\n\npartial derivative\n\n\n\\partial_{\\mu} = \\frac{\\partial}{\\partial x^{\\mu}}\n\n\n(- + + +) metric signature\n\nFor example, the Minkowski metric \\eta_{\\mu \\nu} in the Cartesian coordinates is written as the following matrix form. \n[\\eta_{\\mu \\nu}] = [\\eta^{\\mu \\nu}] = \\begin{bmatrix}\n-1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\nThe general metric tensor is denoted by g_{\\mu \\nu}.\n\nChristoffel symbol\n\n\n\\Gamma^{\\sigma}{}_{\\mu \\nu} = \\frac{1}{2} g^{\\sigma \\alpha}(\\partial_{\\mu} g_{\\alpha \\nu} + \\partial_{\\nu} g_{\\alpha \\mu} - \\partial_{\\alpha} g_{\\mu \\nu})\n\n\nRiemann curvature tensor\n\n\nR^{\\mu}{}_{\\nu \\alpha \\beta} = \\partial_{\\alpha} \\Gamma^{\\mu}{}_{\\nu \\beta} - \\partial_{\\beta} \\Gamma^{\\mu}{}_{\\nu \\alpha} + \\Gamma^{\\mu}{}_{\\sigma \\alpha}\\Gamma^{\\sigma}{}_{\\nu \\beta} - \\Gamma^{\\mu}{}_{\\sigma \\beta}\\Gamma^{\\sigma}{}_{\\nu \\alpha}\n\n\nRicci tensor\n\n\nR_{\\mu \\nu} = R^{\\alpha}{}_{\\mu \\alpha \\nu}\n\n\nRicci scalar\n\n\nR = R^{\\alpha}{}_{\\alpha}\n\n\nEinstein tensor\n\n\nG_{\\mu \\nu} = R_{\\mu \\nu} - \\frac{1}{2} R g_{\\mu \\nu}\n\n\nFour-velocity\n\nIn Minkowski space with the Cartesian coordinates, \n[U^{\\mu}] = \\begin{bmatrix}\n\\gamma c \\\\\n\\gamma u_{x} \\\\\n\\gamma u_{y} \\\\\n\\gamma u_{z} \\\\\n\\end{bmatrix}\n\nHere, the Lorentz factor is denoted by \\displaystyle \\gamma = \\frac{1}{\\sqrt{1-\\frac{u^2}{c^2}}} and u^2 = u_{x}^2 + u_{y}^2 + u_{z}^2.\n\nEnergy-momentum tensor\n\nperfect fluid\n\n  T^{\\mu \\nu} = \\left(\\rho_{0} + \\frac{p}{c^2}\\right)U^{\\mu}U^{\\nu} + p g^{\\mu \\nu}\n  \n\nHere, \\rho_{0} is the rest mass denstiy and \\rho = \\gamma^2 \\rho_{0} is the relativistic mass density.\n\ndust (p=0)\n\n  T^{\\mu \\nu} = \\rho_{0} U^{\\mu}U^{\\nu}\n  \n\nIn this case, T^{00} = \\rho c^2."
  },
  {
    "objectID": "posts/baisc_GWs/index.html#linearized-gravity",
    "href": "posts/baisc_GWs/index.html#linearized-gravity",
    "title": "The very, very basics of gravitational wave",
    "section": "Linearized gravity",
    "text": "Linearized gravity\nThe Einstein Field Equation (EFE) is\n\nG_{\\mu \\nu} + \\Lambda g_{\\mu \\nu} = \\frac{8 \\pi G}{c^{4}}T_{\\mu \\nu}\n\nwhere \\Lambda is the cosmological constant.\nIn the linearized gravity, we ignore the cosmological constant, i.e. \\Lambda = 0.\nThen, EFE is\n\nG_{\\mu \\nu} = \\frac{8 \\pi G}{c^{4}}T_{\\mu \\nu}\n\nConsider the small perturbation h_{\\mu \\nu} of the metric.\n\ng_{\\mu \\nu} = \\eta_{\\mu \\nu} + h_{\\mu \\nu}, \\qquad |h_{\\mu \\nu}| \\ll 1\n\nFor convenience, we define the following two quantities.\n\nh = \\eta^{\\mu \\nu} h_{\\mu \\nu}\n\n\n\\bar{h}_{\\mu \\nu} = h_{\\mu \\nu} - \\frac{1}{2} \\eta_{\\mu \\nu} h\n\nAlso, we use the Lorenz gauge condition \\partial_{\\mu} \\bar{h}^{\\mu}{}_{\\nu} = 0.\nAfter many calculations, we get the following linearized EFE in the Lorenz gauge.\n\n\\Box \\bar{h}_{\\mu \\nu} = - \\frac{16 \\pi G}{c^4} T_{\\mu \\nu}\n\nHere, \\Box = \\eta^{\\mu \\nu}\\partial_{\\mu}\\partial_{\\nu} = \\partial^{\\nu} \\partial_{\\nu} is the d’Alembertian.\nThis is the wave equation.\nIn electrodynamics, the Maxwell equation in the Lorenz gauge \\partial_{\\mu}A^{\\mu}=0 in the Gaussian units is\n\n\\Box A^{\\mu} = -\\frac{4 \\pi}{c} J^{\\mu}\n\nHere, A^{\\mu} is the four-potential and J^{\\mu} is the four-current.\nAs you can see, the form of the linearized EFE is very similar to that of the Maxwell equation which predicts electromagnetic waves. Therefore, we can expect the existence of gravitational waves. The theory of gravitational waves in linearized gravity is also very similar to that of electromagnetic waves. For these reasons, Landau and Lifshitz wrote the famous physics textbook “The Classical Theory of Fields” which covers both electrodynamics and relativity."
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html",
    "href": "posts/draw-1d-scalar-function/index.html",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "",
    "text": "If you know the mathematical formula of a 1D scalar function y=f(x), then I believe the best tool for drawing 1D scalar functions is Desmos. However, you can also plot the functions in Python using various visualization libraries. In this post, I will draw 1D scalar functions y=x^2 and y=\\sin(x) using basic features of these libraries. Keep in mind that there are many advanced features not covered here, so for more information, refer to the official document of the respective library."
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#sympy",
    "href": "posts/draw-1d-scalar-function/index.html#sympy",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "SymPy",
    "text": "SymPy\n\nSymPy is a Python library for symbolic mathematics.\n\nEven though SymPy’s strength lies in symbolic computations, it can also be used for drawing 1D scalar functions, see Figure 1.\n\nfrom sympy import symbols, sin\nfrom sympy.plotting import plot \n\nx = symbols('x')\n\np1 = plot(x**2, (x, -2, 2), legend=True, show=False)\np2 = plot(sin(x), (x, -5, 5), legend=True, show=False)\np1.extend(p2)\np1.show()\n\n\n\n\n\n\n\nFigure 1: A plot using sympy\n\n\n\n\n\nThe graph depicts y=x^2 for x \\in [-2, 2] and y=\\sin(x) for x \\in [-5, 5]. The purpose of using different ranges for x is to display both graphs in a single figure without one being much smaller than the other. This is because the values of x^2 rapidly increases as |x| increases, while |\\sin(x)| \\leq 1 always."
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#data-generation-using-numpy",
    "href": "posts/draw-1d-scalar-function/index.html#data-generation-using-numpy",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "Data generation using NumPy",
    "text": "Data generation using NumPy\n\nNumPy is the fundamental package for scientific computing with Python.\n\nOther visualization libraries usually don’t understand symbolic representation of a function. They just draw (x, y) points in a coordinate plane. Therefore, before you use them, you have to generate (x, y) points using NumPy.\n\nimport numpy as np\n\nx1 = np.linspace(-2, 2, 100)\ny1 = x1**2\n\nx2 = np.linspace(-5, 5, 100)\ny2 = np.sin(x2)\n\nnp.linspace(start, stop, num) creates num evenly spaced numbers within a closed interval [start, stop]. So, x1 is an array containing 100 evenly spaced numbers within the interval [-2, 2], and x2 is the same array but within the interval [-5, 5].\n\nx1\n\narray([-2.        , -1.95959596, -1.91919192, -1.87878788, -1.83838384,\n       -1.7979798 , -1.75757576, -1.71717172, -1.67676768, -1.63636364,\n       -1.5959596 , -1.55555556, -1.51515152, -1.47474747, -1.43434343,\n       -1.39393939, -1.35353535, -1.31313131, -1.27272727, -1.23232323,\n       -1.19191919, -1.15151515, -1.11111111, -1.07070707, -1.03030303,\n       -0.98989899, -0.94949495, -0.90909091, -0.86868687, -0.82828283,\n       -0.78787879, -0.74747475, -0.70707071, -0.66666667, -0.62626263,\n       -0.58585859, -0.54545455, -0.50505051, -0.46464646, -0.42424242,\n       -0.38383838, -0.34343434, -0.3030303 , -0.26262626, -0.22222222,\n       -0.18181818, -0.14141414, -0.1010101 , -0.06060606, -0.02020202,\n        0.02020202,  0.06060606,  0.1010101 ,  0.14141414,  0.18181818,\n        0.22222222,  0.26262626,  0.3030303 ,  0.34343434,  0.38383838,\n        0.42424242,  0.46464646,  0.50505051,  0.54545455,  0.58585859,\n        0.62626263,  0.66666667,  0.70707071,  0.74747475,  0.78787879,\n        0.82828283,  0.86868687,  0.90909091,  0.94949495,  0.98989899,\n        1.03030303,  1.07070707,  1.11111111,  1.15151515,  1.19191919,\n        1.23232323,  1.27272727,  1.31313131,  1.35353535,  1.39393939,\n        1.43434343,  1.47474747,  1.51515152,  1.55555556,  1.5959596 ,\n        1.63636364,  1.67676768,  1.71717172,  1.75757576,  1.7979798 ,\n        1.83838384,  1.87878788,  1.91919192,  1.95959596,  2.        ])\n\n\n\\Delta x for this array is (2 - (-2)) / (100 - 1) = 0.\\overline{04}\n\nnp.isclose(np.diff(x1)[0], (2 - (-2)) / (100 - 1))\n\nTrue\n\n\n\nx2\n\narray([-5.        , -4.8989899 , -4.7979798 , -4.6969697 , -4.5959596 ,\n       -4.49494949, -4.39393939, -4.29292929, -4.19191919, -4.09090909,\n       -3.98989899, -3.88888889, -3.78787879, -3.68686869, -3.58585859,\n       -3.48484848, -3.38383838, -3.28282828, -3.18181818, -3.08080808,\n       -2.97979798, -2.87878788, -2.77777778, -2.67676768, -2.57575758,\n       -2.47474747, -2.37373737, -2.27272727, -2.17171717, -2.07070707,\n       -1.96969697, -1.86868687, -1.76767677, -1.66666667, -1.56565657,\n       -1.46464646, -1.36363636, -1.26262626, -1.16161616, -1.06060606,\n       -0.95959596, -0.85858586, -0.75757576, -0.65656566, -0.55555556,\n       -0.45454545, -0.35353535, -0.25252525, -0.15151515, -0.05050505,\n        0.05050505,  0.15151515,  0.25252525,  0.35353535,  0.45454545,\n        0.55555556,  0.65656566,  0.75757576,  0.85858586,  0.95959596,\n        1.06060606,  1.16161616,  1.26262626,  1.36363636,  1.46464646,\n        1.56565657,  1.66666667,  1.76767677,  1.86868687,  1.96969697,\n        2.07070707,  2.17171717,  2.27272727,  2.37373737,  2.47474747,\n        2.57575758,  2.67676768,  2.77777778,  2.87878788,  2.97979798,\n        3.08080808,  3.18181818,  3.28282828,  3.38383838,  3.48484848,\n        3.58585859,  3.68686869,  3.78787879,  3.88888889,  3.98989899,\n        4.09090909,  4.19191919,  4.29292929,  4.39393939,  4.49494949,\n        4.5959596 ,  4.6969697 ,  4.7979798 ,  4.8989899 ,  5.        ])\n\n\n\\Delta x for this array is (5 - (-5)) / (100 - 1) = 0.\\overline{10}\n\nnp.isclose(np.diff(x2)[0], (5 - (-5)) / (100 - 1))\n\nTrue\n\n\n(x1, y1) points are used to draw the graph of y=x^2, while (x2, y2) points are used for the graph of y=\\sin(x)."
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#matplotlib",
    "href": "posts/draw-1d-scalar-function/index.html#matplotlib",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nMatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n\nMatplotlib is one of the most popular visualization libraries in Python, see Figure 2.\n\nimport matplotlib.pyplot as plt\n\nplt.plot(x1, y1, label=r'$x^2$')\nplt.plot(x2, y2, label=r'$\\sin x$')\nplt.legend()\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: A plot using matplotlib"
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#pandas",
    "href": "posts/draw-1d-scalar-function/index.html#pandas",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "pandas",
    "text": "pandas\n\npandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nSince pandas is a great data analysis tool in Python, it can used for drawing graphs, see Figure 3.\n\nimport pandas as pd \n\ndf1 = pd.DataFrame(data={'x':x1, 'y':y1})\ndf2 = pd.DataFrame(data={'x':x2, 'y':y2})\n\nax = df1.plot(x='x', y='y', label=r'$x^2$')\ndf2.plot(ax=ax, x='x', y='y', label=r'$\\sin x$')\nax.axvline(0, color='k')\nax.axhline(0, color='k')\nplt.show()\n\n\n\n\n\n\n\nFigure 3: A plot using pandas"
  },
  {
    "objectID": "posts/draw-1d-scalar-function/index.html#plotly",
    "href": "posts/draw-1d-scalar-function/index.html#plotly",
    "title": "How to Draw 1D Scalar Functions in Python",
    "section": "Plotly",
    "text": "Plotly\n\nPlotly is a technical computing company headquartered in Montreal, Quebec, that develops online data analytics and visualization tools. Plotly provides online graphing, analytics, and statistics tools for individuals and collaboration, as well as scientific graphing libraries for Python, R, MATLAB, Perl, Julia, Arduino, JavaScript and REST. #\n\nPlotly is a useful tool for creating interactive plots, see Figure 4.\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x1, y=y1, mode='lines', name='x²'))\nfig.add_trace(go.Scatter(x=x2, y=y2, mode='lines', name='sin(x)'))\nfig.show()\n\n\n\n                                                \n\n\nFigure 4: A plot using plotly"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html",
    "href": "posts/derivative-of-1d-scalar-function/index.html",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "",
    "text": "There are four techniques to compute derivatives: hand-coded analytical derivative, finite differentiation, symbolic differentiation, and automatic differentiation (Margossian 2019). In this post, I will demonstrate how to find the derivative of a simple 1D scalar function, f(x) = x^2 + \\sin(3x), using each of these four methods in Python within the interval x \\in [0, \\pi]."
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#hand-coded-analytical-derivative",
    "href": "posts/derivative-of-1d-scalar-function/index.html#hand-coded-analytical-derivative",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "1 Hand-coded analytical derivative",
    "text": "1 Hand-coded analytical derivative\nYou can find the analytical derivative of the fucntion f(x) = x^2 + \\sin(3x) using the table of derivatives learned in your Calculus course.\n\n\\begin{align*}\n\\frac{d}{dx} f(x) & = \\frac{d}{dx} (x^2 + \\sin(3x)) \\\\\n& = \\frac{d}{dx} x^2 + \\frac{d}{dx} \\sin(3x) \\\\\n& = 2x + 3\\cos(3x)\n\\end{align*}\n\nThus, for every x,\n\nf'(x) = 2x + 3\\cos(3x)\n\nAccording to the previous post where I explained how to draw a 1D scalar function in Python, I will plot f(x) and its derivative f'(x) using Matplotlib.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"Helvetica\",\n    \"font.size\": 15,\n    \"figure.figsize\": (8, 6)\n})\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfunc = lambda x: x**2 + np.sin(3*x)\nd_func = lambda x: 2*x + 3*np.cos(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\ndf_dx = d_func(x)\n\nplt.plot(x, f, label=r\"$f(x)$\")\nplt.plot(x, df_dx, label=r\"$f'(x)$\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#finite-differentiation",
    "href": "posts/derivative-of-1d-scalar-function/index.html#finite-differentiation",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "2 Finite differentiation",
    "text": "2 Finite differentiation\nYou can derive the following finite differentiation formulae using Taylor’s theorem.\n\n2.1 Finite differentiation formulae\n\nForward difference\n\n1st order accuracy \n  f'(x) = \\frac{f(x+h) - f(x)}{h} + \\mathcal{O}(h)\n  \n2nd order accuracy \n  f'(x) = \\frac{-3f(x) + 4f(x+h) - f(x+2h)}{2h} + \\mathcal{O}(h^2)\n  \n\nBackward difference\n\n1st order accuracy \n  f'(x) = \\frac{f(x) - f(x-h)}{h} + \\mathcal{O}(h)\n  \n2nd order accuracy \n  f'(x) = \\frac{3f(x) - 4f(x-h) + f(x-2h)}{2h} + \\mathcal{O}(h^2)\n  \n\nCentral difference\n\n2nd order accuracy \n  f'(x) = \\frac{f(x+h) - f(x-h)}{2h} + \\mathcal{O}(h^2)\n  \n\n\n\n\n2.2 Implementation using NumPy\nNumPy arrays make the implementation of finite differentitation very straightforward. It’s important to note that at the left boundary (x=0), I use the forward difference, while at the right boundary (x=\\pi), I use the backward difference. Within the domain (0&lt;x&lt;\\pi), I use the central difference.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\n\n# calculate the spacing dx \ndx = x[1]-x[0]\n# you can use np.diff\n# dx = np.diff(x)[0] or\n# dx = np.mean(np.diff(x))\n\n# create an array with the same shape as `f`\ndf_dx = np.zeros_like(f)\n\n# forward difference (1st order) at the left boundary\ndf_dx[0] = (f[1] - f[0]) / dx\n\n# backward difference (1st order) at the right boundary\ndf_dx[-1] = (f[-1] - f[-2]) / dx\n\n# central difference (2nd accuracy) within the domain\ndf_dx[1:-1] = (f[2:] - f[:-2]) / (2*dx)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx, 'r--', lw=3, label=\"finite diff.\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIn the case of using 1st order accuracy formulae, the errors at the boundaries are\n\nprint('1st order')\nprint(f'dx   : {dx:.4f}')\nprint(f'left : {np.abs(df_dx[0] - df_dx_exact[0]):.4f}')\nprint(f'right: {np.abs(df_dx[-1] - df_dx_exact[-1]):.4f}')\n\n1st order\ndx   : 0.0317\nleft : 0.0272\nright: 0.0272\n\n\nIf we use the 2nd order accuracy formulae at the boundaries instead, we get the following errors:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\n\n# calculate the spacing dx \ndx = x[1]-x[0]\n# you can use np.diff\n# dx = np.diff(x)[0] or\n# dx = np.mean(np.diff(x))\n\n# create an array with the same shape as `f`\ndf_dx = np.zeros_like(f)\n\n# forward difference (2nd order) at the left boundary\ndf_dx[0] = (-3*f[0] + 4*f[1] - f[2]) / (2*dx)\n\n# backward difference (2nd order) at the right boundary\ndf_dx[-1] = (3*f[-1] - 4*f[-2] + f[-3]) / (2*dx)\n\n# central difference (2nd accuracy) within the domain\ndf_dx[1:-1] = (f[2:] - f[:-2]) / (2*dx)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx, 'r--', lw=3, label=\"finite diff.\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint('2nd order')\nprint(f'dx^2 : {dx**2:.4f}')\nprint(f'left : {np.abs(df_dx[0] - df_dx_exact[0]):.4f}')\nprint(f'right: {np.abs(df_dx[-1] - df_dx_exact[-1]):.4f}')\n\n2nd order\ndx^2 : 0.0010\nleft : 0.0090\nright: 0.0090\n\n\n\nnp.allclose(df_dx_exact, df_dx)\n\nFalse\n\n\n\nnp.allclose(df_dx_exact, df_dx, atol=1e-2)\n\nTrue\n\n\n\n2.2.1 numpy.gradient\nThere is a function for finite differentiation in NumPy.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\n\n# calculate the spacing dx \ndx = x[1]-x[0]\n\n# calculate derivatives of `f` using np.gradient (2nd order)\ndf_dx_npgrad = np.gradient(f, dx, axis=0, edge_order=2)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx_npgrad, 'r--', lw=3, label=\"finite diff. (np.gradient)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_npgrad)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis computes the same value as manually computed.\n\nnp.allclose(df_dx, df_dx_npgrad)\n\nTrue\n\n\n\n\n\n2.3 Implementation using findiff\nThere is a convenient library for finite differentiation in Python: findiff\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom findiff import FinDiff\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\nx = np.linspace(0, np.pi, 100)\nf = func(x)\n\n# calculate the spacing dx \ndx = x[1]-x[0]\n\n# construct the differential operator: FinDiff(axis, spacing, degree)\nd_dx = FinDiff(0, dx, 1) \n\ndf_dx_findiff = d_dx(f)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx_findiff, 'r--', lw=3, label=\"finite diff. (findiff)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_findiff)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nBy default, findiff uses 2nd order accuray.\n\nnp.allclose(df_dx, df_dx_findiff)\n\nTrue\n\n\nYou can also find finite difference coefficients using this library. (see Section 2.1)\n\nimport findiff\n# coefficients of 2nd order accuracy formulae for 1st derivative\nfindiff.coefficients(deriv=1, acc=2)\n\n{'center': {'coefficients': array([-0.5,  0. ,  0.5]),\n  'offsets': array([-1,  0,  1]),\n  'accuracy': 2},\n 'forward': {'coefficients': array([-1.5,  2. , -0.5]),\n  'offsets': array([0, 1, 2]),\n  'accuracy': 2},\n 'backward': {'coefficients': array([ 0.5, -2. ,  1.5]),\n  'offsets': array([-2, -1,  0]),\n  'accuracy': 2}}\n\n\n\nnp.allclose(df_dx_exact, df_dx_findiff)\n\nFalse\n\n\n\nnp.allclose(df_dx_exact, df_dx_findiff, atol=1e-2)\n\nTrue\n\n\n\n\n2.4 Implementation using numdifftools\nThere is another convenient library for automatic numerical differentiation in Python: numdifftools\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numdifftools as nd\n\nfunc = lambda x: x**2 + np.sin(3*x)\n\n# construct a derivative function (FD)\nd_func = nd.Derivative(func, n=1)\n\nx = np.linspace(0, np.pi, 100)\ndf_dx_numdifftools = d_func(x)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\ndf_dx_exact = exact_d_func(x)\nplt.plot(x, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x, df_dx_numdifftools, 'r--', lw=3, label=\"finite diff. (numdifftools)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_numdifftools)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nSince this library uses an adaptive finite differences with a Richardson extrapolation methodology, the result is maximally accurate.\n\nnp.allclose(df_dx_exact, df_dx_numdifftools)\n\nTrue"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#symbolic-differentiation",
    "href": "posts/derivative-of-1d-scalar-function/index.html#symbolic-differentiation",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "3 Symbolic differentiation",
    "text": "3 Symbolic differentiation\n\n3.1 Implementation using Sympy\nBy using SymPy, we can symbolically differentiate f(x) = x^2 + \\sin(3x)\n\nfrom sympy import symbols, sin, diff\nx = symbols('x')\nf = x**2 + sin(3*x)\nf\n\n\\displaystyle x^{2} + \\sin{\\left(3 x \\right)}\n\n\n\ndf_dx_sympy = diff(f, x)\ndf_dx_sympy\n\n\\displaystyle 2 x + 3 \\cos{\\left(3 x \\right)}\n\n\nAs I mentioned in the previous post, SymPy also supports plotting of a function.\n\nfrom sympy.plotting import plot\np1 = plot(f, (x, 0, np.pi), legend=True, show=False, label=\"$f(x)$\", ylabel='')\np2 = plot(df_dx_sympy, (x, 0, np.pi), legend=True, show=False, label=r\"$f'(x)$\", ylabel='')\np1.extend(p2)\np1.show()"
  },
  {
    "objectID": "posts/derivative-of-1d-scalar-function/index.html#automatic-differentiation",
    "href": "posts/derivative-of-1d-scalar-function/index.html#automatic-differentiation",
    "title": "Calculating Derivatives of a 1D Scalar Function in Python",
    "section": "4 Automatic differentiation",
    "text": "4 Automatic differentiation\n\nIn mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program. #\n\nThe efficient implementation of automatic differentiation is quite challenging. However, since the backpropagation is used to minimize loss in neural networks and is essentially a reverse-mode automatic differentiation, most deep learning libraries natively support automatic differentiation tools. In this post, I will demonstrate how to use automatic differentiation in Python with TensorFlow, PyTorch, and JAX.\n\n4.1 Implementation using TensorFlow\n\nTensorFlow is an end-to-end open source platform for machine learning.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfunc = lambda x: x**2 + tf.math.sin(3*x)\n\nx = tf.linspace(0.0, tf.constant(np.pi), 100)\n\n# calculate derivatives of `f`\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    f = func(x)\n    df_dx_tf = tape.gradient(f, x)\n\ndf_dx_tf = df_dx_tf.numpy()\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\nx_numpy = x.numpy()\ndf_dx_exact = exact_d_func(x_numpy)\nplt.plot(x_numpy, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x_numpy, df_dx_tf, 'r--', lw=3, label=\"auto diff. (TensorFlow)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_tf)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show();\n\n\n\n\n\n\n\n\n\nnp.allclose(df_dx_exact, df_dx_tf)\n\nTrue\n\n\n\n\n4.2 Implementation using PyTorch\n\nPyTorch is a Python package that supports tensors and dynamic neural networks in Python with strong GPU acceleration.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\nfunc = lambda x: x**2 + torch.sin(3*x)\n\nx = torch.linspace(0, np.pi, 100)\n\n# calculate derivatives of `f`\nx.requires_grad = True\nf = func(x)\ndf_dx_torch = torch.autograd.grad(f, x, grad_outputs=torch.ones_like(f), \n                            retain_graph=True, create_graph=True, allow_unused=True)[0]\n\ndf_dx_torch = df_dx_torch.detach().numpy()\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\nx_numpy = x.detach().numpy()\ndf_dx_exact = exact_d_func(x_numpy)\nplt.plot(x_numpy, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x_numpy, df_dx_torch, 'r--', lw=3, label=\"auto diff. (PyTorch)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_torch)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nnp.allclose(df_dx_exact, df_dx_torch)\n\nTrue\n\n\n\n\n4.3 Implementation using JAX\n\nJAX is Autograd and XLA, brought together for high-performance numerical computing, including large-scale machine learning research.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\nimport jax\n\nfunc = lambda x: x**2 + jnp.sin(3*x)\n\nx = jnp.linspace(0, jnp.pi, 100)\n\n# calculate derivatives of `f`\ndf_dx_jax = jax.vmap(jax.grad(func))(x)\n\ndf_dx_jax = np.array(df_dx_jax)\n\n# plot for comparison\nexact_d_func = lambda x: 2*x + 3*np.cos(3*x)\nx_numpy = np.array(x)\ndf_dx_exact = exact_d_func(x_numpy)\nplt.plot(x_numpy, df_dx_exact, 'b-', lw=3, label=\"exact\")\nplt.plot(x_numpy, df_dx_jax, 'r--', lw=3, label=\"auto diff. (JAX)\")\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$f'(x)$\")\nRMSE = np.sqrt(np.mean((df_dx_exact - df_dx_jax)**2))\nplt.title(f\"RMSE: {RMSE:.3e}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nnp.allclose(df_dx_exact, df_dx_jax)\n\nTrue"
  },
  {
    "objectID": "posts/field-line-isee-nlfff/index.html",
    "href": "posts/field-line-isee-nlfff/index.html",
    "title": "Visualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python",
    "section": "",
    "text": "The database contains the three-dimensional (3D) magnetic fields of solar active regions analyzed by Kusano et al. (2020). The 3D magnetic field are extrapolated by the magnetohydrodynamic relaxation method (Inoue et al., 2014) from the vector magnetic field data observed by the Solar Dynamics Observatory (SDO/HMI). In this database, Space weather HMI Active Region Patch data remapped to a Lambert Cylindrical Equal-Area projection (SHARP CEA) are used. For the detailed list and parameters of the sampled data, please refer to Kusano et al. (2020)."
  },
  {
    "objectID": "posts/field-line-isee-nlfff/index.html#isee-database-for-nonlinear-force-free-field-of-solar-active-regions",
    "href": "posts/field-line-isee-nlfff/index.html#isee-database-for-nonlinear-force-free-field-of-solar-active-regions",
    "title": "Visualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python",
    "section": "",
    "text": "The database contains the three-dimensional (3D) magnetic fields of solar active regions analyzed by Kusano et al. (2020). The 3D magnetic field are extrapolated by the magnetohydrodynamic relaxation method (Inoue et al., 2014) from the vector magnetic field data observed by the Solar Dynamics Observatory (SDO/HMI). In this database, Space weather HMI Active Region Patch data remapped to a Lambert Cylindrical Equal-Area projection (SHARP CEA) are used. For the detailed list and parameters of the sampled data, please refer to Kusano et al. (2020)."
  },
  {
    "objectID": "posts/field-line-isee-nlfff/index.html#download-a-sample-data",
    "href": "posts/field-line-isee-nlfff/index.html#download-a-sample-data",
    "title": "Visualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python",
    "section": "Download a sample data",
    "text": "Download a sample data\nI will use the 3D magnetic field data from NOAA active region 12673 at 2017-09-06 08:36:00. The data size is about 1.5 GB.\n\n# !wget https://hinode.isee.nagoya-u.ac.jp/nlfff_database/v12/12673/20170906/12673_20170906_083600.nc\n\n--2023-11-12 11:45:53--  https://hinode.isee.nagoya-u.ac.jp/nlfff_database/v12/12673/20170906/12673_20170906_083600.nc\nResolving hinode.isee.nagoya-u.ac.jp (hinode.isee.nagoya-u.ac.jp)... 133.47.151.53, 133.47.151.53\nConnecting to hinode.isee.nagoya-u.ac.jp (hinode.isee.nagoya-u.ac.jp)|133.47.151.53|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1626400312 (1.5G) [application/x-netcdf]\nSaving to: ‘12673_20170906_083600.nc’\n\n12673_20170906_0836 100%[===================&gt;]   1.51G  1.74MB/s    in 5m 11s  \n\n2023-11-12 11:51:04 (4.99 MB/s) - ‘12673_20170906_083600.nc’ saved [1626400312/1626400312]"
  },
  {
    "objectID": "posts/field-line-isee-nlfff/index.html#a-sample-python-script-load_nlfff.py",
    "href": "posts/field-line-isee-nlfff/index.html#a-sample-python-script-load_nlfff.py",
    "title": "Visualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python",
    "section": "A sample Python script load_nlfff.py",
    "text": "A sample Python script load_nlfff.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport netCDF4\nimport sys\n\nclass nlfff:\n\n      def __init__(self,filename):\n            self.filename=filename\n\n            nc=netCDF4.Dataset(self.filename,'r')\n            self.NOAA=nc.NOAA\n            self.year_month_day_time=nc.year_month_day_time\n            self.project=nc.project\n            self.production_date=nc.production_date\n            self.version=nc.version\n            self.data_doi=nc.data_doi\n            self.http_link=nc.http_link\n            self.Distributor=nc.Distributor\n            \n            nc_x=nc.variables['x']\n            self.x=nc_x[:]\n            print(nc_x.long_name,' unit:',nc_x.units)\n            nc_y=nc.variables['y']\n            self.y=nc_y[:]\n            print(nc_y.long_name,' unit:',nc_y.units)\n            nc_z=nc.variables['z']\n            self.z=nc_z[:]\n            print(nc_z.long_name,' unit:',nc_z.units)\n            \n            nc_bx=nc.variables['Bx']\n            self.bx=nc_bx[:].transpose(2,1,0)\n            print(nc_bx.long_name,' unit:',nc_bx.units)\n            nc_by=nc.variables['By']\n            self.by=nc_by[:].transpose(2,1,0)\n            print(nc_by.long_name,' unit:',nc_by.units)\n            nc_bz=nc.variables['Bz']\n            self.bz=nc_bz[:].transpose(2,1,0)\n            print(nc_bz.long_name,' unit:',nc_bz.units)\n            \n            nc_bxp=nc.variables['Bx_pot']\n            self.bx_pot=nc_bxp[:].transpose(2,1,0)\n            print(nc_bxp.long_name,' unit:',nc_bxp.units)\n            nc_byp=nc.variables['By_pot']\n            self.by_pot=nc_byp[:].transpose(2,1,0)\n            print(nc_byp.long_name,' unit:',nc_byp.units)\n            nc_bzp=nc.variables['Bz_pot']\n            self.bz_pot=nc_bzp[:].transpose(2,1,0)\n            print(nc_bzp.long_name,' unit:',nc_bzp.units)\n            \n      def info(self):\n            print(f\"NOAA\",self.NOAA)\n            print(f'year_month_day_time',self.year_month_day_time)\n            print(f\"project\",self.project)\n            print(f\"production_date\",self.production_date)\n            print(f\"version\",self.version)\n            print(f\"data_doi\",self.data_doi)\n            print(f\"http_link\",self.http_link)\n            print(f\"Distributor\",self.Distributor)\n\n      def plot(self):\n            xs=12.0\n            ys=4.0\n\n            xmin=min(self.x)\n            xmax=max(self.x)\n            ymin=min(self.y)\n            ymax=max(self.y)\n\n            plt.close()\n            fig=plt.figure(figsize=(xs,ys))\n            ax1=fig.add_axes((0.08,0.35,0.25,0.25*xs/ys*(ymax-ymin)/(xmax-xmin)))\n            ax2=fig.add_axes((0.4,0.35,0.25,0.25*xs/ys*(ymax-ymin)/(xmax-xmin)))\n            ax3=fig.add_axes((0.72,0.35,0.25,0.25*xs/ys*(ymax-ymin)/(xmax-xmin)))\n            cax1=fig.add_axes((0.08,0.15,0.25,0.05))\n            cax2=fig.add_axes((0.4,0.15,0.25,0.05))\n            cax3=fig.add_axes((0.72,0.15,0.25,0.05))\n            \n            vmin=-3000.0 \n            vmax=3000.0\n            \n            im1=ax1.pcolormesh(self.x,self.y,self.bx[:,:,0].transpose(),vmin=vmin,vmax=vmax,cmap='gist_gray',shading='auto')\n            im2=ax2.pcolormesh(self.x,self.y,self.by[:,:,0].transpose(),vmin=vmin,vmax=vmax,cmap='gist_gray',shading='auto')\n            im3=ax3.pcolormesh(self.x,self.y,self.bz[:,:,0].transpose(),vmin=vmin,vmax=vmax,cmap='gist_gray',shading='auto')\n\n            cbar1=plt.colorbar(im1,cax=cax1,orientation='horizontal')\n            cbar2=plt.colorbar(im2,cax=cax2,orientation='horizontal')\n            cbar3=plt.colorbar(im3,cax=cax3,orientation='horizontal')\n            \n            ax1.set_title('Bx [G]')\n            ax1.set_xlabel('x [Mm]')\n            ax1.set_ylabel('y [Mm]')\n            \n            ax2.set_title('By [G]')\n            ax2.set_xlabel('x [Mm]')\n            ax2.set_ylabel('y [Mm]')\n            \n            ax3.set_title('Bz [G]')\n            ax3.set_xlabel('x [Mm]')\n            ax3.set_ylabel('y [Mm]')\n            \n            plt.pause(0.1)\n\n\ndata = nlfff('12673_20170906_083600.nc') \n\nx (westward)  unit: Mm\ny (northward)  unit: Mm\nz (out ot photosphere)  unit: Mm\nBx (westward)  unit: G\nBy (northward)  unit: G\nBz (out of photosphere)  unit: G\nBx_pot (westward)  unit: G\nBy_pot (northward)  unit: G\nBz_pot (out of photosphere)  unit: G\n\n\n\ndata.info()\n\nNOAA 12673\nyear_month_day_time 2017_9_6_83600\nproject ISEE Database for Nonlinear Force-Free Field of Solar Active Region\nproduction_date 2023-03-22\nversion v1.2\ndata_doi 10.34515/DATA.HSC-00000\nhttp_link https://hinode.isee.nagoya-u.ac.jp/nlfff_database/\nDistributor Hinode Science Center, Institute for Space-Earth Environmental Research, Nagoya University\n\n\n\ndata.plot()\n\n\n\n\n\n\n\n\n\nvars(data).keys()\n\ndict_keys(['filename', 'NOAA', 'year_month_day_time', 'project', 'production_date', 'version', 'data_doi', 'http_link', 'Distributor', 'x', 'y', 'z', 'bx', 'by', 'bz', 'bx_pot', 'by_pot', 'bz_pot'])"
  },
  {
    "objectID": "posts/field-line-isee-nlfff/index.html#visualization-script",
    "href": "posts/field-line-isee-nlfff/index.html#visualization-script",
    "title": "Visualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python",
    "section": "Visualization script",
    "text": "Visualization script\n\nimport numpy as np\nimport pyvista as pv\nimport k3d\nfrom k3d import matplotlib_color_maps\n\ndef create_coordinates(bounds):\n    xbounds = (bounds[0], bounds[1])\n    ybounds = (bounds[2], bounds[3])\n    zbounds = (bounds[4], bounds[5])\n    meshgrid = np.mgrid[xbounds[0]:xbounds[1]+1, ybounds[0]:ybounds[1]+1, zbounds[0]:zbounds[1]+1]\n    return np.stack(meshgrid, axis=-1).astype(np.float32)\n\n\ndef create_mesh(bx, by, bz):\n    bx, by, bz = map(np.array, (bx, by, bz))\n    Nx, Ny, Nz = bx.shape\n    co_bounds = (0, Nx-1, 0, Ny-1, 0, Nz-1)\n    co_coords = create_coordinates(co_bounds).reshape(-1, 3)\n    co_coord = co_coords.reshape(Nx, Ny, Nz, 3)\n    x = co_coord[..., 0]\n    y = co_coord[..., 1]\n    z = co_coord[..., 2]\n    mesh = pv.StructuredGrid(x, y, z)\n    vectors = np.stack([bx, by, bz], axis=-1).transpose(2, 1, 0, 3).reshape(-1, 3)\n    mesh['vector'] = vectors\n    mesh.active_vectors_name = 'vector'\n    magnitude = np.linalg.norm(vectors, axis=-1)\n    mesh['magnitude'] = magnitude\n    mesh.active_scalars_name = 'magnitude'\n    return mesh\n\n\ndef create_mesh_xyz(x, y, z, bx, by, bz):\n    x, y, z, bx, by, bz = map(np.array, (x, y, z, bx, by, bz))\n    X, Y, Z = np.meshgrid(x, y, z, indexing='ij')\n    mesh = pv.StructuredGrid(X, Y, Z)\n    vectors = np.stack([bx, by, bz], axis=-1).transpose(2, 1, 0, 3).reshape(-1, 3)\n    mesh['vector'] = vectors\n    mesh.active_vectors_name = 'vector'\n    magnitude = np.linalg.norm(vectors, axis=-1)\n    mesh['magnitude'] = magnitude\n    mesh.active_scalars_name = 'magnitude'\n    return mesh\n\ndef plot_xy_yz_zx(p, dargs, targs):\n    pl = pv.Plotter()\n    pl.show_bounds()\n    pl.add_mesh(p.meshes[0])\n    pl.add_mesh(p.meshes[1], **dargs)\n    pl.add_mesh(p.meshes[2], **targs)\n    pl.camera_position = 'xy'\n    pl.show()\n\n    pl = pv.Plotter()\n    pl.show_bounds()\n    pl.add_mesh(p.meshes[0])\n    pl.add_mesh(p.meshes[1], **dargs)\n    pl.add_mesh(p.meshes[2], **targs)\n    pl.camera_position = 'yz'\n    pl.show()\n\n    pl = pv.Plotter()\n    pl.show_bounds()\n    pl.add_mesh(p.meshes[0])\n    pl.add_mesh(p.meshes[1], **dargs)\n    pl.add_mesh(p.meshes[2], **targs)\n    pl.camera_position = 'xz'\n    pl.show()\n\n\ndef plot_k3d(p):\n    plot = k3d.plot()\n    plot += k3d.vtk_poly_data(p.meshes[0])\n    plot += k3d.vtk_poly_data(p.meshes[1], color_attribute=('vector-2', -2500, 2500), color_map=matplotlib_color_maps.gray)\n    plot += k3d.vtk_poly_data(p.meshes[2])\n    plot.display()\n\n\nclass plotting:\n    def __init__(self, grid):\n        self.grid = grid\n        x_ind_min, y_ind_min, z_ind_min = 0, 0, 0\n        Nx, Ny, Nz = self.grid.dimensions\n        x_ind_max, y_ind_max, z_ind_max = Nx-1, Ny-1, Nz-1\n\n        self.x_ind_min, self.y_ind_min, self.z_ind_min = x_ind_min, y_ind_min, z_ind_min\n        self.x_ind_max, self.y_ind_max, self.z_ind_max = x_ind_max, y_ind_max, z_ind_max\n        \n        bottom_subset = (x_ind_min, x_ind_max, y_ind_min, y_ind_max, 0, 0)\n        bottom = self.grid.extract_subset(bottom_subset).extract_surface()\n        bottom.active_vectors_name = 'vector'\n        bottom.active_scalars_name = 'magnitude'\n\n        self.bottom = bottom\n\n        self.x_bottom = bottom.points[:, 0].reshape(Nx, Ny)\n        self.y_bottom = bottom.points[:, 1].reshape(Nx, Ny)\n        self.B_bottom = bottom['vector'].reshape(Nx, Ny, 3)\n\n        B = self.grid['vector'].reshape(Nz, Ny, Nx, 3)\n        self.B = B.transpose(2, 1, 0, 3)\n\n    def fieldline(self, window_size=None, title=None, title_fontsize=20, camera_position=None, i_siz=160, j_siz=100, i_resolution=16, j_resolution=16, vmin=-2500, vmax=2500, max_time=1000, tube_size=None):\n        p = pv.Plotter()\n        p.show_bounds()\n        p.add_mesh(self.grid.outline())\n        sargs = dict(\n            title='Bz [G]',\n            title_font_size=15,\n            height=0.25,\n            width=0.05,\n            vertical=True,\n            position_x = 0.05,\n            position_y = 0.05,\n        )\n        dargs = dict(\n            cmap='gray',\n            scalars='vector', \n            component=2, \n            clim=(vmin, vmax), \n            scalar_bar_args=sargs, \n            show_scalar_bar=True, \n            lighting=False\n        )\n        p.add_mesh(self.bottom, **dargs)\n\n        if (i_siz is not None) and (j_siz is not None):\n            i_size = i_siz\n            j_size = j_siz\n        else:\n            i_size = self.grid.bounds[1]-self.grid.bounds[0]\n            j_size = self.grid.bounds[3]-self.grid.bounds[2]\n        seed = pv.Plane(center=(self.grid.center[0], self.grid.center[1], 0), direction=(0,0,1), \n                i_size=i_size, j_size=j_size, \n                i_resolution=i_resolution, j_resolution=j_resolution)\n        strl = self.grid.streamlines_from_source(seed,\n                                                 vectors='vector',\n                                                 max_time=max_time,\n                                                 initial_step_length=0.1,\n                                                 integration_direction='both')\n        \n        targs = dict(\n            lighting=False,\n            color='blue'\n        )\n        if tube_size is not None:\n            p.add_mesh(strl.tube(radius=tube_size), **targs)\n        else:\n            p.add_mesh(strl.tube(radius=i_size/400), **targs)\n        if camera_position is not None:\n             p.camera_position = camera_position\n        if window_size is not None:\n            p.window_size = window_size\n        if title is not None:\n            p.add_title(title, font_size=title_fontsize)\n        return p, dargs, targs\n\n\npv.set_jupyter_backend('static')"
  },
  {
    "objectID": "posts/field-line-isee-nlfff/index.html#nonlinear-force-free-field",
    "href": "posts/field-line-isee-nlfff/index.html#nonlinear-force-free-field",
    "title": "Visualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python",
    "section": "Nonlinear force-free field",
    "text": "Nonlinear force-free field\n\ncreate_mesh\n\nmesh = create_mesh(data.bx, data.by, data.bz)\nB = plotting(mesh)\np, dargs, targs = B.fieldline()\np.show()\n\n\n\n\n\n\n\n\n\nplot_xy_yz_zx(p, dargs, targs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_k3d(p)\n\n\n\n\n\n    \n\n\n\ncreate_mesh_xyz\n\nmesh = create_mesh_xyz(data.x, data.y, data.z, data.bx, data.by, data.bz)\nB = plotting(mesh)\np, dargs, targs = B.fieldline()\np.show()\n\n\n\n\n\n\n\n\n\nplot_xy_yz_zx(p, dargs, targs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_k3d(p)"
  },
  {
    "objectID": "posts/field-line-isee-nlfff/index.html#potential-field",
    "href": "posts/field-line-isee-nlfff/index.html#potential-field",
    "title": "Visualization of ISEE Nonlinear Force-Free Field of Solar Active Regions in Python",
    "section": "Potential field",
    "text": "Potential field\n\ncreate_mesh\n\nmesh = create_mesh(data.bx_pot, data.by_pot, data.bz_pot)\nBp = plotting(mesh)\np, dargs, targs = Bp.fieldline()\np.show()\n\n\n\n\n\n\n\n\n\nplot_xy_yz_zx(p, dargs, targs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_k3d(p)\n\n\n\n\n\n    \n\n\n\ncreate_mesh_xyz\n\nmesh = create_mesh_xyz(data.x, data.y, data.z, data.bx_pot, data.by_pot, data.bz_pot)\nBp = plotting(mesh)\np, dargs, targs = Bp.fieldline()\np.show()\n\n\n\n\n\n\n\n\n\nplot_xy_yz_zx(p, dargs, targs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_k3d(p)"
  },
  {
    "objectID": "posts/pendulum/index.html",
    "href": "posts/pendulum/index.html",
    "title": "A collection of resources on pendulum",
    "section": "",
    "text": "This post contains a collection of resources on pendulum."
  },
  {
    "objectID": "posts/pendulum/index.html#contents",
    "href": "posts/pendulum/index.html#contents",
    "title": "A collection of resources on pendulum",
    "section": "Contents",
    "text": "Contents\n\nDescription\nExperiment\nSimulation\n\nC\nJavaScript\nProcessing\nJulia\nPython\n\nInverted Pendulum"
  },
  {
    "objectID": "posts/pendulum/index.html#c",
    "href": "posts/pendulum/index.html#c",
    "title": "A collection of resources on pendulum",
    "section": "C",
    "text": "C\nThe Double Pendulum\nMichael S. Wheatland\nWebsite\n2014-09-18"
  },
  {
    "objectID": "posts/pendulum/index.html#javascript",
    "href": "posts/pendulum/index.html#javascript",
    "title": "A collection of resources on pendulum",
    "section": "JavaScript",
    "text": "JavaScript\nDouble Pendulum\nmyPhysicsLab\nWebsite"
  },
  {
    "objectID": "posts/pendulum/index.html#processing",
    "href": "posts/pendulum/index.html#processing",
    "title": "A collection of resources on pendulum",
    "section": "Processing",
    "text": "Processing\nCoding Challenge 93: Double Pendulum\nThe Coding Train\nWebsite\n2018-02-14"
  },
  {
    "objectID": "posts/pendulum/index.html#julia",
    "href": "posts/pendulum/index.html#julia",
    "title": "A collection of resources on pendulum",
    "section": "Julia",
    "text": "Julia\nDouble Pendulum Problem\nPlots.jl\nWebsite\n[07x10] How to Solve the Double Pendulum Equations in Julia using DifferentialEquations.jl and Pluto\ndoggo dot jl\nYoutube\n2022-11-21"
  },
  {
    "objectID": "posts/pendulum/index.html#python",
    "href": "posts/pendulum/index.html#python",
    "title": "A collection of resources on pendulum",
    "section": "Python",
    "text": "Python\nThe double pendulum problem\nMatplotlib\nWebsite\nThe double pendulum\nChristian Hill\nWebsite\n2017-07-16\nThe Double-SPRINGED Pendulum in PYTHON\nMr. P Solver\nYoutube\n2021-04-06\nThe Double Pendulum in PYTHON\nMr. P Solver\nYoutube\n2021-09-07\nModeling a Double Pendulum with Python\nDot Physics\nYoutube\n2021-12-30\nHow to Cheat the Double Pendulum Using Springs Instead of Strings\nDot Physics\nYoutube\n2022-01-04\nHow To Solve and Animate a 3D Double Pendulum in Python\nMr. P Solver\nYoutube\n2022-04-04\nModeling the Double Pendulum with Python and sympy\nDot Physics\nYoutube\n2022-11-29\nDouble Pendulum Simulation in Python! || Simulating Physics with Python\nYounes Lab\nYoutube\n2024-01-04"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "Create an image using Diffusers library.\n\n\n\n!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)\n\n\n\n\n\n# 1. Load the autoencoder model which will be used to decode the latents into image space.\nvae = pipe.vae\n\n# 2. Load the tokenizer and text encoder to tokenize and encode the text.\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\n\n# 3. The UNet model for generating the latents.\nunet = pipe.unet\n\n\n\n\nvae\n\nAutoencoderKL(\n  (encoder): Encoder(\n    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (down_blocks): ModuleList(\n      (0): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (1): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (2): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (3): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (decoder): Decoder(\n    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (up_blocks): ModuleList(\n      (0-1): 2 x UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0-2): 3 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (2): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (3): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\ntokenizer\n\nCLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"&lt;|startoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True)\n\n\n\ntext_encoder\n\nCLIPTextModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 768)\n      (position_embedding): Embedding(77, 768)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n\n\n\nunet\n\nUNet2DConditionModel(\n  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n\n\n\n\n\ntext_input = tokenizer(\n    prompt * batch_size,\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\ntext_input.shape, text_input\n\n(torch.Size([2, 77]),\n tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input)[0]\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nuncond_input = tokenizer(\n    [\"\"] * batch_size,\n    padding=\"max_length\",\n    max_length=text_input.shape[-1],\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nuncond_input.shape, uncond_input\n\n(torch.Size([2, 77]),\n tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input)[0]\n\nuncond_embeddings.shape, uncond_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]]],\n        device='cuda:0'))\n\n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([4, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nlatents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n).to(device)\n\nlatents.shape, latents\n\n(torch.Size([2, 4, 64, 64]),\n tensor([[[[ 0.1884, -0.6394,  0.1089,  ..., -0.9887, -0.7133, -1.1545],\n           [ 0.4124,  1.5587, -0.3407,  ...,  2.1968, -0.0356, -0.0810],\n           [-1.8912,  0.0528, -0.4425,  ...,  1.3110,  0.7100,  0.6802],\n           ...,\n           [-1.3443, -0.1747, -0.6298,  ...,  0.4572, -0.8584, -0.1284],\n           [-1.7920, -0.6554, -0.0439,  ...,  0.5436,  2.2266, -0.5003],\n           [ 0.6213, -1.3155,  0.7470,  ..., -0.2354,  0.7097,  0.6170]],\n \n          [[-0.5007, -1.4418,  0.2598,  ..., -0.2586,  2.3239, -1.3245],\n           [ 0.8540, -0.4135,  0.5658,  ..., -1.9556,  2.0454, -0.2454],\n           [-0.3212, -1.9329, -1.1598,  ...,  0.7156, -0.7228, -0.6992],\n           ...,\n           [ 0.0180, -0.7993,  2.3330,  ...,  0.2594, -0.0333, -0.0826],\n           [-1.2569, -0.8219,  1.3467,  ...,  0.4792,  1.8265, -0.6156],\n           [-1.9367, -0.0949,  0.0720,  ...,  0.0806,  0.2966, -1.0284]],\n \n          [[ 0.2291, -0.0936, -1.3283,  ...,  1.4995, -0.1965, -0.2879],\n           [-1.0226, -1.2896,  1.6202,  ..., -0.3910, -0.3834,  0.5519],\n           [ 0.5424,  0.2685,  0.4912,  ...,  0.9773, -0.8260,  1.1552],\n           ...,\n           [-1.5280, -0.2530, -1.3748,  ..., -1.4948,  1.3661, -1.1294],\n           [ 0.4241, -0.2996,  1.8231,  ...,  0.6968,  0.8247, -0.0279],\n           [-3.3711, -0.7468, -1.3212,  ..., -0.4128,  0.4621,  2.6297]],\n \n          [[-0.7510, -0.7452, -0.8998,  ..., -1.6957, -0.4004, -0.2596],\n           [-1.2092, -1.8881, -0.5828,  ..., -1.0428, -0.6500,  0.3601],\n           [-0.4254,  0.9478,  1.3083,  ..., -0.0259, -0.4542,  0.4353],\n           ...,\n           [-0.1918,  0.4858,  0.0666,  ...,  0.8505, -0.6606, -0.3193],\n           [ 1.3620,  0.2283,  0.6292,  ..., -0.9271,  1.7018,  0.2161],\n           [-0.3891, -1.8911, -0.7501,  ..., -0.2330, -1.0460,  0.4121]]],\n \n \n         [[[ 0.3649, -1.3183, -1.3308,  ..., -0.5548, -1.3610, -1.9329],\n           [-0.0071,  0.1977,  1.5517,  ..., -1.6664,  1.6551,  0.1798],\n           [-1.0404,  0.6524,  0.4654,  ..., -0.5947, -1.0871,  2.2230],\n           ...,\n           [-0.6844,  0.1692, -0.2559,  ...,  0.5511,  0.9734,  0.7936],\n           [-1.1951,  0.5016,  0.8089,  ...,  0.2337, -0.2213, -1.1724],\n           [-0.5055, -0.7491, -1.4940,  ..., -2.1332,  0.9120,  0.2057]],\n \n          [[ 1.3668, -1.1680, -0.8574,  ..., -0.0635, -1.9132, -0.6023],\n           [ 1.0974, -0.9654,  1.2987,  ...,  1.3187, -0.0241, -0.5427],\n           [-2.0427, -1.4358, -0.7115,  ...,  0.1088,  0.0764,  0.7254],\n           ...,\n           [ 1.0957,  1.4058, -0.0178,  ...,  0.5748,  0.0953,  0.7550],\n           [ 0.4080,  0.8792,  0.6801,  ..., -0.7215,  1.1261,  0.0551],\n           [-0.3183, -2.3306,  0.7155,  ...,  0.4291, -0.2074, -1.1237]],\n \n          [[-0.2401,  0.9229,  0.0212,  ...,  0.2128, -0.4705, -0.3262],\n           [ 0.1108,  0.8909,  0.5309,  ..., -1.7175, -1.6657, -1.7706],\n           [-0.1654, -0.4582, -1.2832,  ...,  0.5297, -0.8363,  1.0293],\n           ...,\n           [-1.3526,  2.1482,  0.5417,  ..., -2.2156, -1.9940, -0.9745],\n           [-0.5821,  0.0492,  0.6693,  ..., -0.8610,  0.5864, -0.6040],\n           [ 1.0180,  1.4447,  0.9563,  ...,  0.9034,  0.7988, -1.7119]],\n \n          [[-1.6146,  0.0868,  0.6415,  ...,  0.2083,  0.4058,  0.2813],\n           [ 0.1969, -0.3334, -0.6526,  ..., -1.4639, -1.6302, -0.6036],\n           [ 0.1556, -0.0859, -0.0230,  ..., -0.7900, -0.3481,  0.8767],\n           ...,\n           [ 0.6056,  0.8374, -0.3834,  ..., -0.6636, -0.4814,  0.8244],\n           [ 0.6982, -0.4884, -1.3777,  ...,  0.5876, -2.0944,  0.0853],\n           [ 0.0388, -0.5761, -0.5116,  ..., -1.6645,  0.1752, -0.1923]]]],\n        device='cuda:0'))\n\n\n\nscheduler.set_timesteps(num_inference_steps)\n\nscheduler.timesteps.shape, pipe.scheduler.timesteps\n\n(torch.Size([50]),\n tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n         856.2857, 835.8980, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n         713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n         570.8571, 550.4694, 530.0816, 509.6939, 489.3061, 468.9184, 448.5306,\n         428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n         285.4286, 265.0408, 244.6531, 224.2653, 203.8776, 183.4898, 163.1020,\n         142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n           0.0000], dtype=torch.float64))\n\n\n\n\n\n\n\\tilde{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{z}_t, \\mathbf{c}) = w\\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) + (1-w)\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t).\n\nHere, \\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) and \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t) are conditional and unconditional \\boldsymbol{\\epsilon}-predictions, given by \\boldsymbol{\\epsilon}_\\theta := (\\mathbf{z}_t - \\alpha_t\\hat{\\mathbf{x}}_\\theta)/\\sigma_t, and w is the guidance weight. Setting w = 1 disables classifier-free guidance, while increasing w &gt; 1 strengthens the effect of guidance.1\n\nfrom tqdm.auto import tqdm\n\nlatents = latents * scheduler.init_noise_sigma\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(\n            latent_model_input, t, encoder_hidden_states=text_embeddings\n        ).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = (\n        guidance_scale * noise_pred_text + (1 - guidance_scale) * noise_pred_uncond\n    )\n\n    # compute the previous noisy sample x_t -&gt; x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n\nfrom PIL import Image\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\n\n\nfor pil_image in pil_images:\n    display(pil_image)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#install-and-import-libraries",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#install-and-import-libraries",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#low-level",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#low-level",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "# 1. Load the autoencoder model which will be used to decode the latents into image space.\nvae = pipe.vae\n\n# 2. Load the tokenizer and text encoder to tokenize and encode the text.\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\n\n# 3. The UNet model for generating the latents.\nunet = pipe.unet\n\n\n\n\nvae\n\nAutoencoderKL(\n  (encoder): Encoder(\n    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (down_blocks): ModuleList(\n      (0): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (1): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (2): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (3): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (decoder): Decoder(\n    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (up_blocks): ModuleList(\n      (0-1): 2 x UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0-2): 3 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (2): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (3): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\ntokenizer\n\nCLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"&lt;|startoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True)\n\n\n\ntext_encoder\n\nCLIPTextModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 768)\n      (position_embedding): Embedding(77, 768)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n\n\n\nunet\n\nUNet2DConditionModel(\n  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n\n\n\n\n\ntext_input = tokenizer(\n    prompt * batch_size,\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\ntext_input.shape, text_input\n\n(torch.Size([2, 77]),\n tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input)[0]\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nuncond_input = tokenizer(\n    [\"\"] * batch_size,\n    padding=\"max_length\",\n    max_length=text_input.shape[-1],\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nuncond_input.shape, uncond_input\n\n(torch.Size([2, 77]),\n tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input)[0]\n\nuncond_embeddings.shape, uncond_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]]],\n        device='cuda:0'))\n\n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([4, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nlatents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n).to(device)\n\nlatents.shape, latents\n\n(torch.Size([2, 4, 64, 64]),\n tensor([[[[ 0.1884, -0.6394,  0.1089,  ..., -0.9887, -0.7133, -1.1545],\n           [ 0.4124,  1.5587, -0.3407,  ...,  2.1968, -0.0356, -0.0810],\n           [-1.8912,  0.0528, -0.4425,  ...,  1.3110,  0.7100,  0.6802],\n           ...,\n           [-1.3443, -0.1747, -0.6298,  ...,  0.4572, -0.8584, -0.1284],\n           [-1.7920, -0.6554, -0.0439,  ...,  0.5436,  2.2266, -0.5003],\n           [ 0.6213, -1.3155,  0.7470,  ..., -0.2354,  0.7097,  0.6170]],\n \n          [[-0.5007, -1.4418,  0.2598,  ..., -0.2586,  2.3239, -1.3245],\n           [ 0.8540, -0.4135,  0.5658,  ..., -1.9556,  2.0454, -0.2454],\n           [-0.3212, -1.9329, -1.1598,  ...,  0.7156, -0.7228, -0.6992],\n           ...,\n           [ 0.0180, -0.7993,  2.3330,  ...,  0.2594, -0.0333, -0.0826],\n           [-1.2569, -0.8219,  1.3467,  ...,  0.4792,  1.8265, -0.6156],\n           [-1.9367, -0.0949,  0.0720,  ...,  0.0806,  0.2966, -1.0284]],\n \n          [[ 0.2291, -0.0936, -1.3283,  ...,  1.4995, -0.1965, -0.2879],\n           [-1.0226, -1.2896,  1.6202,  ..., -0.3910, -0.3834,  0.5519],\n           [ 0.5424,  0.2685,  0.4912,  ...,  0.9773, -0.8260,  1.1552],\n           ...,\n           [-1.5280, -0.2530, -1.3748,  ..., -1.4948,  1.3661, -1.1294],\n           [ 0.4241, -0.2996,  1.8231,  ...,  0.6968,  0.8247, -0.0279],\n           [-3.3711, -0.7468, -1.3212,  ..., -0.4128,  0.4621,  2.6297]],\n \n          [[-0.7510, -0.7452, -0.8998,  ..., -1.6957, -0.4004, -0.2596],\n           [-1.2092, -1.8881, -0.5828,  ..., -1.0428, -0.6500,  0.3601],\n           [-0.4254,  0.9478,  1.3083,  ..., -0.0259, -0.4542,  0.4353],\n           ...,\n           [-0.1918,  0.4858,  0.0666,  ...,  0.8505, -0.6606, -0.3193],\n           [ 1.3620,  0.2283,  0.6292,  ..., -0.9271,  1.7018,  0.2161],\n           [-0.3891, -1.8911, -0.7501,  ..., -0.2330, -1.0460,  0.4121]]],\n \n \n         [[[ 0.3649, -1.3183, -1.3308,  ..., -0.5548, -1.3610, -1.9329],\n           [-0.0071,  0.1977,  1.5517,  ..., -1.6664,  1.6551,  0.1798],\n           [-1.0404,  0.6524,  0.4654,  ..., -0.5947, -1.0871,  2.2230],\n           ...,\n           [-0.6844,  0.1692, -0.2559,  ...,  0.5511,  0.9734,  0.7936],\n           [-1.1951,  0.5016,  0.8089,  ...,  0.2337, -0.2213, -1.1724],\n           [-0.5055, -0.7491, -1.4940,  ..., -2.1332,  0.9120,  0.2057]],\n \n          [[ 1.3668, -1.1680, -0.8574,  ..., -0.0635, -1.9132, -0.6023],\n           [ 1.0974, -0.9654,  1.2987,  ...,  1.3187, -0.0241, -0.5427],\n           [-2.0427, -1.4358, -0.7115,  ...,  0.1088,  0.0764,  0.7254],\n           ...,\n           [ 1.0957,  1.4058, -0.0178,  ...,  0.5748,  0.0953,  0.7550],\n           [ 0.4080,  0.8792,  0.6801,  ..., -0.7215,  1.1261,  0.0551],\n           [-0.3183, -2.3306,  0.7155,  ...,  0.4291, -0.2074, -1.1237]],\n \n          [[-0.2401,  0.9229,  0.0212,  ...,  0.2128, -0.4705, -0.3262],\n           [ 0.1108,  0.8909,  0.5309,  ..., -1.7175, -1.6657, -1.7706],\n           [-0.1654, -0.4582, -1.2832,  ...,  0.5297, -0.8363,  1.0293],\n           ...,\n           [-1.3526,  2.1482,  0.5417,  ..., -2.2156, -1.9940, -0.9745],\n           [-0.5821,  0.0492,  0.6693,  ..., -0.8610,  0.5864, -0.6040],\n           [ 1.0180,  1.4447,  0.9563,  ...,  0.9034,  0.7988, -1.7119]],\n \n          [[-1.6146,  0.0868,  0.6415,  ...,  0.2083,  0.4058,  0.2813],\n           [ 0.1969, -0.3334, -0.6526,  ..., -1.4639, -1.6302, -0.6036],\n           [ 0.1556, -0.0859, -0.0230,  ..., -0.7900, -0.3481,  0.8767],\n           ...,\n           [ 0.6056,  0.8374, -0.3834,  ..., -0.6636, -0.4814,  0.8244],\n           [ 0.6982, -0.4884, -1.3777,  ...,  0.5876, -2.0944,  0.0853],\n           [ 0.0388, -0.5761, -0.5116,  ..., -1.6645,  0.1752, -0.1923]]]],\n        device='cuda:0'))\n\n\n\nscheduler.set_timesteps(num_inference_steps)\n\nscheduler.timesteps.shape, pipe.scheduler.timesteps\n\n(torch.Size([50]),\n tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n         856.2857, 835.8980, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n         713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n         570.8571, 550.4694, 530.0816, 509.6939, 489.3061, 468.9184, 448.5306,\n         428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n         285.4286, 265.0408, 244.6531, 224.2653, 203.8776, 183.4898, 163.1020,\n         142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n           0.0000], dtype=torch.float64))\n\n\n\n\n\n\n\\tilde{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{z}_t, \\mathbf{c}) = w\\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) + (1-w)\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t).\n\nHere, \\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) and \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t) are conditional and unconditional \\boldsymbol{\\epsilon}-predictions, given by \\boldsymbol{\\epsilon}_\\theta := (\\mathbf{z}_t - \\alpha_t\\hat{\\mathbf{x}}_\\theta)/\\sigma_t, and w is the guidance weight. Setting w = 1 disables classifier-free guidance, while increasing w &gt; 1 strengthens the effect of guidance.1\n\nfrom tqdm.auto import tqdm\n\nlatents = latents * scheduler.init_noise_sigma\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(\n            latent_model_input, t, encoder_hidden_states=text_embeddings\n        ).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = (\n        guidance_scale * noise_pred_text + (1 - guidance_scale) * noise_pred_uncond\n    )\n\n    # compute the previous noisy sample x_t -&gt; x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n\nfrom PIL import Image\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\n\n\nfor pil_image in pil_images:\n    display(pil_image)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#references",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#references",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "Patil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#footnotes",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#footnotes",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSaharia et al. (2022) Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, https://arxiv.org/abs/2205.11487↩︎"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html",
    "href": "posts/solar_eruptions_magnetic_fields/index.html",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "",
    "text": "This post is based on the presentation during the lab meeting on 2023-11-28."
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#heliophysics-2024-decadal-whitepapers",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#heliophysics-2024-decadal-whitepapers",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Heliophysics 2024 Decadal Whitepapers",
    "text": "Heliophysics 2024 Decadal Whitepapers"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#d-storage-release-caspi2023",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#d-storage-release-caspi2023",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "3D Storage & Release (Caspi et al. 2023)",
    "text": "3D Storage & Release (Caspi et al. 2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#solar-eruptions---storage-release-caspi2023",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#solar-eruptions---storage-release-caspi2023",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Solar eruptions - Storage & Release (Caspi et al. 2023)",
    "text": "Solar eruptions - Storage & Release (Caspi et al. 2023)\n\nCoronal mass ejections (CMEs)\nSolar flares\n\neruptive flares : flares with a CME\nconfined flares : flares not associated with a CME"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#the-big-open-questions-caspi2023",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#the-big-open-questions-caspi2023",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "The Big Open Questions (Caspi et al. 2023)",
    "text": "The Big Open Questions (Caspi et al. 2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#d-knowledge-caspi2023",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#d-knowledge-caspi2023",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "3D Knowledge (Caspi et al. 2023)",
    "text": "3D Knowledge (Caspi et al. 2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#solar-magnetic-field-models",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#solar-magnetic-field-models",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Solar magnetic field models",
    "text": "Solar magnetic field models\n\nStatic model (time-independent); \\mathbf{v} = \\mathbf{0}\n\nForce-free field model : initial condition of MHD simulations\n\npros: related with magnetic field observations directly\ncons: cannot explain the forced structures in the photosphere and lower chromosphere and their dynamical evolution\n\nMagnetohydrostatic (MHS) model\n\npros: partly overcome the disadvantage of the force-free field model\ncons: still static model\n\n\nDynamic model (time-dependent); \\mathbf{v} \\neq \\mathbf{0}\n\nMagnetohydrodynamic (MHD) model\n\npros: may provide the best way to study the observed complex magnetic structures and dynamical evolution\ncons: the most difficult, complicated and expensive model\n\n\n\n\nGuo et al. (2017b)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#plasma-beta-in-the-solar-atmosphere",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#plasma-beta-in-the-solar-atmosphere",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Plasma \\beta in the solar atmosphere",
    "text": "Plasma \\beta in the solar atmosphere\n\n\\beta = \\dfrac{p}{\\frac{B^2}{8\\pi}} is the ratio of gas pressure to magnetic pressure (cgs-Gaussian unit)\n\n\n\n\n\n\n\n\n\nGary (2001), Wiegelmann et al. (2017)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#suggested-physical-mechanisms",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#suggested-physical-mechanisms",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Suggested physical mechanisms",
    "text": "Suggested physical mechanisms\n\n\n\n\n\n\n\n\nGreen et al. (2018)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-fields",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-fields",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Magnetic fields",
    "text": "Magnetic fields\n\nFundamental quantities\n\nEnergy\nHelicity\nSurface-calculated quantities\n\nInstabilities\n\nKink instability\nTorus instability\nDouble arc instability\n\nMagnetic topology analysis (magnetic reconnection)\n\nSeparatrices\nQuasi-Separatrix Layers (QSLs)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-energy",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-energy",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Magnetic energy",
    "text": "Magnetic energy\n\nTotal magnetic energy E in a volume V (cgs-Gaussian units)\n\n\nE = \\frac{1}{8\\pi} \\int_{V} B^2 \\rm{d}V\n\n\n\n\n\n\n\n\n\nDeRosa et al. (2009)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#helmholtz-decomposition",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#helmholtz-decomposition",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Helmholtz decomposition",
    "text": "Helmholtz decomposition\n\nMagnetic field\n\n\n\\mathbf{B} = \\mathbf{B}_\\text{p} + \\mathbf{B}_\\text{J}\n\n\nPotential (current-free) field\n\n\n\\mathbf{B}_\\text{p} = \\nabla \\phi\n\n\nNon-potential (current-carrying) component of the magnetic field\n\n\n\\mathbf{B}_\\text{J} = \\mathbf{B} - \\mathbf{B}_\\text{p}\n\n\nCurrent density in MHD (cgs-Gaussian units)\n\n\n\\mathbf{J} = \\frac{1}{4\\pi} \\nabla \\times \\mathbf{B}"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#potential-field",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#potential-field",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Potential field",
    "text": "Potential field\n\nUsually, the potential field \\mathbf{B}_p is computed from the same distribution of normal field of \\mathbf{B} on the boundary of V.\n\n\n(\\mathbf{\\hat{n}} \\cdot \\mathbf{B}_\\text{p})|_{\\partial V} = (\\mathbf{\\hat{n}} \\cdot \\mathbf{B})|_{\\partial V}\n\n\nThen, the potential field \\mathbf{B}_\\text{p} = \\nabla \\phi is calculated by solving the Laplace equation with the Neumann boundary condition.\n\n\n\\begin{cases}\n\\nabla^2 \\phi &= 0\\\\\n(\\mathbf{\\hat{n}} \\cdot \\nabla \\phi)|_{\\partial V} & = (\\mathbf{\\hat{n}} \\cdot \\mathbf{B})|_{\\partial V}\n\\end{cases}\n\n\nWith such a prescription, the potential field is chosen as a reference is uniquely defined and represents the minimal energy state for a given distribution of the normal component of the field on the boundaries.\n\n\nValori et al. (2013)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-free-energy",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-free-energy",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Magnetic free energy",
    "text": "Magnetic free energy\n\nWithout changing the field significantly at the boundaries of the considered volume, the energy that can be converted into kinetic and thermal energies is given by the free enery, i.e., by the difference between the total magnetic energy and the energy of the corresponding current-free (potential) field.\n\n\nE_\\text{free} = E - E_\\text{p}\n\n\nTo use this formula, the magnetic field must be divergence-free (solenoidal).\n\n\n\n\n\n\n\n\n\nValori et al. (2013)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#accuracy-of-magnetic-energy-computations-valori2013",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#accuracy-of-magnetic-energy-computations-valori2013",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Accuracy of magnetic energy computations (Valori et al. 2013)",
    "text": "Accuracy of magnetic energy computations (Valori et al. 2013)\n\nDecomposition of the magnetic energy (Thomson’s theorem extended by Valori et al. 2013)\n\n\nE = E_\\text{p, s} + E_\\text{J, s} + E_\\text{p, ns} + E_\\text{J, ns} + E_\\text{mix}\n\nE_\\text{p, s} : energy of solenoidal component of the potential field (positive)\nE_\\text{J, s} : energy of solenoidal component of the current-carrying field (positive)\nE_\\text{p, ns} : energy of nonsolenoidal component of the potential field (positive)\nE_\\text{J, ns} : energy of nonsolenoidal component of the current-carrying field (positive)\nE_\\text{mix} : energy corresponding to all cross terms (can be negative)\n\n\\begin{align*}\nE_{\\text{mix}} & = \\frac{1}{4\\pi} \\left( \\int_{V} \\mathbf{B}_\\text{p,s} \\cdot \\nabla \\zeta \\, dV + \\int_{V} \\mathbf{B}_\\text{J,s} \\cdot \\nabla \\psi \\, dV \\right. \\\\\n& \\left. + \\int_{V} \\mathbf{B}_\\text{p,s} \\cdot \\nabla \\psi \\, dV + \\int_{V} \\mathbf{B}_\\text{J,s} \\cdot \\nabla \\zeta \\, dV \\right. \\\\\n& \\left. + \\int_{V} \\nabla \\zeta \\cdot \\nabla \\psi \\, dV + \\int_{V} \\mathbf{B}_\\text{p,s} \\cdot \\mathbf{B}_\\text{J,s} \\, dV \\right)\n\\end{align*}"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#accuracy-of-magnetic-energy-computations-valori2013-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#accuracy-of-magnetic-energy-computations-valori2013-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Accuracy of magnetic energy computations (Valori et al. 2013)",
    "text": "Accuracy of magnetic energy computations (Valori et al. 2013)\n\nFor a perfectly solenoidal field,\n\n\nE = E_\\text{p, s} + E_\\text{J, s}\n\n\nE_\\text{J, s} = E - E_\\text{p, s}\n\n\nThe free energy of the field is the energy of the solenoidal component of its current-carrying part\n\n\nE_\\text{free} = E_\\text{J, s}\n\n\nFor a nonsolenoidal field, \\tilde{E}_\\text{free} even can be negative.\n\n\n\\begin{align*}\n\\tilde{E}_{\\text{free}} & = E - E_\\text{p} \\\\\n& = E - (E_\\text{p, s} + E_\\text{p, ns}) \\\\\n& = E_\\text{J, s} + E_\\text{J, ns} + E_\\text{mix} \\\\\n& = E_\\text{free} + E_\\text{J, ns} + E_\\text{mix} \\\\\n\\end{align*}"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#accuracy-of-magnetic-energy-computations-valori2013-2",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#accuracy-of-magnetic-energy-computations-valori2013-2",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Accuracy of magnetic energy computations (Valori et al. 2013)",
    "text": "Accuracy of magnetic energy computations (Valori et al. 2013)\n\nE_\\text{div} : upper limit of the energy associated with all non-solenoidal components\n\n\nE_\\text{div} = E_\\text{p, ns} + E_\\text{J, ns} + |E_\\text{mix}|\n\n\nAt least (Valori et al. 2016),\n\n\nE_\\text{div} / E &lt; 0.1\n\n\nMore strictly (Thalmann et al. 2019),\n\n\nE_\\text{div} / E \\lesssim 0.05\n\n\nSimpler test (Mastrano et al. 2018) \\nabla \\cdot \\mathbf{B} = 0 \\Rightarrow W_\\text{f1} = W_\\text{f2} (inverse is not true)\n\n\nW_\\text{f1} = \\frac{1}{8\\pi} \\int_V (\\mathbf{B} - \\mathbf{B}_\\text{p})^2 \\rm{d}V\n\n\nW_\\text{f2} = \\frac{1}{8\\pi} \\int_V \\mathbf{B}^2 \\rm{d}V - \\frac{1}{8\\pi} \\int_V \\mathbf{B}_\\text{p}^2 \\rm{d}V"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#how-to-calculate-potential-field-mathbfb_textp-nabla-phi",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#how-to-calculate-potential-field-mathbfb_textp-nabla-phi",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "How to calculate potential field \\mathbf{B}_\\text{p} = \\nabla \\phi",
    "text": "How to calculate potential field \\mathbf{B}_\\text{p} = \\nabla \\phi\n\n\\mathbf{\\hat{n}} \\cdot \\mathbf{B} is given on the all boundaries\n\nSolve Laplace equation directly\n\n\n\n\\begin{cases}\n\\nabla^2 \\phi &= 0\\\\\n(\\mathbf{\\hat{n}} \\cdot \\nabla \\phi)|_{\\partial V} & = (\\mathbf{\\hat{n}} \\cdot \\mathbf{B})|_{\\partial V}\n\\end{cases}\n\n\n\\mathbf{\\hat{n}} \\cdot \\mathbf{B} is given on the only one boundary (usually bottom one)\n\nUse alternative ways such as Green’s function methods (Sakurai 1982)\n\n\n\n\\phi(\\mathbf{r}) = \\int_{S} \\frac{B_n(\\mathbf{r}')}{2\\pi|\\mathbf{r}-\\mathbf{r'}|} \\rm{d}S\n\n\nIf the reference potential field does not have the same normal components on all boundaries, then the potential field does not satisfy Thomson’s theorem, and does not represent the state with the minimum energy for a given distribution of magnetic field on the boundaries (Mastrano et al. 2018)."
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-helicity",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-helicity",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Magnetic helicity",
    "text": "Magnetic helicity\n\nClassical magnetic helicity\n\n\n\\mathscr{H} = \\int_V \\mathbf{A} \\cdot \\mathbf{B} \\rm{d}V\n\n\nRelative magnetic helicity (Berger & Field (1984) and Finn & Antonsen (1985))\n\n\nH_V = \\int_V (\\mathbf{A} + \\mathbf{A}_\\text{p}) \\cdot (\\mathbf{B} - \\mathbf{B}_\\text{p}) \\rm{d}V\n\n\n\\mathbf{A} : vector potential of \\mathbf{B} = \\nabla \\times \\mathbf{A}\n\\mathbf{A}_\\text{p} : vector potential of \\mathbf{B}_\\text{p} = \\nabla \\times \\mathbf{A}_\\text{p}\nIn order for H_V to be gauge invariant,\n\n\\mathbf{B} and \\mathbf{B}_\\text{p} are solenoidal (divergence-free)\nreference field \\mathbf{B}_\\text{p} satisfies (\\mathbf{\\hat{n}} \\cdot \\mathbf{B}_\\text{p})|_{\\partial V} = (\\mathbf{\\hat{n}} \\cdot \\mathbf{B})|_{\\partial V}\n\nThe usual choice of the reference field is the electirc current-free (potential) field.\n\n\nValori et al. (2016)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#decomposition-of-the-magnetic-helicity-berger2003",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#decomposition-of-the-magnetic-helicity-berger2003",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Decomposition of the magnetic helicity (Berger 2003)",
    "text": "Decomposition of the magnetic helicity (Berger 2003)\n\nRelative magnetic helicity\n\n\nH_V = \\int_V (\\mathbf{A} + \\mathbf{A}_\\text{p}) \\cdot (\\mathbf{B} - \\mathbf{B}_\\text{p}) \\rm{d}V\n\n\nDecomposition of H_V\n\n\nH_V = H_\\text{J} + H_\\text{PJ}\n\n\nH_\\text{J}: classical magnetic helicity of \\mathbf{B}_\\text{J} = \\mathbf{B} - \\mathbf{B}_\\text{p}\n\n\nH_\\text{J} = \\int_V (\\mathbf{A} - \\mathbf{A}_\\text{p}) \\cdot (\\mathbf{B} - \\mathbf{B}_\\text{p}) \\rm{d}V\n\n\nH_\\text{PJ}: a sorf of multual helicity between \\mathbf{B}_\\text{p} and \\mathbf{B}_\\text{J}\n\n\nH_\\text{PJ} = 2\\int_V \\mathbf{A}_\\text{p} \\cdot (\\mathbf{B} - \\mathbf{B}_\\text{p}) \\rm{d}V"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#reliability-of-magnetic-energy-and-helicity-computations-thalmann2019",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#reliability-of-magnetic-energy-and-helicity-computations-thalmann2019",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Reliability of magnetic energy and helicity computations (Thalmann et al. 2019)",
    "text": "Reliability of magnetic energy and helicity computations (Thalmann et al. 2019)\n\nQuantitative threshold for the divergence-freeness\n\n\nE_\\text{div} / E \\lesssim 0.05\n\nHere, E_\\text{div} = E_\\text{p, ns} + E_\\text{J, ns} + |E_\\text{mix}| is the nonpotential contribution and E is the total magnetic energy."
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#reliability-of-magnetic-energy-and-helicity-computations-thalmann2019-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#reliability-of-magnetic-energy-and-helicity-computations-thalmann2019-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Reliability of magnetic energy and helicity computations (Thalmann et al. 2019)",
    "text": "Reliability of magnetic energy and helicity computations (Thalmann et al. 2019)\n\nNOAA 11158 between February 12 00:00 UT and February 16 00:00 UT\nFree parameters\n\nPreprocessing (Wiegelmann et al. 2006)\n\n(\\mu_1, \\mu_2, \\mu_3, \\mu_4)\n\nNLFFF optimization method of Wiegelmann & Inhester (2010)\n\n(w_f, w_d, w_\\text{hor}, \\nu)\n\n\n(\\mu_1, \\mu_2, w_f, \\nu) = (1, 1, 1, 10^{-3})\n\nSERIES I\n\n(\\mu_3, \\mu_4, w_d, w_\\text{hor}) = (10^{-3}, 10^{-3}, 1, 1)\n\nSERIES II\n\n(\\mu_3, \\mu_4, w_d, w_\\text{hor}) = (10^{-3}, 10^{-3}, 2, \\propto B_\\text{hor})"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#reliability-of-magnetic-energy-and-helicity-computations-thalmann2019-2",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#reliability-of-magnetic-energy-and-helicity-computations-thalmann2019-2",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Reliability of magnetic energy and helicity computations (Thalmann et al. 2019)",
    "text": "Reliability of magnetic energy and helicity computations (Thalmann et al. 2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoulomb gauge\n\n\n\\nabla \\cdot \\mathbf{A} = \\nabla \\cdot \\mathbf{A}_\\text{p} = 0\n\n\nDeVore gauge (DeVore 2000)\n\n\nA_z = A_\\text{p, z} = 0"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#how-to-calculate-magnetic-helicity",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#how-to-calculate-magnetic-helicity",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "How to calculate magnetic helicity",
    "text": "How to calculate magnetic helicity\n\n\n\n\n\n\n\n\nValori et al. (2016)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#connectivity-based-methods",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#connectivity-based-methods",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Connectivity-based methods",
    "text": "Connectivity-based methods\n\nUse the magnetic connectivity matrix to calculate E_\\text{c} and H_m (Georgoulis et al. 2012)\n\n\n\n\\begin{align*}\nE_\\text{c} & = E_{\\text{c}_\\text{self}} + E_{\\text{c}_\\text{mul}} \\\\\n& = Ad^2 \\sum_{l=1}^{N} \\alpha_l^2 \\Phi_l^{2\\delta} + \\frac{1}{8\\pi}\\sum_{l=1}^{N}\\sum_{m=1, l\\neq m}^{N} \\alpha_l \\mathcal{L}_{lm}^{\\text{arch}}\\Phi_{l}\\Phi_{m}\n\\end{align*}\n\n\n\n\n\\begin{align*}\nH_\\text{m} & = H_{\\text{m}_\\text{self}} + H_{\\text{m}_\\text{mul}} \\\\\n& = 8\\pi d^2 A \\sum_{l=1}^{N} \\alpha_l \\Phi_l^{2\\delta} + \\sum_{l=1}^{N}\\sum_{m=1, l\\neq m}^{N} \\mathcal{L}_{lm}^{\\text{arch}}\\Phi_{l}\\Phi_{m}\n\\end{align*}\n\n\n\n\n\n\n\n\n\n\nTziotziou et al. (2012)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#free-energy-relative-helicity-diagram-of-solar-ars-tziotziou2012",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#free-energy-relative-helicity-diagram-of-solar-ars-tziotziou2012",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Free energy – Relative helicity diagram of solar ARs (Tziotziou et al. 2012)",
    "text": "Free energy – Relative helicity diagram of solar ARs (Tziotziou et al. 2012)\n\nBlue diamonds : Non-flaring ARs\nRed squares/asterisks : M-/X-class flaring ARs\n\n\n\n\n\n\n\n\n\n\nThreshold for relative magnetic helicity H_\\text{V}  (denoted as |H_m| in this diagram)\n\n\n|H_\\text{V}| \\sim 2 \\times 10^{42} \\text{ Mx}^2\n\n\nThreshold for free magnetic energy E_\\text{free}  (denoted as E_\\text{c} in this diagram)\n\n\nE_\\text{free} \\sim 4 \\times 10^{31} \\text{ erg}\n\n\nScaling relation (Tziotziou et al. 2014)\n\n\n|H_V| \\propto E_\\text{free}^{0.84 \\pm 0.05}"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#relative-magnetic-helicity-as-a-diagnostic-of-solar-eruptivity-pariat2017",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#relative-magnetic-helicity-as-a-diagnostic-of-solar-eruptivity-pariat2017",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Relative magnetic helicity as a diagnostic of solar eruptivity (Pariat et al. 2017)",
    "text": "Relative magnetic helicity as a diagnostic of solar eruptivity (Pariat et al. 2017)\n\n\n\n\n\n\n\n\n\n3D visco-resistive MHD simulations\n\nND : no arcade\nWD : weak arcade\nMD : medium arcade\nSD : strong arcade\n\n\n\n\n\nfrom t = 0 to t=200\n\nt &lt; 30 : emerging flux rope rises in the convection zone\nt \\in [30, 120] : pre-eruptive phase\nt \\in [120, 150] : eruptive phase\nt  &gt; 150 : post-eruptive phase"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#relative-magnetic-helicity-as-a-diagnostic-of-solar-eruptivity-pariat2017-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#relative-magnetic-helicity-as-a-diagnostic-of-solar-eruptivity-pariat2017-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Relative magnetic helicity as a diagnostic of solar eruptivity (Pariat et al. 2017)",
    "text": "Relative magnetic helicity as a diagnostic of solar eruptivity (Pariat et al. 2017)\n\nH_V = H_\\text{J} + H_\\text{PJ}\n\n\nE = E_\\text{free} + E_\\text{p}\n\nHere, E = E_\\text{mag} and E_\\text{inj} = E_\\text{mag} - E_\\text{mag}(t=0)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#relative-magnetic-helicity-as-a-diagnostic-of-solar-eruptivity-pariat2017-2",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#relative-magnetic-helicity-as-a-diagnostic-of-solar-eruptivity-pariat2017-2",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Relative magnetic helicity as a diagnostic of solar eruptivity (Pariat et al. 2017)",
    "text": "Relative magnetic helicity as a diagnostic of solar eruptivity (Pariat et al. 2017)\n\nH_V = H_\\text{J} + H_\\text{PJ}\n\n\nE = E_\\text{free} + E_\\text{p}\n\nHere, E = E_\\text{mag} and E_\\text{inj} = E_\\text{mag} - E_\\text{mag}(t=0)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#changes-of-magnetic-energy-and-helicity-in-solar-ars-from-major-flares-liu2023",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#changes-of-magnetic-energy-and-helicity-in-solar-ars-from-major-flares-liu2023",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Changes of Magnetic Energy and Helicity in Solar ARs from Major Flares (Liu et al. 2023)",
    "text": "Changes of Magnetic Energy and Helicity in Solar ARs from Major Flares (Liu et al. 2023)\n\nHere, H = H_V"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---sharp-parameters",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---sharp-parameters",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - SHARP parameters",
    "text": "Surface-calculated quantities - SHARP parameters\n\nSOLAR FLARE PREDICTION USING SDO/HMI VECTOR MAGNETIC FIELD DATA WITH A MACHINE-LEARNING ALGORITHM (Bobra et al. 2015)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---mathbfv-or-mathbfe",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---mathbfv-or-mathbfe",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - \\mathbf{v} or \\mathbf{E}",
    "text": "Surface-calculated quantities - \\mathbf{v} or \\mathbf{E}\n\n\\mathbf{v} : DAVE4VM (differential affine velocity estimator for vector magnetograms) (Schuck 2008)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---mathbfv-or-mathbfe-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---mathbfv-or-mathbfe-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - \\mathbf{v} or \\mathbf{E}",
    "text": "Surface-calculated quantities - \\mathbf{v} or \\mathbf{E}\n\n\\mathbf{E} : PDFI_SS (Fisher et al. 2020)\n\nPDFI : poloidal–toroidal decomposition (PTD) plus Doppler plus Fourier local correlation tracking (FLCT) plus ideal\nSS : spherical staggered"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---mathbfv-or-mathbfe-2",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---mathbfv-or-mathbfe-2",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - \\mathbf{v} or \\mathbf{E}",
    "text": "Surface-calculated quantities - \\mathbf{v} or \\mathbf{E}\n\nDAVE4VM vs PDFI_SS (Wang et al. 2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - Flux of quantities",
    "text": "Surface-calculated quantities - Flux of quantities\n\nMagnetic energy (Poynting) flux (Schuck 2006; Guo et al. 2017b; Liu et al. 2023)\n\n\n\\left.\\frac{dE}{dt}\\right|_{S} = \\frac{1}{4\\pi}\\int_{S}B_t^2V_{\\perp n}\\rm{d}S - \\frac{1}{4\\pi}\\int_{S}(\\mathbf{B}_t \\cdot \\mathbf{V}_{\\perp t})B_n \\rm{d}S\n\n\nFlux of magnetic helicity (Berger 1984; Chae 2001; Liu et al. 2023)\n\n\n\\left.\\frac{dH}{dt}\\right|_{S} = 2\\int_{S}(\\mathbf{A}_p \\cdot \\mathbf{B}_t)\\rm{d}S - 2\\int_{S}(\\mathbf{A}_p \\cdot \\mathbf{V}_{\\perp t})B_n \\rm{d}S"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - Flux of quantities",
    "text": "Surface-calculated quantities - Flux of quantities\n\nOn short timescales (~1 hr) after eruptive flares, there are decreases in both the coronal magnetic energy, E, and helicity H; and their photospheric fluxes, dE/dt and dH/dt. (Liu et al. 2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities-2",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities-2",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - Flux of quantities",
    "text": "Surface-calculated quantities - Flux of quantities\n\nFluxes of topological quantities (Alielden et al. 2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities-3",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#surface-calculated-quantities---flux-of-quantities-3",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Surface-calculated quantities - Flux of quantities",
    "text": "Surface-calculated quantities - Flux of quantities\n\nFluxes of topological quantities (Alielden et al. 2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-flux-rope-mfr",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-flux-rope-mfr",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Magnetic flux rope (MFR)",
    "text": "Magnetic flux rope (MFR)\n\nMagnetic flux rope (MFR) can be defined as a coherent group of magnetic field lines winding an axis with more than one turn (Liu et al. 2016; Duan et al. 2019).\nMagnetic flux rope of the Titov–Démoulin semi-analytical NLFFF model (Titov et al. 1999; Guo et al. 2017a)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#kink-instability-ki",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#kink-instability-ki",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Kink instability (KI)",
    "text": "Kink instability (KI)\n\nMagnetic twist number T_\\text{w} for a given (closed) field line (Berger et al. 2006; Jing et al. 2018; Duan et al. 2019)\n\n\nT_\\text{w} = \\int_L \\frac{(\\nabla \\times \\mathbf{B}) \\cdot \\mathbf{B}}{4 \\pi B^2} \\rm{d}l\n\n\nIf the magnetic field is force-free, i.e., \\nabla \\times \\mathbf{B} = \\alpha \\mathbf{B},\n\n\nT_\\text{w} = \\frac{1}{4\\pi} \\int_L \\alpha \\rm{d}l\n\n\nKI parameter - There is a critical value to determine whether MFR is eruptive or not.\n\n\n|T_\\text{w}| &gt; |T_\\text{w}|_\\text{crit}\n\n\nT_\\text{w} is not identical to the classical winding number of field lines about a common axis, the parameter often used in the analysis of the helical KI. Nevertheless, accroding to Liu et al.’s (2016) analysis, the magnetic field line that possesses the extremum value (maximum or minimum) of |T_\\text{w}| in a MFR can be reliably regarded as the rope axis, and T_\\text{w} computed in the vicinity of the axis approaches the winding number. (Duan et al. 2019)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#identification-of-mfrs-duan2019",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#identification-of-mfrs-duan2019",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Identification of MFRs (Duan et al. 2019)",
    "text": "Identification of MFRs (Duan et al. 2019)\n\nMFRs generally exist prior to major solar flares. With a rigorous definition, over 90% of the studied events have well-defined MFRs in the flare site, i.e., a coherent group of magnetic field lines with twists above one turn and the field line possessing the peak value of twist being the rope axis. The other 10% of events also have MFR-like structures as their magnetic twist numbers are very close to one. Most of the MFRs have corresponding filaments or filament channels as seen in SDO/AIA 304 Å observations. (Duan et al. 2019)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#identification-of-mfrs-duan2019-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#identification-of-mfrs-duan2019-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Identification of MFRs (Duan et al. 2019)",
    "text": "Identification of MFRs (Duan et al. 2019)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#torus-instability-ti",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#torus-instability-ti",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Torus instability (TI)",
    "text": "Torus instability (TI)\n\nDecay index n (Kliem et al. 2006; Jing et al. 2018; Duan et al. 2019)\n\n\nn = - \\frac{\\partial \\log (B_\\text{ext})}{\\partial \\log (h)}\n\n\nB_\\text{ext} : external strapping field stabilizing the MFR (usually potential field)\nh : vertical height locally, radial distance globally, or oblique height\nTI parameter - There is a critical value to determine whether MFR is eruptive or not.\n\n\nn &gt; n_\\text{crit}"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#torus-instability-ti-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#torus-instability-ti-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Torus instability (TI)",
    "text": "Torus instability (TI)\n\nIllustration of calculating the decay index n at the apex of the MFR axis (Duan et al. 2019)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "TI Parameter - KI Parameter diagram",
    "text": "TI Parameter - KI Parameter diagram\n\nLaboratory experiment designed to study the Sun-like line-tied MFRs (Myers et al. 2015)\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "TI Parameter - KI Parameter diagram",
    "text": "TI Parameter - KI Parameter diagram\n\nLaboratory experiment designed to study the Sun-like line-tied MFRs (Myers et al. 2015)\n\n\n\n\n\n\n\n\n\n\nPotential field decay index n\n\n\nn(z) = - \\frac{z}{|\\mathbf{B}_\\text{pot}|}\\frac{\\partial |\\mathbf{B}_\\text{pot}|}{\\partial z}\n\n\nEdge safety factor q_a : inverse of the edge magnetic twist \\iota_a\n\n\nq_a = \\frac{2\\pi}{\\iota_a} = \\left.\\frac{\\rm{d}\\Phi_\\text{T}}{\\rm{d}\\psi_\\text{P}}\\right|_{r=a} \\approx \\frac{2\\pi a}{L}\\frac{B_\\text{Ta}}{B_\\text{Pa}}"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram-2",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram-2",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "TI Parameter - KI Parameter diagram",
    "text": "TI Parameter - KI Parameter diagram\n\nStatistical Analysis of Torus and Kink Instabilities in Solar Eruptions (Jing et al. 2018)\n\nBlack : confined flare\nColor : ejective flare\n\n\n\n\n\n\n\n\n\n\n\n\nT_\\text{w} appears to play little role in discriminating between confined and ejective events\n\n\nThe events with n \\gtrsim 0.8 are all ejective, and all confined events have n \\lesssim 0.8. However, n \\gtrsim 0.8 is not a necessary condition for eruption because some events with n \\lesssim 0.8 also erupted."
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram-3",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#ti-parameter---ki-parameter-diagram-3",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "TI Parameter - KI Parameter diagram",
    "text": "TI Parameter - KI Parameter diagram\n\nA Study of Pre-flare Solar Coronal Magnetic Fields: Magnetic Flux Ropes (Duan et al. 2019)\n\nGreen : non-MFR event (|T_\\text{w}|_\\text{max} &lt; 1)\n\n\n\n\n\n\n\n\n\n\nIt clearly shows lower limits for TI and KI thresholds, which are n_\\text{crit}=1.3 and |T_\\text{w}|_\\text{crit}=2, respectively, as all the events above n_\\text{crit} and nearly 90% of the events above |T_\\text{w}|_\\text{crit}| erupted. Furthermore, by such criterion, over 70% of the events can be discriminated between eruptive and confined flares, and KI seems to play a nearly equally important role as TI in discriminating between the two types of flares. More than half of the events with both parameters are below the lower limits, and 29% are eruptive. These events might be triggered by magnetic reconnection rather than MHD instabilities."
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#tether-cutting-reconnection",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#tether-cutting-reconnection",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Tether-cutting reconnection",
    "text": "Tether-cutting reconnection\n\nTether-cutting reconnection scenario proposed by Moore et al. (2001) explains the eruption of the sigmoidal field often observed before solar eruptions. (Ishiguro et al. 2017)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#tether-cutting-reconnection-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#tether-cutting-reconnection-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Tether-cutting reconnection",
    "text": "Tether-cutting reconnection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChen et al. (2014)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#double-arc-instability-ishiguro2017",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#double-arc-instability-ishiguro2017",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Double arc instability (Ishiguro et al. 2017)",
    "text": "Double arc instability (Ishiguro et al. 2017)\n\nThe tether-cutting scenario proposes that the internal reconnection proceeds in the core of the sheared magnetic field in the pre-eruptive phase, and it may form a double arc flux rope (sigmoidal field) that carries an electric current (Ishiguro et al. 2017).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIshiguro et al. (2017), Kusano et al. (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#double-arc-instability",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#double-arc-instability",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Double arc instability",
    "text": "Double arc instability\n\nDAI parameter \\kappa (Ishiguro et al. 2017; Muhamad et al. 2018; Kusano et al. 2020)\n\n\n\\kappa = T_\\text{w} \\frac{\\Phi_\\text{rec}}{\\Phi_\\text{over}}\n\n\nT_\\text{w} : magnetic twist of the DA\n\\Phi_\\text{rec} : magnetic flux within DA\n\\Phi_\\text{over} : magnetic flux overlying DA\n\n\nThere is a threshold.\n\n\n\\kappa &gt; \\kappa_0"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#a-physics-based-method-that-can-predict-imminent-large-solar-flares-kusano2020",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#a-physics-based-method-that-can-predict-imminent-large-solar-flares-kusano2020",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "A physics-based method that can predict imminent large solar flares (Kusano et al. 2020)",
    "text": "A physics-based method that can predict imminent large solar flares (Kusano et al. 2020)\n\n\\kappa is evaluated by\n\n\n\\kappa = \\left|\\frac{\\int_\\text{rec} T_\\text{w}\\rm{d}\\Phi}{\\Phi_{\\text{over}}}\\right|\n\nwhere the integral is taken over the magnetic flux subject to the trigger-reconnection which forms the DA.\n\nIf the DA is formed by the magnetic reconnection of magnetic fluxes rooted near the PIL, then \\kappa can be calculated by the area integral over the reconnection-region S_\\text{rec} on the photosphere: \n\\kappa = \\left|\\frac{\\int_{S_\\text{rec}} \\tau\\rm{d}S}{\\Phi_{\\text{over}}}\\right|\n\n\nwhere\n\n\\tau = T_\\text{w}|B_z|\n\nis defined as the magnetic twist flux density with the vertical magnetic field B_z."
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#kappa-scheme-kusano2020",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#kappa-scheme-kusano2020",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "\\kappa-scheme (Kusano et al. 2020)",
    "text": "\\kappa-scheme (Kusano et al. 2020)\n\ncalculate T_\\text{w}\ncalculate \\tau = T_\\text{w} |B_z|\ncalculate B_{\\text{np}} = |\\mathbf{B}_{\\text{h}} - \\mathbf{B}_\\text{p}|\nidentify multiple High Free-Energy Regions (HiFERs) using the condition B_\\text{np} &gt; B_0 = 1000 \\text{ G}\nsort the HiFERs by the size (largest size is the fisrt HiFER). S_{H, i} is the area of the i-th HiFER\nselect one point in PIL within HiFER\nconsider hypothetical reconnection region as a circle with r\ncalculate unsigned magnetic flux of each pole\nfor area with small unsigned magnetic flux, calculate \\int_{S_\\text{rec}}  \\tau \\rm{d}S\nidentify magnetic field lines rooted on the hypothetical reconnection region (predicted trigger-reconnection region)\nidentify strongest sheared field line among them, which is defined by field line that reaches farthest from the reconnection region\ncalculate \\Phi_{\\text{over}} for all flux crossing over the strongest sheared field line\ncalculate \\kappa = \\left| \\frac{\\int_{S_\\text{rec}}  \\tau \\rm{d}S}{\\Phi_{\\text{over}}} \\right|\nsince \\kappa depends on r (radius of reconnection region), we can find the minimum r while satisfying \\kappa &gt; \\kappa_0 = 0.1\nThe minium r is the critical length scale r_c\nfor each point with r_c, calculate the area of the footprint of the overlying field within the HiFER S_r\ncalculate the first level of releasable energy E_r = \\displaystyle \\frac{S_r ^{1/2}}{8\\pi}  \\int_{S_r} B^2_{\\text{np}} \\rm{d} S, which is the mininum energy of the flare driven by a DAI\ncalculate the second level of releasable energy for i-th HiFER E_{H, i} = \\displaystyle \\frac{S_{H, i} ^{1/2}}{8\\pi}  \\int_{S_{H, i}} B^2_{\\text{np}} \\rm{d} S\ncalculate the total free energy of an AR E_{\\text{AR}} = \\displaystyle \\sum_i E_{H, i}"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#kappa-scheme-kusano2020-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#kappa-scheme-kusano2020-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "\\kappa-scheme (Kusano et al. 2020)",
    "text": "\\kappa-scheme (Kusano et al. 2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#a-physics-based-method-that-can-predict-imminent-large-solar-flares-kusano2020-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#a-physics-based-method-that-can-predict-imminent-large-solar-flares-kusano2020-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "A physics-based method that can predict imminent large solar flares (Kusano et al. 2020)",
    "text": "A physics-based method that can predict imminent large solar flares (Kusano et al. 2020)\n\nIf any point on a PIL satisfies the conditions r_c &lt; 1 \\text{ Mm} and E_r &gt; 4 \\times 10^{31} \\text{ erg}, an X-class flare usually occurs.\n\n\n\n\n\n\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-topology-analysis",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#magnetic-topology-analysis",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Magnetic topology analysis",
    "text": "Magnetic topology analysis\n\nThe random or user-defined tracing of magnetic field lines does not guarantee that key features of the magnetic field will be identified.\n\n\n\n\n\n\n\n\n\nMagnetic reconnection, responsible for changing both the topology and geometry of a magnetic field, is a typical example of a phenomenon whose occurrence is strongly associated with the geometry of the magnetic field.\nIn particular, magnetic reconnection is linked to the formation of intense field-aligned current sheets, that are induced by the existence of gradients of the magnetic field, following Ampère’s equation (\\mu_0 \\mathbf{J} = \\nabla \\times \\mathbf{B}).\nThese currents develop preferentially in specific locations of the magnetic field, that is in regions where the connectivity of the magnetic field is discontinuous.\nThe aim of the magnetic topology is to determine the locations of such regions.\n\n\nPariat (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#separatrices",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#separatrices",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Separatrices",
    "text": "Separatrices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us consider a field line which links the footpoint \\mathbf{r}_1 of coordinate (x_1, y_1) of the plane P_1 to the footpoint \\mathbf{r}_2, of coordinate (X_2, Y_2) of the plane P_2.\nTwo mappings exist that associate a footpoint on one plane to the other: the mapping \\Pi_{12} from P_1 to P_2: \\mathbf{r}_1(x_1, y_1) \\mapsto \\mathbf{r}_2(X_2, Y_2); and the inverse mapping \\Pi_{21} from P_2 to P_1: \\mathbf{r}_2(X_2, Y_2) \\mapsto \\mathbf{r}_1(x_1, y_1).\nA separatrix is present when the mapping contains a discontinuity, i.e. where the function \\Pi_{12} (or \\Pi_{21}) is discontinuous.\n\n\nLongcope et al. (2008), Pariat et al. (2012), Pariat (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#separatrices---regions-of-magnetic-field-discontinuity",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#separatrices---regions-of-magnetic-field-discontinuity",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Separatrices - Regions of magnetic field discontinuity",
    "text": "Separatrices - Regions of magnetic field discontinuity\n\nNull points \\mathbf{x}_\\text{NP}\n\n\n\\mathbf{B}(\\mathbf{x}_\\text{NP}) = \\mathbf{0}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongcope (2005), Jiang et al. (2017), Mason et al. (2019), Pariat (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#separatrices---regions-of-magnetic-field-discontinuity-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#separatrices---regions-of-magnetic-field-discontinuity-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Separatrices - Regions of magnetic field discontinuity",
    "text": "Separatrices - Regions of magnetic field discontinuity\n\nBald patches (BPs)\n\nRegions where some field lines touch the boundary tangentially\n\n\n\n\\mathbf{x}_\\text{BP} \\in \\partial V \\quad \\text{with } \\partial V \\text{ a line-tying boundary}\n\n\nB_n (\\mathbf{x}_\\text{BP}) = 0\n\n\n(\\mathbf{B} \\cdot \\nabla B_n)(\\mathbf{x}_\\text{BP}) &gt; 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongcope (2005), Jiang et al. (2017), Mason et al. (2019), Pariat (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#separatrices---regions-of-magnetic-field-discontinuity-2",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#separatrices---regions-of-magnetic-field-discontinuity-2",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Separatrices - Regions of magnetic field discontinuity",
    "text": "Separatrices - Regions of magnetic field discontinuity\n\nSeparators\n\n1D topological structures and are found at the intersection of two separatrix surfaces.\nSeparators can be found to connect two 3D magnetic null points.\n\n\n\n\n\n\n\n\n\n\nLongcope (2005), Jiang et al. (2017), Mason et al. (2019), Pariat (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#quasi-separatrix-layers-qsls",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#quasi-separatrix-layers-qsls",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Quasi-Separatrix Layers (QSLs)",
    "text": "Quasi-Separatrix Layers (QSLs)\n\nNumerous solar flares, however, have not been associated with separatrices, i.e. with magnetic-field connectivity discontinuities. This has led to the generalization of the concept of separatrices to Quasi-Separatrix Layers (QSLs). QSLs were introduced in Démoulin et al. (1996) and are defined as regions where the mapping of the field lines, while still continuous (unlike seperatrices), possesses very strong gradients.\nQSLs are 3D magnetic volumes of high squashing factor Q, in which the magnetic connectivity varies strongly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPariat (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#squashing-factor-q",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#squashing-factor-q",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "Squashing factor Q",
    "text": "Squashing factor Q\n\n\n\n\n\n\n\n\nMappings\n\n\\Pi_{12} : \\mathbf{r}_1(x_1, y_1) \\mapsto \\mathbf{r}_2(X_2, Y_2)\n\\Pi_{21} : \\mathbf{r}_2(X_2, Y_2) \\mapsto \\mathbf{r}_1(x_1, y_1).\n\nJacobian matrices associated with the mappings\n\n\nD_{12} = \\frac{d\\mathbf{r}_2}{d\\mathbf{r}_1} = \\begin{pmatrix}\n\\displaystyle  \\frac{\\partial X_2}{\\partial x_1} & \\displaystyle \\frac{\\partial X_2}{\\partial y_1} \\\\\n\\\\\n\\displaystyle  \\frac{\\partial Y_2}{\\partial x_1} & \\displaystyle \\frac{\\partial Y_2}{\\partial y_1}\n\\end{pmatrix}\n\\quad \\quad\nD_{21} = \\frac{d\\mathbf{r}_1}{d\\mathbf{r}_2} = \\begin{pmatrix}\n\\displaystyle  \\frac{\\partial x_1}{\\partial X_2} & \\displaystyle \\frac{\\partial x_1}{\\partial Y_2} \\\\\n\\\\\n\\displaystyle  \\frac{\\partial y_1}{\\partial X_2} & \\displaystyle \\frac{\\partial y_1}{\\partial Y_2}\n\\end{pmatrix}\n\n\nNorms of the Jacobian matrices\n\n\nN_{12} = \\sqrt{\\left( \\frac{\\partial X_2}{\\partial x_1} \\right)^2 + \\left( \\frac{\\partial X_2}{\\partial y_1} \\right)^2 + \\left( \\frac{\\partial Y_2}{\\partial x_1} \\right)^2 + \\left( \\frac{\\partial Y_2}{\\partial y_1} \\right)^2}\n\n\nN_{21} = \\sqrt{\\left( \\frac{\\partial x_1}{\\partial X_2} \\right)^2 + \\left( \\frac{\\partial x_1}{\\partial Y_2} \\right)^2 + \\left( \\frac{\\partial y_1}{\\partial X_2} \\right)^2 + \\left( \\frac{\\partial y_1}{\\partial Y_2} \\right)^2}.\n\n\nDeterminants of the Jacobian matrices\n\n\n\\Delta_{12} = \\text{det}(D_{12})\n\\quad \\quad\n\\Delta_{21} = \\text{det}(D_{21})\n\n\nSquashing factor (or squashing degree) Q for a field line\n\n\nQ = \\frac{N_{12}^2}{|\\Delta_{12}|} = \\frac{N_{21}^2}{|\\Delta_{21}|}\n\n\nPariat (2020)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#qsl-reconnection",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#qsl-reconnection",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "QSL reconnection",
    "text": "QSL reconnection\n\nQSLs are preferential sites for electric current build-up and magnetic reconnection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZhao et al. (2016), Yang et al. (2020), Pariat (2020), Cheng et al. (2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#qsl-reconnection-1",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#qsl-reconnection-1",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "QSL reconnection",
    "text": "QSL reconnection\n\nQSLs have been associated with flare ribbons in a large number of events.\n\n\n\n\n\n\n\n\n\nZhao et al. (2016), Yang et al. (2020), Pariat (2020), Cheng et al. (2023)"
  },
  {
    "objectID": "posts/solar_eruptions_magnetic_fields/index.html#fastqsl-zhang2022",
    "href": "posts/solar_eruptions_magnetic_fields/index.html#fastqsl-zhang2022",
    "title": "Solar Eruptions & Magnetic Fields",
    "section": "FastQSL (Zhang et al. 2022)",
    "text": "FastQSL (Zhang et al. 2022)\n\n\n\n\n\n\n\n\nComputation time : 107s for 900 x 540 x 360 grid data"
  },
  {
    "objectID": "posts/conservation-law/index.html",
    "href": "posts/conservation-law/index.html",
    "title": "보존법칙",
    "section": "",
    "text": "2018년에 물리학실험 보고서용으로 작성한 글"
  },
  {
    "objectID": "posts/conservation-law/index.html#역학적-에너지-보존-법칙",
    "href": "posts/conservation-law/index.html#역학적-에너지-보존-법칙",
    "title": "보존법칙",
    "section": "역학적 에너지 보존 법칙",
    "text": "역학적 에너지 보존 법칙\n뉴턴의 운동법칙은 근본적으로 미분방정식으로 서술된다. 특히 힘과 가속도와 같은 물리량들은 벡터량이기 때문에 다루기가 쉽지 않다. 이러한 이유로 물리학자들은 스칼라량인 일이라는 개념을 만든 후에, 뉴턴 운동법칙을 수학적으로 약간 조작하여 운동에너지라는 새로운 개념을 만들게 되었다. 그리고 그것을 통해 일-(운동)에너지 정리를 이끌어 내었다. 이 정리는 ’물체에 가해진 알짜힘이 한 일은 물체의 운동에너지 변화량과 같다’라는 사실을 알려준다(\\mathbf{F} = m \\mathbf{a}에서의 \\mathbf{F}가 알짜힘(합력, net force)이기 때문이다). 일-운동에너지 정리를 식으로 나타내면 다음과 같다.\n\nW_{\\text{알짜 힘}} = \\Delta K\n\n이 정리는 수학적으로 유도된 정리이기 때문에, 물체에 가해지는 모든 알짜힘에 대해 성립하는 정리이다. 그러나 물리학자들은 여기서 한발 더 나아가 좌변을 다음과 같이 쓰고 싶어했다.\n\n-\\Delta U = \\Delta K\n\n왜냐하면 만약 이렇게 쓸 수만 있다면, K라는 양과 U라는 양이 ’보존’되기 때문이다. 그런데 다행히도 물리학자들은 중력이나 탄성력, 전기력과 같은 특정한 힘들이 하는 일은 위와 같이 쓸수 있음을 알게 되었고, U를 ’위치에너지(퍼텐셜에너지)’라 정의하였다. 또한 퍼텐셜에너지를 정의할 수 있는 힘을 보존력이라고 부르게 되었다. 따라서 다음과 같이 쓸 수 있다.\n\nW_{\\text{보존력}} = -\\Delta U_{\\text{보존력}}\n\n또한 모든 힘은 이러한 보존력과 보존력이 아닌 힘, 즉 비보존력 두 가지로 나뉜다는 사실을 알게 되었다. 일은 힘과 변위벡터의 내적으로 정의되고, 내적은 수학적으로 분배법칙이 성립하기 때문에 일-운동에너지 정리는 다음과 같이 쓸 수 있다.\n\nW_{\\text{보존력}} + W_{\\text{비보존력}} = \\Delta K\n \nW_{\\text{비보존력}} = \\Delta K + \\Delta U_{\\text{보존력}}\n\n이때 물체의 운동에너지와 퍼텐셜에너지의 합을 역학적 에너지라고 정의하게 되면 다음을 얻는다.\n\nW_{\\text{비보존력}} = \\Delta E_{\\text{역학적}}\n\n따라서 비보존력이 일을 하지 않으면 물체의 역학적 에너지는 일정하게 유지된다. 즉 어떤 물체에 보존력만 일을 한다면 그 물체의 역학적 에너지는 일정하게 유지된다. 바로 이것을 역학적 에너지 보존 법칙이라고 부른다.\n\n퍼텐셜 에너지(Potential Energy)의 정의\n\n어떤 물체의 위치벡터가 벡터함수 \\mathbf{r}(t) (a\\leq t \\leq b)로 주어지고, 시간 a\\leq t \\leq b 동안 물체가 움직인 경로가 곡선 C이며 그동안 물체에 가해진 힘이 \\mathbf{F}(\\mathbf{r})일 때(또는 물체가 벡터장 \\mathbf{F}(\\mathbf{r})가 있는 공간에서 곡선 C를 따라 움직였을 때), 힘 \\mathbf{F}가 물체에 한 일 W는 다음과 같이 정의한다. 이때 \\rm{d}\\mathbf{l} = \\rm{d}\\mathbf{r} = \\mathbf{r}'\\rm{d}t 이다.\n\n\n\\begin{align*}\nW & = \\int_{C} \\mathbf{F} \\cdot \\rm{d}\\mathbf{l} \\\\\n& = \\int_{a}^{b} \\mathbf{F}(\\mathbf{r}(t)) \\cdot \\mathbf{r}'(t)\\rm{d}t\n\\end{align*}\n\n\n어떤 힘 \\mathbf{F}가 보존력일때, 즉 위치벡터 \\mathbf{r}(t)에 대한 벡터함수 \\mathbf{F}(\\mathbf{r}(t))가 보존장일때, 다음을 만족하는 스칼라 함수 U(\\mathbf{r}(t))를 힘 \\mathbf{F}에 대한 퍼텐셜 에너지라고 정의한다.\n\n\n\\mathbf{F}(\\mathbf{r}) = -\\nabla U(\\mathbf{r})\n\n\n어떤 힘 \\mathbf{F}가 \\mathbf{F}(\\mathbf{r}) = - \\nabla U(\\mathbf{r})로 주어지는 보존력이고, 위치벡터가 \\mathbf{r}(t)로 주어지는 물체가 시간 a \\leq t \\leq b동안 힘 \\mathbf{F}를 받으며 경로 C를 따라 움직였다고 하면, 선적분의 기본정리에 의해 다음이 성립한다.\n\n\nU(\\mathbf{r}(b)) - U(\\mathbf{r}(a)) = - \\int_{C} \\mathbf{F} \\cdot \\rm{d}\\mathbf{l}\n\n\n따라서 보존력 \\mathbf{F}에 대한 위치 \\mathbf{r}에서의 퍼텐셜 에너지 U(\\mathbf{r})은 선적분을 이용해 다음과 같이 쓸 수 있다. 이때 \\mathbf{r}^{*}는 퍼텐셜 에너지가 0이 되는 임의의 기준점이다.\n\n\nU(\\mathbf{r}) = - \\int_{\\mathbf{r}^{*}}^{\\mathbf{r}} \\mathbf{F} \\cdot \\rm{d} \\mathbf{l}"
  },
  {
    "objectID": "posts/conservation-law/index.html#운동량-보존-법칙",
    "href": "posts/conservation-law/index.html#운동량-보존-법칙",
    "title": "보존법칙",
    "section": "운동량 보존 법칙",
    "text": "운동량 보존 법칙\n관성기준계의 원점에 대해 위치벡터가 \\mathbf{r}_{1}, \\mathbf{r}_{2}, \\cdots, \\mathbf{r}_{n}으로 주어지고, 질량이 각각 m_{1}, m_{2}, \\cdots, m_{n}인 n개의 물체들로 구성된 계를 설정하자. 각 물체는 위치벡터의 첨자에 따라 1, 2, \\cdots, n번째 물체라고 하겠다. 관성기준계에 대해 물체의 위치벡터를 표현하였으므로, i번째 물체에 대해 뉴턴 제 2법칙을 적용하면 다음과 같다. 이때 \\mathbf{F}_{\\text{내력}, i}는 i번째 물체를 제외한 계 내부의 n-1개의 물체들이 i번째 물체에 작용하는 힘들의 합력이고, \\mathbf{F}_{\\text{외력}, i}는 주어진 계 외부의 물체들이 i번째 물체에 작용하는 힘들의 합력이다. 또한 \\mathbf{v}_{i} = \\displaystyle \\frac{d}{dt} \\mathbf{r}_{i}이다.\n\n\\mathbf{F}_{\\text{내력}, i} + \\mathbf{F}_{\\text{외력}, i} = m_{i} \\frac{d}{dt} \\mathbf{v}_{i}\n\n이때 뉴턴 제 3법칙에 의해\n\n\\sum_{i=1}^{n} \\mathbf{F}_{\\text{내력}, i} = \\mathbf{0}\n\n이 성립하고, 물체의 질량이 시간에 대해 변하지 않는다는 가정을 한다면\n\nm_{i} \\frac{d}{dt} \\mathbf{v}_{i} = \\frac{d}{dt} (m_{i} \\mathbf{v}_{i})\n\n이 성립한다. 따라서 i번째 물체에 대한 뉴턴 운동방정식 양변에 시그마를 취하면 다음과 같다. 이때 \\mathbf{F}_{\\text{외력}} = \\displaystyle \\sum_{i=1}^{n} \\mathbf{F}_{\\text{외력}, i}이다.\n\n\\mathbf{F}_{\\text{외력}} = \\frac{d}{dt} \\left( \\sum_{i=1}^{n} m_{i} \\mathbf{v}_{i} \\right)\n\n이때 새로운 물리량 \\mathbf{p} \\equiv m\\mathbf{v}를 정의하고, 이것을 (선)운동량이라고 부르자. 그러면 i번째 물체의 운동량은 \\mathbf{p}_{i} = m_{i}\\mathbf{v}_{i}이므로, 위 식의 우변은 계 내부 물체들의 총운동량의 시간에 대한 변화율이다. 그러므로 계 내부 물체들의 총운동량을 \\mathbf{P} = \\displaystyle \\sum_{i=1}^{n} m_{i} \\mathbf{v}_{i}라고 하면 다음 식이 성립한다.\n\n\\mathbf{F}_{\\text{외력}} = \\frac{d}{dt} \\mathbf{P}\n\n여기서 \\mathbf{F}_{\\text{외력}} = \\mathbf{0}일때 \\mathbf{P} = \\text{일정} 임을 알수 있다. 즉 어떤 계에 작용하는 알짜 외력이 0이면, 그 계의 총운동량은 시간에 대해 변하지 않는다. 이 사실을 운동량 보존 법칙이라고 한다."
  },
  {
    "objectID": "posts/conservation-law/index.html#각운동량-보존-법칙",
    "href": "posts/conservation-law/index.html#각운동량-보존-법칙",
    "title": "보존법칙",
    "section": "각운동량 보존 법칙",
    "text": "각운동량 보존 법칙\n일반적으로 힘 \\mathbf{F}가 작용하는 위치의 위치벡터를 \\mathbf{r}이라고 한다면, 그 위치에 있는 물체에 작용하는 토크(돌림힘)는 다음과 같이 주어진다.\n\n\\boldsymbol{\\tau} = \\mathbf{r} \\times \\mathbf{F}\n\n만약 이 힘 \\mathbf{F}가 그 위치에 있는 물체에 작용하는 알짜힘이라면, 뉴턴 제 2법칙에 의해 다음과 같이 나타날 수 있다.\n\n\\boldsymbol{\\tau} = \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt}\n\n그런데 다음이 성립하므로\n\n\\begin{align*}\n\\frac{d}{dt} (\\mathbf{r} \\times \\mathbf{p}) & = \\frac{d\\mathbf{r}}{dt} \\times \\mathbf{p} + \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt} \\\\\n& = \\mathbf{v} \\times m\\mathbf{v} + \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt} \\\\\n& = \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt}\n\\end{align*}\n\n알짜힘에 의한 토크는 다음과 같이 쓸 수 있다. 이때 \\mathbf{l} = \\mathbf{r} \\times \\mathbf{p}은 각운동량이다.\n\n\\begin{align*}\n\\boldsymbol{\\tau} & = \\frac{d}{dt} (\\mathbf{r} \\times \\mathbf{p}) \\\\\n& = \\frac{d\\mathbf{l}}{dt}\n\\end{align*}\n\n여러 물체로 이루어진 계에서 각 물체에 대해, 뉴턴 제 2법칙을 반복적으로 적용하면 다음을 얻는다. 이때 \\mathbf{F}_{\\text{ext}}는 계에 대한 외력의 총합이고, \\mathbf{P}는 계 내부 물체들의 운동량의 총합이다.\n\n\\mathbf{F}_{\\text{ext}} = \\frac{d\\mathbf{P}}{dt}\n\n따라서 \\mathbf{F}_{\\text{ext}}가 작용하는 위치에 작용하는 토크를 \\boldsymbol{\\tau}_{\\text{ext}}라고 쓴다면 다음이 성립한다. 이때 \\mathbf{L}은 계 내부 물체들의 각운동량의 총합이다.\n\n\\boldsymbol{\\tau}_{\\text{ext}} = \\frac{d\\mathbf{L}}{dt}\n\n따라서 알짜 외부 토크가 \\mathbf{0}이면 계의 총 각운동량은 보존됨을 알 수 있고, 이를 각운동량 보존 법칙이라고 한다."
  }
]