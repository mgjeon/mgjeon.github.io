[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Master’s student at Kyung Hee University, studying solar physics and deep learning. My research interest includes magnetohydrodynamics, simulation, and scientific machine learning."
  },
  {
    "objectID": "about.html#mingyu-jeon-전민규",
    "href": "about.html#mingyu-jeon-전민규",
    "title": "About",
    "section": "",
    "text": "I am a Master’s student at Kyung Hee University, studying solar physics and deep learning. My research interest includes magnetohydrodynamics, simulation, and scientific machine learning."
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "Create an image using Diffusers library.\n\n\n\n!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)\n\n\n\n\n\n# 1. Load the autoencoder model which will be used to decode the latents into image space.\nvae = pipe.vae\n\n# 2. Load the tokenizer and text encoder to tokenize and encode the text.\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\n\n# 3. The UNet model for generating the latents.\nunet = pipe.unet\n\n\n\n\nvae\n\nAutoencoderKL(\n  (encoder): Encoder(\n    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (down_blocks): ModuleList(\n      (0): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (1): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (2): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (3): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (decoder): Decoder(\n    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (up_blocks): ModuleList(\n      (0-1): 2 x UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0-2): 3 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (2): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (3): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\ntokenizer\n\nCLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"&lt;|startoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True)\n\n\n\ntext_encoder\n\nCLIPTextModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 768)\n      (position_embedding): Embedding(77, 768)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n\n\n\nunet\n\nUNet2DConditionModel(\n  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n\n\n\n\n\ntext_input = tokenizer(\n    prompt * batch_size,\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\ntext_input.shape, text_input\n\n(torch.Size([2, 77]),\n tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input)[0]\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nuncond_input = tokenizer(\n    [\"\"] * batch_size,\n    padding=\"max_length\",\n    max_length=text_input.shape[-1],\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nuncond_input.shape, uncond_input\n\n(torch.Size([2, 77]),\n tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input)[0]\n\nuncond_embeddings.shape, uncond_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]]],\n        device='cuda:0'))\n\n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([4, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nlatents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n).to(device)\n\nlatents.shape, latents\n\n(torch.Size([2, 4, 64, 64]),\n tensor([[[[ 0.1884, -0.6394,  0.1089,  ..., -0.9887, -0.7133, -1.1545],\n           [ 0.4124,  1.5587, -0.3407,  ...,  2.1968, -0.0356, -0.0810],\n           [-1.8912,  0.0528, -0.4425,  ...,  1.3110,  0.7100,  0.6802],\n           ...,\n           [-1.3443, -0.1747, -0.6298,  ...,  0.4572, -0.8584, -0.1284],\n           [-1.7920, -0.6554, -0.0439,  ...,  0.5436,  2.2266, -0.5003],\n           [ 0.6213, -1.3155,  0.7470,  ..., -0.2354,  0.7097,  0.6170]],\n \n          [[-0.5007, -1.4418,  0.2598,  ..., -0.2586,  2.3239, -1.3245],\n           [ 0.8540, -0.4135,  0.5658,  ..., -1.9556,  2.0454, -0.2454],\n           [-0.3212, -1.9329, -1.1598,  ...,  0.7156, -0.7228, -0.6992],\n           ...,\n           [ 0.0180, -0.7993,  2.3330,  ...,  0.2594, -0.0333, -0.0826],\n           [-1.2569, -0.8219,  1.3467,  ...,  0.4792,  1.8265, -0.6156],\n           [-1.9367, -0.0949,  0.0720,  ...,  0.0806,  0.2966, -1.0284]],\n \n          [[ 0.2291, -0.0936, -1.3283,  ...,  1.4995, -0.1965, -0.2879],\n           [-1.0226, -1.2896,  1.6202,  ..., -0.3910, -0.3834,  0.5519],\n           [ 0.5424,  0.2685,  0.4912,  ...,  0.9773, -0.8260,  1.1552],\n           ...,\n           [-1.5280, -0.2530, -1.3748,  ..., -1.4948,  1.3661, -1.1294],\n           [ 0.4241, -0.2996,  1.8231,  ...,  0.6968,  0.8247, -0.0279],\n           [-3.3711, -0.7468, -1.3212,  ..., -0.4128,  0.4621,  2.6297]],\n \n          [[-0.7510, -0.7452, -0.8998,  ..., -1.6957, -0.4004, -0.2596],\n           [-1.2092, -1.8881, -0.5828,  ..., -1.0428, -0.6500,  0.3601],\n           [-0.4254,  0.9478,  1.3083,  ..., -0.0259, -0.4542,  0.4353],\n           ...,\n           [-0.1918,  0.4858,  0.0666,  ...,  0.8505, -0.6606, -0.3193],\n           [ 1.3620,  0.2283,  0.6292,  ..., -0.9271,  1.7018,  0.2161],\n           [-0.3891, -1.8911, -0.7501,  ..., -0.2330, -1.0460,  0.4121]]],\n \n \n         [[[ 0.3649, -1.3183, -1.3308,  ..., -0.5548, -1.3610, -1.9329],\n           [-0.0071,  0.1977,  1.5517,  ..., -1.6664,  1.6551,  0.1798],\n           [-1.0404,  0.6524,  0.4654,  ..., -0.5947, -1.0871,  2.2230],\n           ...,\n           [-0.6844,  0.1692, -0.2559,  ...,  0.5511,  0.9734,  0.7936],\n           [-1.1951,  0.5016,  0.8089,  ...,  0.2337, -0.2213, -1.1724],\n           [-0.5055, -0.7491, -1.4940,  ..., -2.1332,  0.9120,  0.2057]],\n \n          [[ 1.3668, -1.1680, -0.8574,  ..., -0.0635, -1.9132, -0.6023],\n           [ 1.0974, -0.9654,  1.2987,  ...,  1.3187, -0.0241, -0.5427],\n           [-2.0427, -1.4358, -0.7115,  ...,  0.1088,  0.0764,  0.7254],\n           ...,\n           [ 1.0957,  1.4058, -0.0178,  ...,  0.5748,  0.0953,  0.7550],\n           [ 0.4080,  0.8792,  0.6801,  ..., -0.7215,  1.1261,  0.0551],\n           [-0.3183, -2.3306,  0.7155,  ...,  0.4291, -0.2074, -1.1237]],\n \n          [[-0.2401,  0.9229,  0.0212,  ...,  0.2128, -0.4705, -0.3262],\n           [ 0.1108,  0.8909,  0.5309,  ..., -1.7175, -1.6657, -1.7706],\n           [-0.1654, -0.4582, -1.2832,  ...,  0.5297, -0.8363,  1.0293],\n           ...,\n           [-1.3526,  2.1482,  0.5417,  ..., -2.2156, -1.9940, -0.9745],\n           [-0.5821,  0.0492,  0.6693,  ..., -0.8610,  0.5864, -0.6040],\n           [ 1.0180,  1.4447,  0.9563,  ...,  0.9034,  0.7988, -1.7119]],\n \n          [[-1.6146,  0.0868,  0.6415,  ...,  0.2083,  0.4058,  0.2813],\n           [ 0.1969, -0.3334, -0.6526,  ..., -1.4639, -1.6302, -0.6036],\n           [ 0.1556, -0.0859, -0.0230,  ..., -0.7900, -0.3481,  0.8767],\n           ...,\n           [ 0.6056,  0.8374, -0.3834,  ..., -0.6636, -0.4814,  0.8244],\n           [ 0.6982, -0.4884, -1.3777,  ...,  0.5876, -2.0944,  0.0853],\n           [ 0.0388, -0.5761, -0.5116,  ..., -1.6645,  0.1752, -0.1923]]]],\n        device='cuda:0'))\n\n\n\nscheduler.set_timesteps(num_inference_steps)\n\nscheduler.timesteps.shape, pipe.scheduler.timesteps\n\n(torch.Size([50]),\n tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n         856.2857, 835.8980, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n         713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n         570.8571, 550.4694, 530.0816, 509.6939, 489.3061, 468.9184, 448.5306,\n         428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n         285.4286, 265.0408, 244.6531, 224.2653, 203.8776, 183.4898, 163.1020,\n         142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n           0.0000], dtype=torch.float64))\n\n\n\n\n\n\n\\tilde{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{z}_t, \\mathbf{c}) = w\\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) + (1-w)\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t).\n\nHere, \\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) and \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t) are conditional and unconditional \\boldsymbol{\\epsilon}-predictions, given by \\boldsymbol{\\epsilon}_\\theta := (\\mathbf{z}_t - \\alpha_t\\hat{\\mathbf{x}}_\\theta)/\\sigma_t, and w is the guidance weight. Setting w = 1 disables classifier-free guidance, while increasing w &gt; 1 strengthens the effect of guidance.1\n\nfrom tqdm.auto import tqdm\n\nlatents = latents * scheduler.init_noise_sigma\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(\n            latent_model_input, t, encoder_hidden_states=text_embeddings\n        ).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = (\n        guidance_scale * noise_pred_text + (1 - guidance_scale) * noise_pred_uncond\n    )\n\n    # compute the previous noisy sample x_t -&gt; x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n\nfrom PIL import Image\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\n\n\nfor pil_image in pil_images:\n    display(pil_image)\n\n\n\n\n\n\n\n\n\n\n\n\nPatil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#install-and-import-libraries",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#install-and-import-libraries",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#low-level",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#low-level",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "# 1. Load the autoencoder model which will be used to decode the latents into image space.\nvae = pipe.vae\n\n# 2. Load the tokenizer and text encoder to tokenize and encode the text.\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\n\n# 3. The UNet model for generating the latents.\nunet = pipe.unet\n\n\n\n\nvae\n\nAutoencoderKL(\n  (encoder): Encoder(\n    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (down_blocks): ModuleList(\n      (0): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (1): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (2): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (3): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (decoder): Decoder(\n    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (up_blocks): ModuleList(\n      (0-1): 2 x UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0-2): 3 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (2): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (3): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\ntokenizer\n\nCLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"&lt;|startoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True)\n\n\n\ntext_encoder\n\nCLIPTextModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 768)\n      (position_embedding): Embedding(77, 768)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n\n\n\nunet\n\nUNet2DConditionModel(\n  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n\n\n\n\n\ntext_input = tokenizer(\n    prompt * batch_size,\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\ntext_input.shape, text_input\n\n(torch.Size([2, 77]),\n tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input)[0]\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nuncond_input = tokenizer(\n    [\"\"] * batch_size,\n    padding=\"max_length\",\n    max_length=text_input.shape[-1],\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nuncond_input.shape, uncond_input\n\n(torch.Size([2, 77]),\n tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407],\n         [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n          49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0'))\n\n\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input)[0]\n\nuncond_embeddings.shape, uncond_embeddings\n\n(torch.Size([2, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]]],\n        device='cuda:0'))\n\n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\ntext_embeddings.shape, text_embeddings\n\n(torch.Size([4, 77, 768]),\n tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n          [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n          ...,\n          [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n          [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n          [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]],\n \n         [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n          [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n          [ 0.4595,  0.5617,  1.6663,  ..., -1.9515, -1.2307,  0.0104],\n          ...,\n          [-3.0421, -0.0656, -0.1793,  ...,  0.3943, -0.0190,  0.7664],\n          [-3.0551, -0.1036, -0.1936,  ...,  0.4236, -0.0190,  0.7575],\n          [-2.9854, -0.0832, -0.1715,  ...,  0.4355,  0.0095,  0.7485]]],\n        device='cuda:0'))\n\n\n\nlatents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n).to(device)\n\nlatents.shape, latents\n\n(torch.Size([2, 4, 64, 64]),\n tensor([[[[ 0.1884, -0.6394,  0.1089,  ..., -0.9887, -0.7133, -1.1545],\n           [ 0.4124,  1.5587, -0.3407,  ...,  2.1968, -0.0356, -0.0810],\n           [-1.8912,  0.0528, -0.4425,  ...,  1.3110,  0.7100,  0.6802],\n           ...,\n           [-1.3443, -0.1747, -0.6298,  ...,  0.4572, -0.8584, -0.1284],\n           [-1.7920, -0.6554, -0.0439,  ...,  0.5436,  2.2266, -0.5003],\n           [ 0.6213, -1.3155,  0.7470,  ..., -0.2354,  0.7097,  0.6170]],\n \n          [[-0.5007, -1.4418,  0.2598,  ..., -0.2586,  2.3239, -1.3245],\n           [ 0.8540, -0.4135,  0.5658,  ..., -1.9556,  2.0454, -0.2454],\n           [-0.3212, -1.9329, -1.1598,  ...,  0.7156, -0.7228, -0.6992],\n           ...,\n           [ 0.0180, -0.7993,  2.3330,  ...,  0.2594, -0.0333, -0.0826],\n           [-1.2569, -0.8219,  1.3467,  ...,  0.4792,  1.8265, -0.6156],\n           [-1.9367, -0.0949,  0.0720,  ...,  0.0806,  0.2966, -1.0284]],\n \n          [[ 0.2291, -0.0936, -1.3283,  ...,  1.4995, -0.1965, -0.2879],\n           [-1.0226, -1.2896,  1.6202,  ..., -0.3910, -0.3834,  0.5519],\n           [ 0.5424,  0.2685,  0.4912,  ...,  0.9773, -0.8260,  1.1552],\n           ...,\n           [-1.5280, -0.2530, -1.3748,  ..., -1.4948,  1.3661, -1.1294],\n           [ 0.4241, -0.2996,  1.8231,  ...,  0.6968,  0.8247, -0.0279],\n           [-3.3711, -0.7468, -1.3212,  ..., -0.4128,  0.4621,  2.6297]],\n \n          [[-0.7510, -0.7452, -0.8998,  ..., -1.6957, -0.4004, -0.2596],\n           [-1.2092, -1.8881, -0.5828,  ..., -1.0428, -0.6500,  0.3601],\n           [-0.4254,  0.9478,  1.3083,  ..., -0.0259, -0.4542,  0.4353],\n           ...,\n           [-0.1918,  0.4858,  0.0666,  ...,  0.8505, -0.6606, -0.3193],\n           [ 1.3620,  0.2283,  0.6292,  ..., -0.9271,  1.7018,  0.2161],\n           [-0.3891, -1.8911, -0.7501,  ..., -0.2330, -1.0460,  0.4121]]],\n \n \n         [[[ 0.3649, -1.3183, -1.3308,  ..., -0.5548, -1.3610, -1.9329],\n           [-0.0071,  0.1977,  1.5517,  ..., -1.6664,  1.6551,  0.1798],\n           [-1.0404,  0.6524,  0.4654,  ..., -0.5947, -1.0871,  2.2230],\n           ...,\n           [-0.6844,  0.1692, -0.2559,  ...,  0.5511,  0.9734,  0.7936],\n           [-1.1951,  0.5016,  0.8089,  ...,  0.2337, -0.2213, -1.1724],\n           [-0.5055, -0.7491, -1.4940,  ..., -2.1332,  0.9120,  0.2057]],\n \n          [[ 1.3668, -1.1680, -0.8574,  ..., -0.0635, -1.9132, -0.6023],\n           [ 1.0974, -0.9654,  1.2987,  ...,  1.3187, -0.0241, -0.5427],\n           [-2.0427, -1.4358, -0.7115,  ...,  0.1088,  0.0764,  0.7254],\n           ...,\n           [ 1.0957,  1.4058, -0.0178,  ...,  0.5748,  0.0953,  0.7550],\n           [ 0.4080,  0.8792,  0.6801,  ..., -0.7215,  1.1261,  0.0551],\n           [-0.3183, -2.3306,  0.7155,  ...,  0.4291, -0.2074, -1.1237]],\n \n          [[-0.2401,  0.9229,  0.0212,  ...,  0.2128, -0.4705, -0.3262],\n           [ 0.1108,  0.8909,  0.5309,  ..., -1.7175, -1.6657, -1.7706],\n           [-0.1654, -0.4582, -1.2832,  ...,  0.5297, -0.8363,  1.0293],\n           ...,\n           [-1.3526,  2.1482,  0.5417,  ..., -2.2156, -1.9940, -0.9745],\n           [-0.5821,  0.0492,  0.6693,  ..., -0.8610,  0.5864, -0.6040],\n           [ 1.0180,  1.4447,  0.9563,  ...,  0.9034,  0.7988, -1.7119]],\n \n          [[-1.6146,  0.0868,  0.6415,  ...,  0.2083,  0.4058,  0.2813],\n           [ 0.1969, -0.3334, -0.6526,  ..., -1.4639, -1.6302, -0.6036],\n           [ 0.1556, -0.0859, -0.0230,  ..., -0.7900, -0.3481,  0.8767],\n           ...,\n           [ 0.6056,  0.8374, -0.3834,  ..., -0.6636, -0.4814,  0.8244],\n           [ 0.6982, -0.4884, -1.3777,  ...,  0.5876, -2.0944,  0.0853],\n           [ 0.0388, -0.5761, -0.5116,  ..., -1.6645,  0.1752, -0.1923]]]],\n        device='cuda:0'))\n\n\n\nscheduler.set_timesteps(num_inference_steps)\n\nscheduler.timesteps.shape, pipe.scheduler.timesteps\n\n(torch.Size([50]),\n tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n         856.2857, 835.8980, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n         713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n         570.8571, 550.4694, 530.0816, 509.6939, 489.3061, 468.9184, 448.5306,\n         428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n         285.4286, 265.0408, 244.6531, 224.2653, 203.8776, 183.4898, 163.1020,\n         142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n           0.0000], dtype=torch.float64))\n\n\n\n\n\n\n\\tilde{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{z}_t, \\mathbf{c}) = w\\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) + (1-w)\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t).\n\nHere, \\boldsymbol{\\epsilon}_\\theta(\\mathbf{z}_t, \\mathbf{c}) and \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t) are conditional and unconditional \\boldsymbol{\\epsilon}-predictions, given by \\boldsymbol{\\epsilon}_\\theta := (\\mathbf{z}_t - \\alpha_t\\hat{\\mathbf{x}}_\\theta)/\\sigma_t, and w is the guidance weight. Setting w = 1 disables classifier-free guidance, while increasing w &gt; 1 strengthens the effect of guidance.1\n\nfrom tqdm.auto import tqdm\n\nlatents = latents * scheduler.init_noise_sigma\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(\n            latent_model_input, t, encoder_hidden_states=text_embeddings\n        ).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = (\n        guidance_scale * noise_pred_text + (1 - guidance_scale) * noise_pred_uncond\n    )\n\n    # compute the previous noisy sample x_t -&gt; x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n\nfrom PIL import Image\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\n\n\nfor pil_image in pil_images:\n    display(pil_image)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#references",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#references",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "",
    "text": "Patil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#footnotes",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_low_level.html#footnotes",
    "title": "Stable Diffusion Inference with Diffusers (low-level)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSaharia et al. (2022) Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, https://arxiv.org/abs/2205.11487↩︎"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html",
    "href": "posts/charged-particle-motion-in-dipole/index.html",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "",
    "text": "This post is based on the homework report that was written during the solar terrestrial physics class in 2022.\n\n\n\n\\mathbf{B} = -\\frac{\\mu_0}{4\\pi}\\left( \\frac{\\mathbf{m}}{r^3} - \\frac{3(\\mathbf{m}\\cdot\\mathbf{r})\\mathbf{r}}{r^5} \\right)\n\nIn (r, \\lambda, \\phi) coordinates, we describe the magnetic field as follows:\n\nB_r = Z = -\\frac{\\mu_0 m}{2\\pi}\\frac{\\sin\\lambda}{r^3}\n\n\nB_\\lambda = H = \\frac{\\mu_0 m}{4\\pi}\\frac{\\cos\\lambda}{r^3}\n\n\nB_\\phi = 0\n\nTherefore, the magnetic field \\mathbf{B} does not depend on longitude \\phi.\n\n\\mathbf{B} = \\mathbf{B}(r, \\lambda)\n\nThe strength B is given by:\n\nB = \\frac{\\mu_0 m}{4\\pi r^3}(1+3\\sin^2\\lambda)^{\\textstyle \\frac{1}{2}}\n\nFor the Earth’s magnetic field at the equator, denoted as B_E, it can be expressed as:\n\nB_E = \\frac{\\mu_0 m}{4\\pi R_E^3}\n\nThe actual value for B_E is approximately 0.31 Gauss (G).\nThe components of the magnetic field can be re-express in terms of B_E as follows:\n\nB_r = -\\frac{2B_E}{(r/R_E)^3}\\sin\\lambda\n\n\nB_\\lambda = \\frac{B_E}{(r/R_E)^3}\\cos\\lambda\n\n\nB_\\phi = 0\n\nThe magnetic field line in the meridian (when \\phi=\\text{const} or in the (r,\\lambda)-plane) is given by:\n\nr = r_\\text{eq} \\cos^2\\lambda\n\nHere, r_\\text{eq} = LR_E and L is called L-parameter.\n\nL-parameter describes the set of magnetic field lines which cross the Earth’s magnetic equator at a number of Earth-radii equal to the L-parameter. For example, L=2 describes the set of the Earth’s magnetic field lines which cross the Earth’s magnetic equator two earth radii from the center of the Earth.1\n\n\nThe dipole model of the Earth’s magnetic field is a first order approximation of the rather complex true Earth’s magnetic field. Due to effects of the interplanetary magnetic field (IMF), and the solar wind, the dipole model is particularly inaccurate at high L-shells (e.g., above L=3), but may be a good approximation for lower L-shells. For more precise work, or for any work at higher L-shells, a more accurate model that incorporates solar effects, such as the Tsyganenko magnetic field model, is recommended.2\n\n\n\\begin{align*}\n\\mathbf{B}(\\mathbf{r}) & = B_r(r, \\lambda) \\hat{r} + B_\\lambda (r, \\lambda) \\hat{\\lambda} \\\\\n& = B_x(x, y)\\hat{x} + B_y(x, y)\\hat{y}\n\\end{align*}\n\n\n\n\n\n\\frac{d^2\\mathbf{r}}{dt^2} = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}\n\n\n\\frac{d^2x}{dt^2}\\hat{x} + \\frac{d^2y}{dt^2}\\hat{y} + \\frac{d^2z}{dt^2}\\hat{z} = \\frac{q}{m}(\\hat{x}(v_yB_z - v_zB_y) + \\hat{y}(v_zB_x - v_xB_z) + \\hat{z}(v_xB_y - v_yB_x))\n\n\n\\frac{d^2x}{dt^2} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{d^2y}{dt^2} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{d^2z}{dt^2} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\n\\frac{dx}{dt} = v_x\n\n\n\\frac{dy}{dt} = v_y\n\n\n\\frac{dz}{dt} = v_z\n\n\n\\frac{dv_x}{dt} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{dv_y}{dt} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{dv_z}{dt} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\nS = (x, y, z, v_x, v_y, v_z)\n\n\n\\frac{dS}{dt} = (\\frac{dx}{dt}, \\frac{dy}{dt}, \\frac{dz}{dt}, \\frac{dv_x}{dt}, \\frac{dv_y}{dt},\\frac{dv_z}{dt})"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#theory",
    "href": "posts/charged-particle-motion-in-dipole/index.html#theory",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "",
    "text": "This post is based on the homework report that was written during the solar terrestrial physics class in 2022.\n\n\n\n\\mathbf{B} = -\\frac{\\mu_0}{4\\pi}\\left( \\frac{\\mathbf{m}}{r^3} - \\frac{3(\\mathbf{m}\\cdot\\mathbf{r})\\mathbf{r}}{r^5} \\right)\n\nIn (r, \\lambda, \\phi) coordinates, we describe the magnetic field as follows:\n\nB_r = Z = -\\frac{\\mu_0 m}{2\\pi}\\frac{\\sin\\lambda}{r^3}\n\n\nB_\\lambda = H = \\frac{\\mu_0 m}{4\\pi}\\frac{\\cos\\lambda}{r^3}\n\n\nB_\\phi = 0\n\nTherefore, the magnetic field \\mathbf{B} does not depend on longitude \\phi.\n\n\\mathbf{B} = \\mathbf{B}(r, \\lambda)\n\nThe strength B is given by:\n\nB = \\frac{\\mu_0 m}{4\\pi r^3}(1+3\\sin^2\\lambda)^{\\textstyle \\frac{1}{2}}\n\nFor the Earth’s magnetic field at the equator, denoted as B_E, it can be expressed as:\n\nB_E = \\frac{\\mu_0 m}{4\\pi R_E^3}\n\nThe actual value for B_E is approximately 0.31 Gauss (G).\nThe components of the magnetic field can be re-express in terms of B_E as follows:\n\nB_r = -\\frac{2B_E}{(r/R_E)^3}\\sin\\lambda\n\n\nB_\\lambda = \\frac{B_E}{(r/R_E)^3}\\cos\\lambda\n\n\nB_\\phi = 0\n\nThe magnetic field line in the meridian (when \\phi=\\text{const} or in the (r,\\lambda)-plane) is given by:\n\nr = r_\\text{eq} \\cos^2\\lambda\n\nHere, r_\\text{eq} = LR_E and L is called L-parameter.\n\nL-parameter describes the set of magnetic field lines which cross the Earth’s magnetic equator at a number of Earth-radii equal to the L-parameter. For example, L=2 describes the set of the Earth’s magnetic field lines which cross the Earth’s magnetic equator two earth radii from the center of the Earth.1\n\n\nThe dipole model of the Earth’s magnetic field is a first order approximation of the rather complex true Earth’s magnetic field. Due to effects of the interplanetary magnetic field (IMF), and the solar wind, the dipole model is particularly inaccurate at high L-shells (e.g., above L=3), but may be a good approximation for lower L-shells. For more precise work, or for any work at higher L-shells, a more accurate model that incorporates solar effects, such as the Tsyganenko magnetic field model, is recommended.2\n\n\n\\begin{align*}\n\\mathbf{B}(\\mathbf{r}) & = B_r(r, \\lambda) \\hat{r} + B_\\lambda (r, \\lambda) \\hat{\\lambda} \\\\\n& = B_x(x, y)\\hat{x} + B_y(x, y)\\hat{y}\n\\end{align*}\n\n\n\n\n\n\\frac{d^2\\mathbf{r}}{dt^2} = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}\n\n\n\\frac{d^2x}{dt^2}\\hat{x} + \\frac{d^2y}{dt^2}\\hat{y} + \\frac{d^2z}{dt^2}\\hat{z} = \\frac{q}{m}(\\hat{x}(v_yB_z - v_zB_y) + \\hat{y}(v_zB_x - v_xB_z) + \\hat{z}(v_xB_y - v_yB_x))\n\n\n\\frac{d^2x}{dt^2} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{d^2y}{dt^2} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{d^2z}{dt^2} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\n\\frac{dx}{dt} = v_x\n\n\n\\frac{dy}{dt} = v_y\n\n\n\\frac{dz}{dt} = v_z\n\n\n\\frac{dv_x}{dt} = \\frac{q}{m}(v_yB_z - v_zB_y)\n\n\n\\frac{dv_y}{dt} = \\frac{q}{m}(v_zB_x - v_xB_z)\n\n\n\\frac{dv_z}{dt} = \\frac{q}{m}(v_xB_y - v_yB_x)\n\n\n\n\n\nS = (x, y, z, v_x, v_y, v_z)\n\n\n\\frac{dS}{dt} = (\\frac{dx}{dt}, \\frac{dy}{dt}, \\frac{dz}{dt}, \\frac{dv_x}{dt}, \\frac{dv_y}{dt},\\frac{dv_z}{dt})"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#code",
    "href": "posts/charged-particle-motion-in-dipole/index.html#code",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "Code",
    "text": "Code\n\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom matplotlib.animation import FuncAnimation\n\n\n# Be = 0.31 G = 0.31 * 10^-4 T\nBe = 0.31 * 1e-4\n# Re = 6371 km = 6371 * 10^3 m\nRe = 6371 * 1e3\nC = Be * (Re**3)\n\n\ndef B(x, y, z):\n    \"\"\"dipole field at (x, y, z)\"\"\"\n    r = np.sqrt(x**2 + y**2 + z**2)\n    Bx = -1 * C * (3 * x * z) / (r**5)\n    By = -1 * C * (3 * y * z) / (r**5)\n    Bz = C * (r**2 - 3 * z**2) / (r**5)\n    return Bx, By, Bz\n\n\ndef field_line_3D(phi, L=6.6):\n    \"\"\"dipole field line (3D)\"\"\"\n    phi = np.deg2rad(phi)\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    rf = L * np.sin(theta) ** 2\n    xf = rf * np.sin(theta) * np.cos(phi)\n    yf = rf * np.sin(theta) * np.sin(phi)\n    zf = rf * np.cos(theta)\n    return xf, yf, zf\n\n\ndef field_line_2D(L=6.6):\n    \"\"\"dipole field line (2D)\"\"\"\n    lamb = np.linspace(0, 2 * np.pi, 1000)\n    rf2 = L * np.cos(lamb) ** 2\n    xf2 = rf2 * np.cos(lamb)\n    zf2 = rf2 * np.sin(lamb)\n    return xf2, zf2\n\n\ndef dSdt(S, t, q_over_m):\n    \"\"\"dS/dt for odeint\"\"\"\n    x, y, z, vx, vy, vz = S\n    Bx, By, Bz = B(x, y, z)\n    dvxdt = q_over_m * (vy * Bz - vz * By)\n    dvydt = q_over_m * (vz * Bx - vx * Bz)\n    dvzdt = q_over_m * (vx * By - vy * Bx)\n    return [vx, vy, vz, dvxdt, dvydt, dvzdt]\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nxmin, xmax = -7, 7\nymin, ymax = -7, 7\n\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.legend()\nax.axhline(y=0, color=\"black\", linewidth=0.5)\nax.xaxis.grid(True, which=\"both\")\nax.set_xlim(xmin, xmax)\nax.set_ylim(ymin, ymax)\nmajors = np.arange(xmin + 1, xmax, 1)\nax.xaxis.set_major_locator(ticker.FixedLocator(majors))\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\n\nLvalues = [2, 4, 6, 8, 10, 20, 100]\ncolors = [\"r\", \"darkorange\", \"gold\", \"green\", \"blue\", \"magenta\", \"purple\"]\nlamb = np.linspace(0, 2 * np.pi, 1000)\nfor i, L in enumerate(Lvalues):\n    x, z = field_line_2D(L)\n    ax.plot(x, z, label=f\"L={L}\", color=colors[i])\n\nax.legend()\nax.set_title(\"Magnetic Dipole Field line (2D)\")\nplt.show()\n\n\n\n\n\nspecies = \"Proton\"\n\ne = 1.602e-19\nq = e  # C\n\nmH = 1.67e-27\nm = mH  # kg\nq_over_m = q / m\n\nE_keV = 2000  # keV\n\n\n# L-parameter\nL = 6.6\n\n# start at equator\nx0, y0, z0 = L * Re, 0, 0\n\nkeV_to_J = 1e3 * e  # J\n\n# particle energy (keV) and pitch angle\nE = E_keV * keV_to_J  # J\npitch_angle_deg = 30\nalpha = np.deg2rad(pitch_angle_deg)\n\n# particle velocity\nv0 = np.sqrt(2 * E / m)\n\n# vx = v_perp\n# vy = 0\n# vz = v_para\nvx0 = v0 * np.sin(alpha)\nvy0 = 0\nvz0 = v0 * np.cos(alpha)\n\n# bounce time_scale\nt_B = 290 * (np.pi * L / 10) * np.sqrt(m / (mH * E_keV))\nprint(f\"bounce time scale ~ {t_B} s\")\n\nS0 = [x0, y0, z0, vx0, vy0, vz0]\n\n# number of bounce\nn = 3\n\ntmin = 0\ntmax = n * t_B\n\nt = np.linspace(tmin, tmax, 1000)\n\n# solve ODE\nsol = odeint(dSdt, S0, t, args=(q_over_m,))\nx, y, z, vx, vy, vz = sol.T\nx, y, z = x / Re, y / Re, z / Re\n\nprint(f\"t_max ~ {tmax:.4f} s\")\n\nbounce time scale ~ 13.44549539521195 s\nt_max ~ 40.3365 s\n\n\n\ntrajectory_linewidth = 0.8\nfieldline_linewidth = 0.5\n\n\n# xyzrange = 10\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(projection=\"3d\")\nax.set_aspect(\"equal\")\n# ax.plot([-xyzrange,xyzrange], [0,0], [0, 0], color='black')\n# ax.plot([0,0], [-xyzrange,xyzrange], [0, 0], color='black')\n# ax.plot([0,0], [0,0], [-xyzrange, xyzrange], color='black')\nax.plot(\n    x,\n    y,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"y ($R_E$)\")\nax.set_zlabel(\"z ($R_E$)\")\nax.set_xlim(-L, L)\nax.set_ylim(-L, L)\nax.set_zlim(-L, L)\nfor az in np.arange(0, 361, 20):\n    xf, yf, zf = field_line_3D(az, L)\n    if az == 360:\n        ax.plot(\n            xf,\n            yf,\n            zf,\n            color=\"blue\",\n            linewidth=fieldline_linewidth,\n            zorder=-1,\n            label=f\"Magnetic Field (L={L})\",\n        )\n    else:\n        ax.plot(xf, yf, zf, color=\"blue\", linewidth=fieldline_linewidth, zorder=-1)\n\n# Sphere with radius Re\nu = np.linspace(0, 2 * np.pi, 1000)\nv = np.linspace(0, np.pi, 1000)\nxs = 1 * np.outer(np.cos(u), np.sin(v))\nys = 1 * np.outer(np.sin(u), np.sin(v))\nzs = 1 * np.outer(np.ones(np.size(u)), np.cos(v))\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.plot_surface(xs, ys, zs, color=\"white\", alpha=1)\nax.legend()\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.set_aspect(\"equal\")\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    y,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"y ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\ntheta = np.linspace(0, 2 * np.pi, 100)\nrc = L\nxc = rc * np.cos(theta)\nyc = rc * np.sin(theta)\nplt.plot(xc, yc, color=\"green\", zorder=-1, label=f\"Circle with radius L={L}\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend(loc=1)\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\n\nxf2, zf2 = field_line_2D(L)\nax.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend()\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot()\nax.add_patch(\n    plt.Circle(\n        (0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\", label=\"Earth\"\n    )\n)\nax.set_aspect(\"equal\")\nax.plot([-10, 10], [0, 0], color=\"black\")\nax.plot([0, 0], [-10, 10], color=\"black\")\nax.plot(\n    x,\n    z,\n    color=\"red\",\n    label=\"Particle Trajectory\",\n    linewidth=trajectory_linewidth,\n    zorder=200,\n)\nax.set_xlabel(\"x ($R_E$)\")\nax.set_ylabel(\"z ($R_E$)\")\nax.set_xlim(-L - 1, L + 1)\nax.set_ylim(-L - 1, L + 1)\n\nxf2, zf2 = field_line_2D(L)\nax.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\nax.set_title(\n    f\"{species}, E = {E_keV/1000:.0f} MeV, initial pitch angle = {pitch_angle_deg} deg\"\n)\nax.legend()\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(16, 8))\nax1 = plt.subplot(121)\nax2 = plt.subplot(122, projection=\"3d\")\n\nxdata = x\nydata = y\nzdata = z\n\nsc1 = ax1.scatter([], [], color=\"green\")\n(ln1,) = ax1.plot([], [], \"r-\", zorder=99)\nsc2 = ax2.scatter([], [], [], color=\"green\")\n(ln2,) = ax2.plot([], [], [], \"r-\", zorder=99)\n\n# Field line (2D)\nxf2, zf2 = field_line_2D(L)\nax1.plot(xf2, zf2, zorder=-1, color=\"blue\", label=f\"Magnetic field line (L={L})\")\n\n# Earth (2D)\nax1.add_patch(plt.Circle((0, 0), 1, zorder=99, facecolor=\"white\", edgecolor=\"black\"))\n\n# Field line (3D)\nfor az in np.arange(0, 361, 20):\n    xf, yf, zf = field_line_3D(az, L)\n    if az == 360:\n        ax2.plot(\n            xf,\n            yf,\n            zf,\n            color=\"blue\",\n            linewidth=fieldline_linewidth,\n            zorder=-1,\n            label=f\"Magnetic Field (L={L})\",\n        )\n    else:\n        ax2.plot(xf, yf, zf, color=\"blue\", linewidth=fieldline_linewidth, zorder=-1)\n\n# Earth (3D)\nu = np.linspace(0, 2 * np.pi, 1000)\nv = np.linspace(0, np.pi, 1000)\nxs = 1 * np.outer(np.cos(u), np.sin(v))\nys = 1 * np.outer(np.sin(u), np.sin(v))\nzs = 1 * np.outer(np.ones(np.size(u)), np.cos(v))\nax2.plot_surface(xs, ys, zs, color=\"white\", alpha=1)\n\n\ndef init():\n    ax1.set_aspect(\"equal\")\n    ax1.plot([-10, 10], [0, 0], color=\"black\")\n    ax1.plot([0, 0], [-10, 10], color=\"black\")\n    ax1.set_xlabel(\"x ($R_E$)\")\n    ax1.set_ylabel(\"z ($R_E$)\")\n    ax1.set_xlim(-L - 1, L + 1)\n    ax1.set_ylim(-L - 1, L + 1)\n\n    ax2.set_aspect(\"equal\")\n    ax2.plot([-10, 10], [0, 0], [0, 0], color=\"black\")\n    ax2.plot([0, 0], [-10, 10], [0, 0], color=\"black\")\n    ax2.plot([0, 0], [0, 0], [-10, 10], color=\"black\")\n    ax2.set_xlabel(\"x ($R_E$)\")\n    ax2.set_ylabel(\"y ($R_E$)\")\n    ax2.set_zlabel(\"z ($R_E$)\")\n    ax2.set_xlim(-L, L)\n    ax2.set_ylim(-L, L)\n    ax2.set_zlim(-L, L)\n\n    return ln1, ln2\n\n\ndef update(frame):\n    sc1.set_offsets([xdata[frame - 1], zdata[frame - 1]])\n    ln1.set_data(xdata[:frame], zdata[:frame])\n    ln1.set_label(f\"t={frame}\")\n    ax1.legend()\n\n    sc2._offsets3d = ([xdata[frame - 1]], [ydata[frame - 1]], [zdata[frame - 1]])\n    ln2.set_data(xdata[:frame], ydata[:frame])\n    ln2.set_3d_properties(zdata[:frame])\n    ln2.set_label(f\"t={frame}\")\n    ax2.legend()\n    return ln1, ln2\n\n\nani = FuncAnimation(\n    fig, update, frames=np.arange(1, len(xdata)), init_func=init, blit=True\n)\n\nani.save(\"simulation.gif\", fps=30)\n\n\n\nani.save(\"simulation.mp4\", fps=30)"
  },
  {
    "objectID": "posts/charged-particle-motion-in-dipole/index.html#footnotes",
    "href": "posts/charged-particle-motion-in-dipole/index.html#footnotes",
    "title": "Motion of Charged Particles in Magnetic Dipole Fields",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/L-shell↩︎\nhttps://en.wikipedia.org/wiki/Dipole_model_of_the_Earth%27s_magnetic_field↩︎"
  },
  {
    "objectID": "posts/spherical-astronomy/index.html",
    "href": "posts/spherical-astronomy/index.html",
    "title": "Spherical Astronomy",
    "section": "",
    "text": "This post is based on the project report that was written during the astronomical observation class in 2019."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#introduction",
    "href": "posts/spherical-astronomy/index.html#introduction",
    "title": "Spherical Astronomy",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is the purpose of this post?\nThe purpose of this post is to help you grasp the concept of the celestial sphere and the motions of celestial objects by utilizing the celestial sphere model. In detail, you will learn and understand that diurnal motion is the result of Earth’s rotation, and annual motion is the result of Earth’s revolution. Furthermore, you will gain insights into solar time, sidereal time, the celestial coordinate system (horizontal and equatorial), and coordinate conversion.\n\n\nWhy do you learn this?\nAll astronomical studies are based on the results of astronomical observations. The majority of these observations are conducted on Earth, with some exceptions. However, Earth is not an inertial frame of reference because it both rotates on its axis and orbits the Sun. Consequently, when we observe celestial objects from Earth, we inevitably observe apparent motion due to Earth’s own movements. Therefore, it is essential to identify the apparent motion in the motion of celestial objects. Additionally, all celestial objects are projected onto the celestial sphere, and as a result, their movements occur on the surface of this sphere. For these reasons, learning spherical astronomy is crucial for astronomers."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#celestial-sphere-model",
    "href": "posts/spherical-astronomy/index.html#celestial-sphere-model",
    "title": "Spherical Astronomy",
    "section": "Celestial Sphere Model",
    "text": "Celestial Sphere Model\n\nCelestial Sphere\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Celestial Sphere is an imaginary sphere with an infinite radius centered at the Earth’s center. While most motions of celestial objects are a result of Earth’s motion, for the sake of convenience, we assume that the Earth is fixed, and all celetial objects are embedded on the surface of the Celetial Sphere. We attribute the motions of celestial objects to the motion of the Celestial Sphere.\nSince the Celestial Sphere is a sphere defined by a set of points equidistant from its center, we designate certain special points or circles for convenience. There are various methods for naming these points and circles, but we typically use Earth’s Rotation, Earth’s Revolution, and Observer. For each of them, we begin by selecting a reference axis and a reference plane that is perpendicular to the axis. We then extend the axis and plane infinitely, resulting in three intersections (two points and one circle), each of which is given a specific name. If additional points or circles on the Celestial Sphere are required, we can refer to 9 intersections (6 points and 3 circles) for reference.\n\nEarth’s Rotation\n\nReference axis: Earth’s rotation axis\nReference plane: The plane containing Earth’s equator\n\n\nTwo points on the Celestial Sphere\n\nCelestial North Pole: The point near Earth’s North Pole\nCelestial South Pole: The point near Earth’s South Pole\n\n\n\nOne circle on the Celestial Sphere\n\nCelestial Equator: The great circle determined by the reference plane of Earth’s Rotation\n\n\n\n\nEarth’s Revolution\n\nReference axis: The axis parallel to Earth’s revolution axis and passing through Earth’s center\nReference plane: The plane containing Earth’s orbital path around the Sun\n\n\nTwo points on the Celestial Sphere\n\nEcliptic North Pole: The point near Earth’s North Pole\nEcliptic South Pole: The point near Earth’s South Pole\n\n\n\nOne circle on the Celestial Sphere\n\nEcliptic: The great circle determined by the reference plane of Earth’s Revolution\n\n\n\n\nObserver\n\nReference axis: The line perpendicular to the reference plane at the observer’s location\nReference plane: The plane parallel to the tangent plane of the Earth at the observer’s location and passing though Earth’s center\n\n\nTwo points on the Celestial Sphere\n\nZenith: The point near observer’s head\nNadir: The point near observer’s feet\n\n\n\nOne circle on the Celestial Sphere\n\nHorizon: The great circle determined by the reference plane of observer\n\n\n\n\nAdditional Points and Circles\n\nCircle on the Celestial Sphere\n\nMeridian: The great circle passing though the Celestial North Pole, the Celestial South Pole, the Zenith, and the Nadir\n\n\n\n4 points on the Ecliptic\n\nVernal Equinox : The point of intersection of the Ecliptic and the Celestial Equator, where the Sun moves from the celestial southern hemisphere to the celestial northern hemisphere\nSummer Solstice​ : The point on the Ecliptic nearest to the Celestial North Pole\nAutumnal Equinox : The point of intersection of the Ecliptic and the Celestial Equator, where the Sun moves from the celestial northern hemisphere to the celestial southern hemisphere\nWinter Solstice​ : The point on the Ecliptic nearest to the Celestial South Pole\n\n\n\n4 points on the Horizon\n\nNorth point​ : The point of intersection of the Horizon and the Meridian near the Celestial North Pole\nEast point : The point on the Horizon located 90 degree clockwise from the North point\nSouth point​ : The point of intersection of the Horizon and the Meridian near the Celestial South Pole\nWest point : The point on the Horizon located 90 degree clockwise from the South point\n\n\n\n\nAltitude of Celestial Poles\n\n\n\n\n\n\n\n(a) Northern hemisphere observer\n\n\n\n\n\n\n\n(b) Southern hemisphere observer\n\n\n\n\nFigure 1: Schematic illustrations explaining the altitude of celestial poles\n\n\nFigure 1 (a) represents a situation in which the observer is located in the northern hemisphere, while Figure 1 (b) depicts a case in the southern hemisphere. In these diagrams, the blue line represents the horizon, the red line signifies the celestial equator, the black line is perpendicular to the horizon, the green line is perpendicular to the celestial equator, the orange point represents the observer, the green point represents Polaris, and \\phi denotes the observer’s latitude. From these figures, we can conclude that the altitude of celestial poles is equal to the observer’s latitude.\n\n\n\nEarth’s Rotation\nFirst, let’s review the following concept for convenience.\n\n\n\n\n\n\n\n(a) Left-hand screw rule\n\n\n\n\n\n\n\n(b) Right-hand screw rule\n\n\n\n\nFigure 2: Illustrations explaining the rotation direction\n\n\nFigure 2 (a) illustrates the left-hand screw rule, while Figure 2 (b) illustrates the right-hand screw rule. Imagine you’re directly looking at your thumbs and curling your other fingers. If you focus on your left thumb, the rotation direction is clockwise. Conversely, if you focus on your right thumb, the rotation direction is counterclockwise. To determine the rotation direction, simply observe your thumbs, curl your other fingers, and match the finger’s rotation direction with the given rotation direction: clockwise for the left hand and counterclockwise for the right hand.\n\n\n\n\n\nIn 3D space, the direction of a rotation vector is uniqe, but the rotation direction is not. Therefore, when describing a rotation, it is necessary to specify the viewpoint or indicate the direction of the rotation vector.\n\n\n\n\n\nIf your right thumb points in the direction of (Celestial) North Pole, then the Earth’s rotation follows right-hand screw rule. In other words, if you were to observe Earth’s North Pole from space, you would see that it rotates counterclockwise. Therefore, the direction of the Earth’s rotation vector is from South Pole to North Pole. Due to relative motion, the celestial sphere appears to rotate in the opposite direction around the Earth.\nWhen you rotate the celestial sphere model, you can observe that the Sun and all celestial objects rise in the east and set in the west in all hemispheres. Furthermore, you’ll notice that at the Sun’s upper culmination (its highest point), it appears in the southern sky in the northern hemisphere and in the northern sky in the southern hemisphere. At the poles, the Sun either remains either continuously visible or permanently hidden.\n\n\nSidereal Time\n\n\n\n\n\nThe (Local) Hour Angle is the inversely measured right ascension from the intersection of the celestial equator and the meridian above horizon. When the hour angle of a celestial object is zero, we call that the celestial object is at its upper culmination.\nThe (Local) Sidereal Time is the hour angle of the vernal equinox. By definition, sidereal time \\Theta of a celestial object is equal to H+\\alpha, where H is hour angle, and \\alpha is right ascension of the celestial object.\n\n\\begin{align*}\n\\text{Sidereal time} = \\Theta & = H_{\\text{vernal equinox}} \\\\\n& = H + \\alpha \\\\\n& = \\alpha_{\\text{celestial object at its upper culmination}}\n\\end{align*}\n\nThe (Local) Solar Time is calculated by adding the hour angle of the Sun to 12 hours.\n\n\\text{Solar time} = S = H_{\\odot} + \\text{12 h}\n\nTherefore, we get the following equation.\n\n\\begin{align*}\n\\text{Sidereal time} = \\Theta & = H_{\\odot} + \\alpha_{\\odot} \\\\\n& = S - \\text{12 h} + \\alpha_{\\odot}\n\\end{align*}\n\nThe right ascension of the Sun \\alpha_{\\odot} increases about +\\text{4 min} per day.\n\n\n\nTime\n\\alpha_{\\odot}\n\n\n\n\nVernal Equinox\n\\text{0 h}\n\n\nSummer Solstice\n\\text{6 h}\n\n\nAutumnal Equinox\n\\text{12 h}\n\n\nWinter Solstice\n\\text{18 h}\n\n\n\nThis solar time is actually local solar time. In Korea, the local solar time of Seoul is that of 127° E but our clock uses the local solar time of 135° E (UTC+9). As a result, there is a difference of about 8° (equivalent to 32 minutes). Therefore, in Seoul, the Sun is at its upper culmination at approximately 12:32 KST."
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#spherical-coordinate-system",
    "href": "posts/spherical-astronomy/index.html#spherical-coordinate-system",
    "title": "Spherical Astronomy",
    "section": "Spherical Coordinate System",
    "text": "Spherical Coordinate System\n\nHorizontal System vs. Equatorial System\n\nDefinition of (Az, Alt) in Horizontal System\nAzimuth (Az, A) of a celestial object is the angle (or angular distance) measured commonly clockwise from the south point along the horizon. Azimuth values are typically within the range of [0°, 360°].\nAltitude (Alt, a) of a celestial object is the angle (or angular distance) measured from the horizon along the great circle passing through the celestial object and the zenith. Altitude values fall within the range [-90°, +90°]. A positive altitude indicates that the object is above the horizon, while a negative altitude indicates that the object is below the horizon.\n\n\nDefinition of (RA, Dec) in Equatorial System\nRight Ascension (RA, \\alpha) is a celestial longitude, equivalent to Earth’s longtiude, except that it is measured from the vernal equinox instead of Greenwich. It is measured only in the eartward direction.\nDeclination (Dec, \\delta) is a celestial latitude, equivalent to Earth’s latitude, except that N and S are respectively replaced by + and -.\n\n\nA formula to convert (RA, Dec) to (Az, Alt)\n\n\n\n\n\nIn the figure above, the point ​P has coordinates (x, y, z) in xyz Cartesian coordinates system, and (x', y', z') in x'y'z' Cartesian coordinates system. Each of these coordinates has the following relations with spherical coordinates in its respective frame.\n\n\\begin{align*}\nx & = \\cos \\theta \\cos \\psi \\\\\ny & = \\cos \\theta \\sin \\psi \\\\\nz & = \\sin \\theta\n\\end{align*}\n\n\n\\begin{align*}\nx' & = \\cos \\theta' \\cos \\psi' \\\\\ny' & = \\cos \\theta' \\sin \\psi' \\\\\nz' & = \\sin \\theta'\n\\end{align*}\n\nThe primed coordinates are related to the unprimed coordinates by the following equations.\n\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos \\chi & -\\sin \\chi \\\\\n0 & \\sin \\chi & \\cos \\chi\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n\n\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos \\chi & \\sin \\chi \\\\\n0 & -\\sin \\chi & \\cos \\chi\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n\n\n\n\n\n\nIn the figure above, we have \\chi = 90^{\\circ} - \\psi. The values of (x, y, z) and (x', y', z') are determined by the following equations.\n\n\\begin{align*}\nx & = \\cos a \\cos(90^{\\circ} - A) \\\\\n& = \\cos a \\sin A \\\\\ny & = \\cos a \\sin(90^{\\circ} - A) \\\\\n& = \\cos a \\cos A \\\\\nz & = \\sin a\n\\end{align*}\n\n\n\\begin{align*}\nx' & = \\cos \\delta \\cos(90^{\\circ} - H) \\\\\n& = \\cos \\delta \\sin H \\\\\ny' & = \\cos \\delta \\sin(90^{\\circ} - H) \\\\\n& = \\cos \\delta \\cos H \\\\\nz' & = \\sin \\delta\n\\end{align*}\n\nThus, we get the following equations.\n\n\\begin{align*}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos (90^{\\circ} - \\phi) & -\\sin (90^{\\circ} - \\phi) \\\\\n0 & \\sin (90^{\\circ} - \\phi) & \\cos (90^{\\circ} - \\phi)\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\sin \\phi & -\\cos \\phi \\\\\n0 & \\cos \\phi & \\sin \\phi\n\\end{bmatrix}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n\\end{align*}\n\n\n\\begin{align*}\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\nz'\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos (90^{\\circ} - \\phi) & \\sin (90^{\\circ} - \\phi) \\\\\n0 & -\\sin (90^{\\circ} - \\phi) & \\cos (90^{\\circ} - \\phi)\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\sin \\phi & \\cos \\phi \\\\\n0 & -\\cos \\phi & \\sin \\phi\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\n\\end{bmatrix}\n\\end{align*}\n\nIn conclusion, we get the following two conversion fomulas.\nWhen we know (\\alpha=\\Theta-H, \\delta, \\phi), we can obtain (A, a) by using following equations.\n\n\\begin{align*}\n\\cos a \\sin A & = \\cos \\delta \\sin H \\\\\n\\cos a \\cos A & = \\sin \\phi \\cos \\delta \\cos H - \\cos \\phi \\sin \\delta \\\\\n\\sin a & = \\cos \\phi \\cos \\delta \\cos H + \\sin \\phi \\sin \\delta\n\\end{align*}\n\nWhen we know (A, a, \\phi), we can obtain (\\alpha=\\Theta-H, \\delta) by using following equations.\n\n\\begin{align*}\n\\cos \\delta \\sin H & = \\cos a \\sin A \\\\\n\\cos \\delta \\cos H & = \\sin \\phi \\cos a \\cos A + \\cos \\phi \\sin a \\\\\n\\sin \\delta & = - \\cos \\phi \\cos a \\cos A + \\sin \\phi \\sin a\n\\end{align*}\n\nTherefore, we can conclude that we need to know sidereal time and latitude when performing conversions between (RA, Dec) and (Az, Alt).\n\n\n\nTargets on Meridian\nLet \\alpha' be the right ascension of celestial object at its upper culmination. By definition of sidereal time, the following equation holds.\n\n\\begin{align*}\n\\Theta & = S - \\text{12 h} + \\alpha_\\odot \\\\\n& = \\alpha'\n\\end{align*}\n\nThus we get the following equation.\n\nS = \\text{12 h} - \\alpha_\\odot + \\alpha'\n\n\n\nHour Angle of Targets\nBy definition of sidereal time, the following equation holds.\n\n\\begin{align*}\n\\Theta & = S - \\text{12 h} + \\alpha_\\odot \\\\\n& = H + \\alpha\n\\end{align*}\n\nThus we get the following equation.\n\nH = S - \\text{12 h} + \\alpha_\\odot - \\alpha"
  },
  {
    "objectID": "posts/spherical-astronomy/index.html#references",
    "href": "posts/spherical-astronomy/index.html#references",
    "title": "Spherical Astronomy",
    "section": "References",
    "text": "References\n\nIntroductory Astronomy: The Celestial Sphere, http://astro.wsu.edu/worthey/astro/html/lec-celestial-sph.html\nMeridian (astronomy), https://en.wikipedia.org/wiki/Meridian_(astronomy)\nRight hand screw rule, https://www3.eng.cam.ac.uk/~hemh1/gyroscopes/screwrule.html\nRotation Vs. Revolution: What Are The Differences?, https://differencecamp.com/rotation-vs-revolution\nSUPPLEMENT: MOTIONS IN THE SKY & COORDINATE SYSTEMS, https://rwoconne.github.io/rwoclass/astr1230/motions-coords.html\nHannu Karttunen et al. ​Fundamental Astronomy​. Sixth Edition. Springer (2016)"
  },
  {
    "objectID": "posts/1d-scalar-function/index.html",
    "href": "posts/1d-scalar-function/index.html",
    "title": "How to draw 1D scalar functions in Python",
    "section": "",
    "text": "If you know the mathematical formula of a 1D scalar function y=f(x), then I believe the best tool for drawing 1D scalar functions is Desmos. However, you can also plot the functions in Python using various visualization libraries. In this post, I will draw 1D scalar functions y=x^2 and y=\\sin(x) using basic features of these libraries. Keep in mind that there are many advanced features not covered here, so for more information, refer to the official document of the respective library."
  },
  {
    "objectID": "posts/1d-scalar-function/index.html#sympy",
    "href": "posts/1d-scalar-function/index.html#sympy",
    "title": "How to draw 1D scalar functions in Python",
    "section": "SymPy",
    "text": "SymPy\n\nSymPy is a Python library for symbolic mathematics.\n\nEven though SymPy’s strength lies in symbolic computations, it can also be used for drawing 1D scalar functions, see Figure 1.\n\nfrom sympy import symbols, sin\nfrom sympy.plotting import plot \n\nx = symbols('x')\n\np1 = plot(x**2, (x, -2, 2), legend=True, show=False)\np2 = plot(sin(x), (x, -5, 5), legend=True, show=False)\np1.extend(p2)\np1.show()\n\n\n\n\nFigure 1: A plot using sympy\n\n\n\n\nThe graph depicts y=x^2 for x \\in [-2, 2] and y=\\sin(x) for x \\in [-5, 5]. The purpose of using different ranges for x is to display both graphs in a single figure without one being much smaller than the other. This is because the values of x^2 rapidly increases as |x| increases, while |\\sin(x)| \\leq 1 always."
  },
  {
    "objectID": "posts/1d-scalar-function/index.html#data-generation-using-numpy",
    "href": "posts/1d-scalar-function/index.html#data-generation-using-numpy",
    "title": "How to draw 1D scalar functions in Python",
    "section": "Data generation using NumPy",
    "text": "Data generation using NumPy\n\nNumPy is the fundamental package for scientific computing with Python.\n\nOther visualization libraries usually don’t understand symbolic representation of a function. They just draw (x, y) points in a coordinate plane. Therefore, before you use them, you have to generate (x, y) points using NumPy.\n\nimport numpy as np\n\nx1 = np.linspace(-2, 2, 100)\ny1 = x1**2\n\nx2 = np.linspace(-5, 5, 100)\ny2 = np.sin(x2)\n\nnp.linspace(start, stop, num) creates num evenly spaced numbers within a closed interval [start, stop]. So, x1 is an array containing 100 evenly spaced numbers within the interval [-2, 2], and x2 is the same array but within the interval [-5, 5].\n\nx1\n\narray([-2.        , -1.95959596, -1.91919192, -1.87878788, -1.83838384,\n       -1.7979798 , -1.75757576, -1.71717172, -1.67676768, -1.63636364,\n       -1.5959596 , -1.55555556, -1.51515152, -1.47474747, -1.43434343,\n       -1.39393939, -1.35353535, -1.31313131, -1.27272727, -1.23232323,\n       -1.19191919, -1.15151515, -1.11111111, -1.07070707, -1.03030303,\n       -0.98989899, -0.94949495, -0.90909091, -0.86868687, -0.82828283,\n       -0.78787879, -0.74747475, -0.70707071, -0.66666667, -0.62626263,\n       -0.58585859, -0.54545455, -0.50505051, -0.46464646, -0.42424242,\n       -0.38383838, -0.34343434, -0.3030303 , -0.26262626, -0.22222222,\n       -0.18181818, -0.14141414, -0.1010101 , -0.06060606, -0.02020202,\n        0.02020202,  0.06060606,  0.1010101 ,  0.14141414,  0.18181818,\n        0.22222222,  0.26262626,  0.3030303 ,  0.34343434,  0.38383838,\n        0.42424242,  0.46464646,  0.50505051,  0.54545455,  0.58585859,\n        0.62626263,  0.66666667,  0.70707071,  0.74747475,  0.78787879,\n        0.82828283,  0.86868687,  0.90909091,  0.94949495,  0.98989899,\n        1.03030303,  1.07070707,  1.11111111,  1.15151515,  1.19191919,\n        1.23232323,  1.27272727,  1.31313131,  1.35353535,  1.39393939,\n        1.43434343,  1.47474747,  1.51515152,  1.55555556,  1.5959596 ,\n        1.63636364,  1.67676768,  1.71717172,  1.75757576,  1.7979798 ,\n        1.83838384,  1.87878788,  1.91919192,  1.95959596,  2.        ])\n\n\n\\Delta x for this array is (2 - (-2)) / (100 - 1) = 0.\\overline{04}\n\nnp.isclose(np.diff(x1)[0], (2 - (-2)) / (100 - 1))\n\nTrue\n\n\n\nx2\n\narray([-5.        , -4.8989899 , -4.7979798 , -4.6969697 , -4.5959596 ,\n       -4.49494949, -4.39393939, -4.29292929, -4.19191919, -4.09090909,\n       -3.98989899, -3.88888889, -3.78787879, -3.68686869, -3.58585859,\n       -3.48484848, -3.38383838, -3.28282828, -3.18181818, -3.08080808,\n       -2.97979798, -2.87878788, -2.77777778, -2.67676768, -2.57575758,\n       -2.47474747, -2.37373737, -2.27272727, -2.17171717, -2.07070707,\n       -1.96969697, -1.86868687, -1.76767677, -1.66666667, -1.56565657,\n       -1.46464646, -1.36363636, -1.26262626, -1.16161616, -1.06060606,\n       -0.95959596, -0.85858586, -0.75757576, -0.65656566, -0.55555556,\n       -0.45454545, -0.35353535, -0.25252525, -0.15151515, -0.05050505,\n        0.05050505,  0.15151515,  0.25252525,  0.35353535,  0.45454545,\n        0.55555556,  0.65656566,  0.75757576,  0.85858586,  0.95959596,\n        1.06060606,  1.16161616,  1.26262626,  1.36363636,  1.46464646,\n        1.56565657,  1.66666667,  1.76767677,  1.86868687,  1.96969697,\n        2.07070707,  2.17171717,  2.27272727,  2.37373737,  2.47474747,\n        2.57575758,  2.67676768,  2.77777778,  2.87878788,  2.97979798,\n        3.08080808,  3.18181818,  3.28282828,  3.38383838,  3.48484848,\n        3.58585859,  3.68686869,  3.78787879,  3.88888889,  3.98989899,\n        4.09090909,  4.19191919,  4.29292929,  4.39393939,  4.49494949,\n        4.5959596 ,  4.6969697 ,  4.7979798 ,  4.8989899 ,  5.        ])\n\n\n\\Delta x for this array is (5 - (-5)) / (100 - 1) = 0.\\overline{10}\n\nnp.isclose(np.diff(x2)[0], (5 - (-5)) / (100 - 1))\n\nTrue\n\n\n(x1, y1) points are used to draw the graph of y=x^2, while (x2, y2) points are used for the graph of y=\\sin(x)."
  },
  {
    "objectID": "posts/1d-scalar-function/index.html#matplotlib",
    "href": "posts/1d-scalar-function/index.html#matplotlib",
    "title": "How to draw 1D scalar functions in Python",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nMatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n\nMatplotlib is one of the most popular visualization libraries in Python, see Figure 2.\n\nimport matplotlib.pyplot as plt\n\nplt.plot(x1, y1, label=r'$x^2$')\nplt.plot(x2, y2, label=r'$\\sin x$')\nplt.legend()\nplt.axvline(0, color='k')\nplt.axhline(0, color='k')\nplt.show()\n\n\n\n\nFigure 2: A plot using matplotlib"
  },
  {
    "objectID": "posts/1d-scalar-function/index.html#pandas",
    "href": "posts/1d-scalar-function/index.html#pandas",
    "title": "How to draw 1D scalar functions in Python",
    "section": "pandas",
    "text": "pandas\n\npandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nSince pandas is a great data analysis tool in Python, it can used for drawing graphs, see Figure 3.\n\nimport pandas as pd \n\ndf1 = pd.DataFrame(data={'x':x1, 'y':y1})\ndf2 = pd.DataFrame(data={'x':x2, 'y':y2})\n\nax = df1.plot(x='x', y='y', label=r'$x^2$')\ndf2.plot(ax=ax, x='x', y='y', label=r'$\\sin x$')\nax.axvline(0, color='k')\nax.axhline(0, color='k')\nplt.show()\n\n\n\n\nFigure 3: A plot using pandas"
  },
  {
    "objectID": "posts/1d-scalar-function/index.html#plotly",
    "href": "posts/1d-scalar-function/index.html#plotly",
    "title": "How to draw 1D scalar functions in Python",
    "section": "Plotly",
    "text": "Plotly\n\nPlotly is a technical computing company headquartered in Montreal, Quebec, that develops online data analytics and visualization tools. Plotly provides online graphing, analytics, and statistics tools for individuals and collaboration, as well as scientific graphing libraries for Python, R, MATLAB, Perl, Julia, Arduino, JavaScript and REST.\n\nPlotly is a useful tool for creating interactive plots, see Figure 4.\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x1, y=y1, mode='lines', name='x²'))\nfig.add_trace(go.Scatter(x=x2, y=y2, mode='lines', name='sin(x)'))\nfig.show()\n\n\n\n                                                \nFigure 4: A plot using plotly"
  },
  {
    "objectID": "posts/explain-words/index.html",
    "href": "posts/explain-words/index.html",
    "title": "천문용어 설명",
    "section": "",
    "text": "2019년 우주관측 수업에서 2020 역서 09. 천문상수와 자료 설명용으로 작성한 글"
  },
  {
    "objectID": "posts/explain-words/index.html#국제천문연맹-천문상수",
    "href": "posts/explain-words/index.html#국제천문연맹-천문상수",
    "title": "천문용어 설명",
    "section": "국제천문연맹 천문상수",
    "text": "국제천문연맹 천문상수\n\nTT, TCG, TCB, TDB\n\n\\text{TCG} = \\text{TT} + L_G \\times (\\text{JD} - 2443144.5) \\times 86400 \\ \\text{s}\n\n\n(\\text{TCB}-\\text{TCG})_{\\text{secular}} = L_C \\times (\\text{JD} - 2443144.5) \\times 86400 \\ \\text{s}\n\n\n\\text{TDB} = \\text{TCB} - L_B \\times (\\text{JD}_{\\text{TCB}} - T_0) \\times 86400 \\ \\text{s} + \\text{TDB}_0\n\n\n\\text{TDB}(T_0) - \\text{TCB}(T_0) = (\\text{TDB} - \\text{TCB} \\quad \\text{at} \\quad \\text{JD}_{\\text{TCB}} = T_0)\n\nSI단위계(International System of Units)의 1 초(second)는 현재 세슘-133 원자의 섭동이 없는 바닥상태의 초미세 전이 주파수 \\Delta \\nu_{\\text{Cs}}를 Hz 단위로 나타날 때 그 수치를 9 192 631 770으로 고정함으로써 정의된다. 여기서 Hz는 s^{-1}과 같다.\nTAI(International Atomic Time)는 원자시계에 기반한 시척도이며, BIPM(International Bureau of Weights and Measures)의 분석에 의해 유지되고 있다. TAI의 단위시간 길이는 지오이드에서의 SI초이다.\n지구 표면에서 정의된 좌표시간(coordinate time)을 TT(Terrestrial Time)라고 부른다. 지표면에서 이루어지는 천문관측에서의 시간측정에 주로 사용되는 시척도이다. TT의 단위 시간 길이는 TAI의 단위 시간 길이로 정의하며, TAI 1977년 1월 1일 0시 0분 0초를 TT 1977년 1월 1일 0시 0분 32.184초로 정의한다.역사적으로 ET(Ephemeris Time)를 TDT(Terrestrial Dynamic Time)가 계승하고 TDT를 TT가 계승하기에 32.184초가 붙는다.\nGCRS(Geocentric Celestial Reference System)는 지구의 질량중심에 원점을 둔 좌표계이며, 지구접근천체(near-Earth object, NEO)에 대한 역학적 계산에서 사용되는 좌표계이다. GCRS의 시간을 TCG(Geocentric Coordinate Time)라고 부른다. TCG는 중력에 의한 시간 지연 효과를 모두 무시하였을 때, 지구의 질량중심과 같이 움직이는 좌표계의 고유시간(proper time)이다.\nBCRS(Barycentric Celestial Reference System)는 태양계의 질량중심에 원점을 둔 좌표계이며, 일반적인 천체에 대한 역학적 계산에서 사용되는 좌표계이다. BCRS의 시간을 TCB(Barycentric Coordinate Time)라고 부른다. TCB는 중력에 의한 시간 지연 효과를 모두 무시하였을때,태양계의 질량중심과 같이 움직이는 좌표계의 고유시간(proper time)이다.\nTDB(Barycentric Dynamical Time)는 2006년 이후로 TCB를 사용하여 정의된다.\n각 시스템 시간의 단위 시간 길이는 상대론적 효과에 의해 미세하게 다르다.각각의 단위 시간 길이를 \\rm{d}(\\text{TT}), \\rm{d}(\\text{TCG}), \\rm{d}(\\text{TCB}), \\rm{d}(\\text{TDB}) 처럼 나타낸다.\n\n\n지구자전각(ERA)\n지구자전각(ERA, Earth Rotation Angle)는 항성시(Sidereal time)와 비슷한 개념으로, 지구가 자전에 의해 돌아간 각도를 나타낸다. rev는 revolutions(회전)의 줄임말이다.\n\n1 \\text{ rev} = 1 \\text{ 회전} = 360^\\circ\n\n\n\n태양질량인수(태양중력상수), 지구중력상수\nGM의 경우 천문관측에 의해 측정이 가능하나, G는 매우 정밀한 실험에 의해서만 측정이 가능하다. 따라서 GM의 정확도가 G, M보다 높은 경우가 많으므로 천문상수에 GM 값을 명시한 것이다.\n\n\n지구적도반경\nSI m(미터)는 SI s(초)에 기반하여 정의되므로, m 단위의 상수에도 시척도 [\\text{TT}]를 밝힌 것이다. \na_E = a_e\n\n\n\n지구역학계수, 지구역학계수 시간변화율\n지구를 꽉찬 회전타원면(spheroid)형태의 질량체라고 가정하자. 원점이 지구의 질량중심에 위치하고, 지구의 회전이 관측되지 않는 구면좌표계(spherical coordinate system)에서의 지구 중력 포텐셜(potential) \\phi는 지구 중력상수 \\mu = GM_E에 대해 다음과 같이 주어진다. \n\\phi = - \\frac{\\mu}{r} + \\sum_{n=2}^{\\infty} \\frac{J_n P_n(\\sin \\theta)}{r^{n+1}}\n\n여기서 P_n은 르장드르 다항식(Legendre polynomial)으로, 로드리게스공식(Rodrigues’ formula)은 다음과 같다. \nP_n(x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2 - 1)^n\n\n\\phi에서 첫항을 제외한 지배항(dominating term)은 n=2인 J_2 term으로, n \\geq 3 항들은 일반적으로 무시가능하다. J_2 항은 다음과 같이 주어진다.\n\\phi_{J_2 \\  \\text{term}} = \\frac{J_2 P_2(\\sin \\theta)}{r^3} = J_2 \\frac{3 \\sin^2 \\theta - 1}{2r^3}\n여기서 J_2 항의 계수가 J_2이며, 지구역학계수(Dynamical form-factor for the Earth)라고 부른다. 지구역학계수 시간변화율에서 cy는 century(1세기, 100년)의 줄임말이다.\nJ_2의 값은 다음과 같이 계산할 수 있다.\n\nJ_2 = \\frac{2}{3}f - \\frac{a^3 \\omega^2}{3GM_E}\n\n여기서 f는 지구 편평도, a는 지구 적도반경, \\omega는 지구 평균 각속도, GM_E는 지구 중력상수이다.\n\n\n지오이드 포텐셜\n중력장(gravitational field) \\mathbf{g}에 대해 \\mathbf{g} = - \\nabla \\phi를 만족시키는 스칼라장(scalar field) \\phi를 중력 포텐셜(potential)이라고 부른다.\n지구 중력장의 등퍼텐셜면(equipotential surface)을 지오이드(geoid)라고 부른다. 그러므로 정의에 의해 지오이드는 중력에 수직이다. 또한 지오이드는 무수히 많으며, 각각에 대응되는 지오이드 포텐셜도 무수히 많다. 다만 관습적으로 특정한 등포텐셜면(바다에서는 평균 해수면, 육지에서는 평균 해수면을 연장한 곡면)을 지오이드라고 부른다. 지오이드 포텐셜 W_0는 현재 다음과 같이 정의된다. \nW_0 = c^2 L_G\n\n\n\n지구 편평도 역수 (IERS 2010)\n지구는 자전에 의해 적도반경이 극반경보다 길다. 따라서 지구의 단면을 장반경이 a, 단반경이 b인 타원으로 근사 가능하다. 이때 지구 편평도(flattening) f는 다음과 같이 정의된다 \nf = \\frac{a-b}{a}\n\nIERS는 International Earth Rotation and Reference Systems Service의 약자이다.\n\n\n일반 경도세차, 경사각 변화율, 경도 적도세차, 경사각 적도세차, 장동상수\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n관성좌표계(inertial frame of reference)에서 관측한 지구는 위의 그림 처럼 자전(Rotation), 세차(Precession), 장동(Nutation)을 한다. 자전은 지구의 적도 평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 지구 전체가 반시계 방향으로 회전하는 현상이다. 자전 주기는 1일이다. 세차는 지구의 황도평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 지구의 자전축이 시계 방향으로 회전하는 현상이다. 세차 주기는 약 26000년이다. 장동은 지구의 황도평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 지구의 자전축과 지구의 공전축 사이의 각도가 주기적으로 변하는 현상이다. 장동 주기는 약 18.61년이다.\n그리스의 히파르코스(Hipparchus of Nicaea)가 춘분점이 이동한다는 현상을 BC 127년에 발견함으로써 지구의 세차운동이 알려졌기 때문에, 역사적으로 지구의 세차운동은 분점의 세차운동(precession of the equinoxes)이라고 불렸다. 그러나 뉴턴 이후 천체역학이 발달하면서 세차운동이 태양, 달, 지구 외 행성들이 지구에 작용하는 중력에 의해 발생한다는 사실을 알게 되면서 새로운 용어가 만들어졌다. 주로 태양과 달에 의해 지구의 적도 평면이 변하는 현상을 일월 세차(Lunisolar precession), 주로 지구 외 행성에 의해 지구의 황도 평면이 변하는 현상을 행성 세차(Planetary precession)라고 명명했으며, 이 둘의 결합으로 발생하는 실제 세차운동을 일반 세차(General Precession)라고 명명했다. 그러나 태양과 달이 지구의 황도평면을 변화시키기도 하고, 지구 외 행성이 지구의 적도평면을 변화시키기도 하기에, 용어에 있어 혼선이 존재한다는 판단 하에 2006년 IAU가 각각의 용어를 재정의했다. 즉 일월세차(Lunisolar precession)를 적도 세차(Precession of the equator)로, 행성 세차(Planetary precession)를 황도 세차(Precession of the ecliptic)로 용어를 바꾸었다.\n\n\n\n\n\n위 그림은 지구의 황도평면과 수직한 직선 상에 존재하면서 지구의 북극 근방에 있는 관측자가 지구를 바라보았을 때 천구의 북극(지구 자전축의 끝부분)이 이동하는 경로를 나타낸 것이다. 점선의 운동경로(path of mean celestial pole)는 세차운동에 의한 경로이고, 실선의 운동경로(path of instantaneous celestial pole)는 세차운동에 의한 원운동에 장동운동에 의한 타원운동이 결합된 운동에 의한 경로이다. 천구의 북극이 점선의 운동경로를 따라 이동하는 각속력을 일반 경도세차(General precession in longitude) p_A라고 한다. 장동운동에 의해 천구의 북극은 국소적인 타원운동을 하게 되는데, 이 타원의 장반경(semi-major axis)을 장동장수(constant of nutation) N이라 한다.\n\n\n\n\n\nEquinoctial colure는 천구의 북극(Celestial North Pole), 천구의 남극(Celestial South Pole), 춘분점(Vernal Equinox), 추분점(Autumnal Equinox)을 지나는 대원이다. Solstitial colure는 천구의 북극(Celestial North Pole), 천구의 남극(Celestial South Pole), 하지점(Summer Solstice), 동지점(Winter Solstice)을 지나는 대원이다.\n어느 시점에서의 \\epsilon_A는 그 시점에서의 황도(ecliptic of date)와 그 시점에서의 평균 적도(mean equator of date)가 이루는 각도이다. 어느 시점에서의 \\psi_A는 그 시점에서의 solstital colure(solstital colure of date)와 역기점에서의 solstital colure(solstitial colure of epoch)가 이루는 각도이다. 어느 시점에서의 \\omega_A는 그 시점에서의 평균적도(mean equator of date)와 역기점에서의 고정된 황도(fixed ecliptic of epoch)가 이루는 각도이다. P03 precession model에 의해 주어지는 각각의 값을 t에 대한 2차항 까지만 나타내면 다음과 같다.\n\n\\epsilon_A = 84381''.406 - 46''.836769t - 0''.0001831t^2\n\n\n\\psi_A = 5038''.481507t - 1''.0790069t^2\n\n\n\\omega_A = 84381''.406 - 0''.025754t + 0''.0512623t^2\n\n여기서 t = (\\text{TT} - \\text{2000 January 1d 12h TT})/36525 (값은 day단위로 넣는다)는 J2000.0 TT 로부터 경과된 시간을 Julian century 단위로 나타낸 값이다.\nJ2000.0 경사각 변화율(Rate of change in obliquity) \\dot{\\epsilon}는 \\epsilon_A를 t에 대해 미분하고 t=0을 대입한 값으로, J2000.0에서의 \\epsilon_A의 시간 변화율을 나타낸다.\nJ2000.0 경도 적도세차(Precession of the equator in longitude) \\dot{\\psi}는 \\psi_A를 t에 대해 미분하고 t=0을 대입한 값으로, J2000.0에서의 \\psi_A의 시간 변화율을 나타낸다.\nJ2000.0 경사각 적도세차(Precession of the equator in obliquity) \\dot{\\omega}는 \\omega_A를 t에 대해 미분하고 t=0을 대입한 값으로, J2000.0에서의 \\omega_A의 시간 변화율을 나타낸다.\n\n\n태양 시차\n태양에서 본 지구의 시반경(apparent radius, angular radius)을 태양 시차(solar parallax)라고 부른다.\n\n\n광행차 상수\n\n\n\n\n\n광행차(aberration)란 관측자의 속도에 의존하여 관측대상의 겉보기 위치가 바뀌는 현상이다. 관측자가 v의 속력으로 움직인다고 하자. 관측자가 움직이는 방향을 각도를 측정하는 기준선으로 설정하고, 관측자가 정지했을 때 특정 관측 대상에 대한 시선방향과 기준선이 이루는 각을 \\theta, 관측자가 움직일 때 특정 관측 대상에 대한 시선방향과 기준선이 이루는 각을 \\phi라고 하자. 그러면 v/c \\ll 1일 때 다음이 성립한다.\n\n\\kappa = \\theta - \\phi \\approx v/c\n\nv가 지구의 평균 공전 속도일때의 \\kappa를 광행차 상수(constant of aberration)라고 부른다."
  },
  {
    "objectID": "posts/explain-words/index.html#일반-천문상수",
    "href": "posts/explain-words/index.html#일반-천문상수",
    "title": "천문용어 설명",
    "section": "일반 천문상수",
    "text": "일반 천문상수\n\n표면중력, 태양상수, 표면탈출속도, 표면유효온도, 전복사에너지, 표면복사에너지\n질량이 M인 천체의 반지름이 R이라면 표면중력은 다음과 같이 구할 수 있다.\n\n\\text{표면중력} = \\frac{GM}{R^2}\n\n태양으로부터 1 \\text{ au}만큼 떨어진 단위면적에 단위시간동안 통과하는 총 태양에너지를 태양상수라고한다. 즉, r=1 \\text{ au}, L = \\text{광도} 일때 다음을 얻는다.\n\n\\text{태양상수} = \\frac{L}{4 \\pi r^2}\n\n질량이 M인 천체의 반지름이 R이라면 표면탈출속도는 다음과 같이 구할 수 있다. \n\\text{표면탈출속도} = \\sqrt{\\frac{2GM}{R}}\n\n천체를 그 천체와 같은 광도를 가진 흑체로 가정했을 때 흑체가 가지는 열평형온도를 표면유효온도라고 한다. \n\\text{표면유효온도} = T_e = \\left( \\frac{F_R}{\\sigma} \\right)^{1/4}\n\n천체가 단위 시간 동안 방출하는 총에너지를 전복사에너지(또는 광도)라고 한다. \n\\text{전복사에너지} = L = \\text{광도}\n\n천체의 단위 표면적이 단위 시간 동안 방출하는 총에너지를 표면복사에너지라고 한다. \n\\text{표면복사에너지} = F_R = \\frac{L}{4 \\pi R^2}\n\n\n\n이심률\n닫힌 공전 궤도는 타원 궤도이고, 타원의 장반경 a, 단반경 b에 대해 궤도의 이심률(eccentricity) e는 다음과 같이 계산된다. \ne = \\frac{\\sqrt{a^2 - b^2}}{a}\n\n\n\n적도 수평 시차\n적도상의 한 관측자 천정에 천체가 위치한다고 하자. 그 관측자로부터 적도를 따라 90^\\circ만큼 떨어져 있는 관측자가 같은 천체를 바라본다고 하자. 이때 두 관측자의 시선방향이 이루는 각을 적도 수평 시차(equatorial horizontal parallax)라고 한다.\n따라서 달의 적도 수평 시차 \\pi는 지구 적도 반경 R, 지구 중심에서 달까지의 거리 r에 대해 다음과 같이 주어진다.\n\n\\pi = \\arcsin \\left( \\frac{R}{r} \\right)\n\n\n\n장동주기 (교점주기)\n\n\n\n\n\n천체의 공전궤도가 어떤 기준평면과 만나는 2개의 점을 통틀어서 궤도 교점(orbital node)이라고 하며, 기준평면의 어느 한쪽을 위쪽으로 정의하였을 때, 천체가 아래에서 위로 올라오는 교점을 승교점(ascending node), 위에서 아래로 내려가는 교점을 강교점(descending node)이라고 한다.\n기준평면을 황도평면으로 하였을때, 달의 공전궤도에 대한 궤도교점을 달의 교점(Lunar node)이라고 한다. 지구의 장동 운동을 발생시키는 주 원인이 달이기 때문에, 이에 대한 반작용으로 달의 교점이 변하게 된다. 달의 교점이 황도를 따라 한바퀴 회전하는데 걸리는 시간은 지구의 장동 운동주기와 같은 약18.61년이다. 따라서 이것을 장동주기(nodal period) 또는 교점주기(draconic period)라고 한다.\n\n\n사로스 주기\n사로스주기(saros)는 태양, 달, 지구의 상대적 위치 관계가 반복되는 주기로, 정확히 223 삭망월(synodic month)이다.\n\n\n오오트 상수\n은하면(galactic midplane)에 존재하는, 은경(galactic longitude)이 \\ell인 임의의 천체가 태양으로부터 떨어진 거리가 d라고 하자. 그러면 태양에 대한 그 천체의 상대속도의 시선방향 성분(radial component)과 접선 방향 성분(tangential component)은 다음과 같이 주어진다.\n\nv_r \\approx Ad\\sin(2\\ell)\n\n\nv_t \\approx Ad\\sin(2\\ell) + Bd\n\n여기서 A, B가 오오트 상수(Oort constant)이다."
  },
  {
    "objectID": "posts/explain-words/index.html#정오표",
    "href": "posts/explain-words/index.html#정오표",
    "title": "천문용어 설명",
    "section": "정오표",
    "text": "정오표\n\n지구 평균 각운동량\n\n\n\\text{지구 평균 각운동량} \\rightarrow \\text{지구 평균 각속도}\n\n지구 평균 각속도는 지구의 평균 자전 각속도이다.\n\n\n태양-(지구+달) 질량비\n \n(S/E)(l+\\mu) \\rightarrow (S/E)(1+\\mu)\n\n\n\\because \\frac{S}{E+M_M} = \\frac{S}{E+\\mu E} = \\frac{S}{E}\\frac{1}{1+\\mu} = (S/E)(1+\\mu)\n\n\n\n전자볼트\n \neV = \\frac{e}{c} J \\rightarrow \\text{eV} = (e/\\text{C}) \\text{ J}\n\n여기서 \\text{J}는 에너지의 단위 줄(joule)이고, (e/\\text{C})는 쿨롬(coulomb) 단위로 나타낸 기본 전하(elementary charge)의 값이다."
  },
  {
    "objectID": "posts/explain-words/index.html#참고문헌",
    "href": "posts/explain-words/index.html#참고문헌",
    "title": "천문용어 설명",
    "section": "참고문헌",
    "text": "참고문헌\n\nCAPITAINE, Nicole; WALLACE, Patrick T.; CHAPRONT, Jean. Expressions for IAU 2000 precession quantities. Astronomy & Astrophysics​, 2003, 412.2: 567-586.\nCARROLL, Bradley W.; OSTLIE, Dale A. An introduction to modern astrophysics​. Cambridge University Press, 2017.\nLIESKE, J.H., et al. Expressions for the precession quantities based upon the IAU/1976/system of astronomical constants. Astronomy and Astrophysics​, 1977, 58: 1-16.\nMCCARTHY, Dennis D.; SEIDELMANN, P.Kenneth. Time: from Earth rotation to atomic physics​. Cambridge University Press, 2018.\nVÖLGYESI, L. Physical backgrounds of Earth’s rotation, revision of the terminology. Acta Geodaetica et Geophysica Hungarica​, 2006, 41.1: 31-44.\nBIPM - Time, https://www.bipm.org/en/bipm/tai/\nFIG Article of the Month - December 2004, https://www.fig.net/resources/monthly_articles/2004/beutLer_july_2004.asp\nFile:Lunar eclipse diagram-en.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Lunar_eclipse_diagram-en.svg\nFile:Praezession.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Praezession.svg#mw-jump-to-license\nFile:Simple stellar aberration diagram.svg - Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Simple_stellar_aberration_diagram.svg\nFundamental Physical Constants from NIST, https://pml.nist.gov/cuu/Constants/\nReference Earth Model - WGS84, https://topex.ucsd.edu/geodynamics/14gravity1_2.pdf\nThe Astronomical Almanac Online, http://asa.hmnao.com/index.html\n기본단위의 정의, https://www.kriss.re.kr/standard/view.do?pg=explanation_tab_02"
  },
  {
    "objectID": "posts/conservation-law/index.html",
    "href": "posts/conservation-law/index.html",
    "title": "보존법칙",
    "section": "",
    "text": "2018년에 물리학실험 보고서용으로 작성한 글"
  },
  {
    "objectID": "posts/conservation-law/index.html#역학적-에너지-보존-법칙",
    "href": "posts/conservation-law/index.html#역학적-에너지-보존-법칙",
    "title": "보존법칙",
    "section": "역학적 에너지 보존 법칙",
    "text": "역학적 에너지 보존 법칙\n뉴턴의 운동법칙은 근본적으로 미분방정식으로 서술된다. 특히 힘과 가속도와 같은 물리량들은 벡터량이기 때문에 다루기가 쉽지 않다. 이러한 이유로 물리학자들은 스칼라량인 일이라는 개념을 만든 후에, 뉴턴 운동법칙을 수학적으로 약간 조작하여 운동에너지라는 새로운 개념을 만들게 되었다. 그리고 그것을 통해 일-(운동)에너지 정리를 이끌어 내었다. 이 정리는 ’물체에 가해진 알짜힘이 한 일은 물체의 운동에너지 변화량과 같다’라는 사실을 알려준다(\\mathbf{F} = m \\mathbf{a}에서의 \\mathbf{F}가 알짜힘(합력, net force)이기 때문이다). 일-운동에너지 정리를 식으로 나타내면 다음과 같다.\n\nW_{\\text{알짜 힘}} = \\Delta K\n\n이 정리는 수학적으로 유도된 정리이기 때문에, 물체에 가해지는 모든 알짜힘에 대해 성립하는 정리이다. 그러나 물리학자들은 여기서 한발 더 나아가 좌변을 다음과 같이 쓰고 싶어했다.\n\n-\\Delta U = \\Delta K\n\n왜냐하면 만약 이렇게 쓸 수만 있다면, K라는 양과 U라는 양이 ’보존’되기 때문이다. 그런데 다행히도 물리학자들은 중력이나 탄성력, 전기력과 같은 특정한 힘들이 하는 일은 위와 같이 쓸수 있음을 알게 되었고, U를 ’위치에너지(퍼텐셜에너지)’라 정의하였다. 또한 퍼텐셜에너지를 정의할 수 있는 힘을 보존력이라고 부르게 되었다. 따라서 다음과 같이 쓸 수 있다.\n\nW_{\\text{보존력}} = -\\Delta U_{\\text{보존력}}\n\n또한 모든 힘은 이러한 보존력과 보존력이 아닌 힘, 즉 비보존력 두 가지로 나뉜다는 사실을 알게 되었다. 일은 힘과 변위벡터의 내적으로 정의되고, 내적은 수학적으로 분배법칙이 성립하기 때문에 일-운동에너지 정리는 다음과 같이 쓸 수 있다.\n\nW_{\\text{보존력}} + W_{\\text{비보존력}} = \\Delta K\n \nW_{\\text{비보존력}} = \\Delta K + \\Delta U_{\\text{보존력}}\n\n이때 물체의 운동에너지와 퍼텐셜에너지의 합을 역학적 에너지라고 정의하게 되면 다음을 얻는다.\n\nW_{\\text{비보존력}} = \\Delta E_{\\text{역학적}}\n\n따라서 비보존력이 일을 하지 않으면 물체의 역학적 에너지는 일정하게 유지된다. 즉 어떤 물체에 보존력만 일을 한다면 그 물체의 역학적 에너지는 일정하게 유지된다. 바로 이것을 역학적 에너지 보존 법칙이라고 부른다.\n\n퍼텐셜 에너지(Potential Energy)의 정의\n\n어떤 물체의 위치벡터가 벡터함수 \\mathbf{r}(t) (a\\leq t \\leq b)로 주어지고, 시간 a\\leq t \\leq b 동안 물체가 움직인 경로가 곡선 C이며 그동안 물체에 가해진 힘이 \\mathbf{F}(\\mathbf{r})일 때(또는 물체가 벡터장 \\mathbf{F}(\\mathbf{r})가 있는 공간에서 곡선 C를 따라 움직였을 때), 힘 \\mathbf{F}가 물체에 한 일 W는 다음과 같이 정의한다. 이때 \\rm{d}\\mathbf{l} = \\rm{d}\\mathbf{r} = \\mathbf{r}'\\rm{d}t 이다.\n\n\n\\begin{align*}\nW & = \\int_{C} \\mathbf{F} \\cdot \\rm{d}\\mathbf{l} \\\\\n& = \\int_{a}^{b} \\mathbf{F}(\\mathbf{r}(t)) \\cdot \\mathbf{r}'(t)\\rm{d}t\n\\end{align*}\n\n\n어떤 힘 \\mathbf{F}가 보존력일때, 즉 위치벡터 \\mathbf{r}(t)에 대한 벡터함수 \\mathbf{F}(\\mathbf{r}(t))가 보존장일때, 다음을 만족하는 스칼라 함수 U(\\mathbf{r}(t))를 힘 \\mathbf{F}에 대한 퍼텐셜 에너지라고 정의한다.\n\n\n\\mathbf{F}(\\mathbf{r}) = -\\nabla U(\\mathbf{r})\n\n\n어떤 힘 \\mathbf{F}가 \\mathbf{F}(\\mathbf{r}) = - \\nabla U(\\mathbf{r})로 주어지는 보존력이고, 위치벡터가 \\mathbf{r}(t)로 주어지는 물체가 시간 a \\leq t \\leq b동안 힘 \\mathbf{F}를 받으며 경로 C를 따라 움직였다고 하면, 선적분의 기본정리에 의해 다음이 성립한다.\n\n\nU(\\mathbf{r}(b)) - U(\\mathbf{r}(a)) = - \\int_{C} \\mathbf{F} \\cdot \\rm{d}\\mathbf{l}\n\n\n따라서 보존력 \\mathbf{F}에 대한 위치 \\mathbf{r}에서의 퍼텐셜 에너지 U(\\mathbf{r})은 선적분을 이용해 다음과 같이 쓸 수 있다. 이때 \\mathbf{r}^{*}는 퍼텐셜 에너지가 0이 되는 임의의 기준점이다.\n\n\nU(\\mathbf{r}) = - \\int_{\\mathbf{r}^{*}}^{\\mathbf{r}} \\mathbf{F} \\cdot \\rm{d} \\mathbf{l}"
  },
  {
    "objectID": "posts/conservation-law/index.html#운동량-보존-법칙",
    "href": "posts/conservation-law/index.html#운동량-보존-법칙",
    "title": "보존법칙",
    "section": "운동량 보존 법칙",
    "text": "운동량 보존 법칙\n관성기준계의 원점에 대해 위치벡터가 \\mathbf{r}_{1}, \\mathbf{r}_{2}, \\cdots, \\mathbf{r}_{n}으로 주어지고, 질량이 각각 m_{1}, m_{2}, \\cdots, m_{n}인 n개의 물체들로 구성된 계를 설정하자. 각 물체는 위치벡터의 첨자에 따라 1, 2, \\cdots, n번째 물체라고 하겠다. 관성기준계에 대해 물체의 위치벡터를 표현하였으므로, i번째 물체에 대해 뉴턴 제 2법칙을 적용하면 다음과 같다. 이때 \\mathbf{F}_{\\text{내력}, i}는 i번째 물체를 제외한 계 내부의 n-1개의 물체들이 i번째 물체에 작용하는 힘들의 합력이고, \\mathbf{F}_{\\text{외력}, i}는 주어진 계 외부의 물체들이 i번째 물체에 작용하는 힘들의 합력이다. 또한 \\mathbf{v}_{i} = \\displaystyle \\frac{d}{dt} \\mathbf{r}_{i}이다.\n\n\\mathbf{F}_{\\text{내력}, i} + \\mathbf{F}_{\\text{외력}, i} = m_{i} \\frac{d}{dt} \\mathbf{v}_{i}\n\n이때 뉴턴 제 3법칙에 의해\n\n\\sum_{i=1}^{n} \\mathbf{F}_{\\text{내력}, i} = \\mathbf{0}\n\n이 성립하고, 물체의 질량이 시간에 대해 변하지 않는다는 가정을 한다면\n\nm_{i} \\frac{d}{dt} \\mathbf{v}_{i} = \\frac{d}{dt} (m_{i} \\mathbf{v}_{i})\n\n이 성립한다. 따라서 i번째 물체에 대한 뉴턴 운동방정식 양변에 시그마를 취하면 다음과 같다. 이때 \\mathbf{F}_{\\text{외력}} = \\displaystyle \\sum_{i=1}^{n} \\mathbf{F}_{\\text{외력}, i}이다.\n\n\\mathbf{F}_{\\text{외력}} = \\frac{d}{dt} \\left( \\sum_{i=1}^{n} m_{i} \\mathbf{v}_{i} \\right)\n\n이때 새로운 물리량 \\mathbf{p} \\equiv m\\mathbf{v}를 정의하고, 이것을 (선)운동량이라고 부르자. 그러면 i번째 물체의 운동량은 \\mathbf{p}_{i} = m_{i}\\mathbf{v}_{i}이므로, 위 식의 우변은 계 내부 물체들의 총운동량의 시간에 대한 변화율이다. 그러므로 계 내부 물체들의 총운동량을 \\mathbf{P} = \\displaystyle \\sum_{i=1}^{n} m_{i} \\mathbf{v}_{i}라고 하면 다음 식이 성립한다.\n\n\\mathbf{F}_{\\text{외력}} = \\frac{d}{dt} \\mathbf{P}\n\n여기서 \\mathbf{F}_{\\text{외력}} = \\mathbf{0}일때 \\mathbf{P} = \\text{일정} 임을 알수 있다. 즉 어떤 계에 작용하는 알짜 외력이 0이면, 그 계의 총운동량은 시간에 대해 변하지 않는다. 이 사실을 운동량 보존 법칙이라고 한다."
  },
  {
    "objectID": "posts/conservation-law/index.html#각운동량-보존-법칙",
    "href": "posts/conservation-law/index.html#각운동량-보존-법칙",
    "title": "보존법칙",
    "section": "각운동량 보존 법칙",
    "text": "각운동량 보존 법칙\n일반적으로 힘 \\mathbf{F}가 작용하는 위치의 위치벡터를 \\mathbf{r}이라고 한다면, 그 위치에 있는 물체에 작용하는 토크(돌림힘)는 다음과 같이 주어진다.\n\n\\boldsymbol{\\tau} = \\mathbf{r} \\times \\mathbf{F}\n\n만약 이 힘 \\mathbf{F}가 그 위치에 있는 물체에 작용하는 알짜힘이라면, 뉴턴 제 2법칙에 의해 다음과 같이 나타날 수 있다.\n\n\\boldsymbol{\\tau} = \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt}\n\n그런데 다음이 성립하므로\n\n\\begin{align*}\n\\frac{d}{dt} (\\mathbf{r} \\times \\mathbf{p}) & = \\frac{d\\mathbf{r}}{dt} \\times \\mathbf{p} + \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt} \\\\\n& = \\mathbf{v} \\times m\\mathbf{v} + \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt} \\\\\n& = \\mathbf{r} \\times \\frac{d\\mathbf{p}}{dt}\n\\end{align*}\n\n알짜힘에 의한 토크는 다음과 같이 쓸 수 있다. 이때 \\mathbf{l} = \\mathbf{r} \\times \\mathbf{p}은 각운동량이다.\n\n\\begin{align*}\n\\boldsymbol{\\tau} & = \\frac{d}{dt} (\\mathbf{r} \\times \\mathbf{p}) \\\\\n& = \\frac{d\\mathbf{l}}{dt}\n\\end{align*}\n\n여러 물체로 이루어진 계에서 각 물체에 대해, 뉴턴 제 2법칙을 반복적으로 적용하면 다음을 얻는다. 이때 \\mathbf{F}_{\\text{ext}}는 계에 대한 외력의 총합이고, \\mathbf{P}는 계 내부 물체들의 운동량의 총합이다.\n\n\\mathbf{F}_{\\text{ext}} = \\frac{d\\mathbf{P}}{dt}\n\n따라서 \\mathbf{F}_{\\text{ext}}가 작용하는 위치에 작용하는 토크를 \\boldsymbol{\\tau}_{\\text{ext}}라고 쓴다면 다음이 성립한다. 이때 \\mathbf{L}은 계 내부 물체들의 각운동량의 총합이다.\n\n\\boldsymbol{\\tau}_{\\text{ext}} = \\frac{d\\mathbf{L}}{dt}\n\n따라서 알짜 외부 토크가 \\mathbf{0}이면 계의 총 각운동량은 보존됨을 알 수 있고, 이를 각운동량 보존 법칙이라고 한다."
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "Create an image using Diffusers library.\n\n\n\n!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)\n\n\n\n\n\npil_images = pipe(\n    prompt=prompt * batch_size,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images\n\n\nfor pil_image in pil_images:\n    display(pil_image)\n\n\n\n\n\n\n\n\n\n\n\nPatil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#install-and-import-libraries",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#install-and-import-libraries",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "!pip install -qq diffusers transformers scipy ftfy accelerate\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512  # default height of Stable Diffusion\nwidth = 512  # default width of Stable Diffusion\n\nnum_inference_steps = 50  # Number of denoising steps\n\nguidance_scale = 7.5  # Scale for classifier-free guidance\n\ngenerator = torch.manual_seed(256)  # Seed generator to create the inital latent noise\n\nbatch_size = 2\n\n\nfrom diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", scheduler=scheduler\n)\n\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.20.2\",\n  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"LMSDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\npipe = pipe.to(device)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#high-level",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#high-level",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "pil_images = pipe(\n    prompt=prompt * batch_size,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images\n\n\nfor pil_image in pil_images:\n    display(pil_image)"
  },
  {
    "objectID": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#references",
    "href": "posts/stable-diffusion-inference/Stable_Diffusion_Inference_high_level.html#references",
    "title": "Stable Diffusion Inference with Diffusers (high-level)",
    "section": "",
    "text": "Patil et al. (2022) Stable Diffusion with 🧨 Diffusers, https://huggingface.co/blog/stable_diffusion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "천문용어 설명\n\n\n\n\n\n\n\nastronomy\n\n\nkorean\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nSpherical Astronomy\n\n\n\n\n\n\n\nastronomy\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nHow to draw 1D scalar functions in Python\n\n\n\n\n\n\n\nmath\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\n보존법칙\n\n\n\n\n\n\n\nphysics\n\n\nkorean\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nMotion of Charged Particles in Magnetic Dipole Fields\n\n\n\n\n\n\n\nphysics\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion Inference with Diffusers (high-level)\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion Inference with Diffusers (low-level)\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nMingyu Jeon\n\n\n\n\n\n\nNo matching items"
  }
]